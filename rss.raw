<?xml version="1.0" encoding="UTF-8"?><rss dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>in Code â€” Entries</title><link>https://blog.jle.im/</link><description>Weblog of Justin Le, covering his various adventures in programming and explorations in the vast worlds of computation physics, and knowledge.</description><item><title>Advent of Code 2017! Ongoing solutions and explanations</title><link>https://blog.jle.im/entry/advent-of-code-2017.html</link><description>&lt;p&gt;Just a short post to share that I started a github repository of my &lt;a href="https://github.com/mstksg/advent-of-code-2017"&gt;Advent of Code 2017 Solutions&lt;/a&gt;, as I write them!&lt;/p&gt;
&lt;p&gt;I also am including my &lt;a href="https://github.com/mstksg/advent-of-code-2017/blob/master/reflections.md"&gt;reflections&lt;/a&gt; and explanations on my solutions, explaining my thought processes and how the solutions work.&lt;/p&gt;
&lt;p&gt;Yes I definitely spent a bit too much time writing the executable, which is an automated (cached) downloader, test suite runner (on sample inputs), and benchmark suite.&lt;/p&gt;
&lt;p&gt;I originally was only going to casually try the problems (like I did last year), but I hit a decent global rank by accident on Day 4 (which was very suited for Haskell!), and since then I've been taking things seriously to try to aim for the global leaderboard (top 100). This is a struggle for me because I'm not really the &lt;em&gt;fastest&lt;/em&gt; algorithm person, but I think it's a fun goal for me to try to hit this year.&lt;/p&gt;
&lt;p&gt;Wish me luck! And if you haven't started yet, it's not too late to join in the fun! &lt;a href="https://twitter.com/glguy"&gt;glguy&lt;/a&gt; has been maintaining the semi-official &lt;a href="adventofcode.com/2017/leaderboard/private"&gt;Haskell Leaderboard&lt;/a&gt; (join code &lt;code&gt;43100-84040706&lt;/code&gt;) -- come join us!&lt;/p&gt;</description><author>Justin Le</author><category>Haskell</category><guid isPermaLink="true">https://blog.jle.im/entry/advent-of-code-2017.html</guid><pubDate>Thu,  7 Dec 2017 21:46:51 UTC</pubDate><creator>Justin Le</creator><subject>Haskell</subject><date>2017-12-07</date></item><item><title>Introduction to Singletons (Part 1)</title><link>https://blog.jle.im/entry/introduction-to-singletons-1.html</link><description>&lt;p&gt;Real dependent types are coming to Haskell soon! Until then, we have the great &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/singletons"&gt;singletons&lt;/a&gt;&lt;/em&gt; library :)&lt;/p&gt;
&lt;p&gt;If you've ever run into dependently typed programming in Haskell, you've probably encountered mentions of singletons (and the &lt;em&gt;singletons&lt;/em&gt; library). This series of articles will be my attempt at giving you the story of the library, the problems it solves, the power that it gives to you, and how you can integrate it into your code today![^origin] (Also, after &lt;a href="https://blog.jle.im/entry/verified-instances-in-haskell.html"&gt;my previous April Fools post&lt;/a&gt;, people have been asking me for an actual non-joke singletons post)&lt;/p&gt;
&lt;p&gt;This post (Part 1) will go over first using the singleton pattern for &lt;em&gt;reflection&lt;/em&gt;, then introducing how the singletons library helps us. Part 2 will discuss using the library for &lt;em&gt;reification&lt;/em&gt;, to get types that depend on values at runtime. Part 3 will go into the basics singleton's &lt;em&gt;defunctionalization&lt;/em&gt; system and how we can promote value-level functions to type-level functions, and Part 4 will delve into deeper applications of defunctionalization.&lt;/p&gt;
&lt;p&gt;I definitely am writing this post with the hope that it will be obsolete in a year or two. When dependent types come to Haskell, singletons will be nothing more than a painful historical note. But for now, singletons might be the best way to get your foot into the door and experience the thrill and benefits of dependently typed programming &lt;em&gt;today&lt;/em&gt;!&lt;/p&gt;
&lt;h3&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;These posts will assume no knowledge of dependent types, and, for now, only basic to intermediate Haskell knowledge (Types, kinds, typeclasses, data types, functions). The material in this post &lt;em&gt;overlaps&lt;/em&gt; with my &lt;a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html"&gt;dependently typed neural networks&lt;/a&gt; series, but the concepts are introduced in different contexts.&lt;/p&gt;
&lt;p&gt;All code is built on &lt;em&gt;GHC 8.2.2&lt;/em&gt; and with the &lt;em&gt;&lt;a href="https://www.stackage.org/lts-10.0"&gt;lts-10.0&lt;/a&gt;&lt;/em&gt; snapshot (so, singletons-2.3.1). However, there are negligible changes in the GHC type system between GHC 8.0 and 8.2 (the only difference is in the libraries, more or less), so everything should work on GHC 8.0 as well!&lt;/p&gt;
&lt;p&gt;The content in the first section of this post, describing the singleton design pattern, uses the following extensions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DataKinds&lt;/li&gt;
&lt;li&gt;GADTs&lt;/li&gt;
&lt;li&gt;KindSignatures&lt;/li&gt;
&lt;li&gt;RankNTypes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With some optional &amp;quot;convenience extensions&amp;quot;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LambdaCase&lt;/li&gt;
&lt;li&gt;TypeApplications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And the second section, introducing the &lt;em&gt;singletons&lt;/em&gt; library itself, uses, additionally:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TemplateHaskell&lt;/li&gt;
&lt;li&gt;TypeFamilies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These extension will be explained when they are used or become relevant.&lt;/p&gt;
&lt;h2&gt;The Phantom of the Types&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;(The code for this pre-singletons section is available &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs"&gt;on github&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's start with a very common Haskell trick that most learn early in their Haskelling journey: the &lt;a href="https://wiki.haskell.org/Phantom_type"&gt;phantom type&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Phantom types in Haskell are a very simple way to add a layer of &amp;quot;type safety&amp;quot; for your types and DSL's. It helps you restrict what values functions can take and encode pre- and post-conditions directly into your types.&lt;/p&gt;
&lt;p&gt;For example, in&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell data Foo a = MkFoo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;a&lt;/code&gt; parameter is &lt;em&gt;phantom&lt;/em&gt;, because nothing of type &lt;code&gt;a&lt;/code&gt; in the data type...it just exists as a dummy parameter for the &lt;code&gt;Foo&lt;/code&gt; type. We can use &lt;code&gt;MkFoo&lt;/code&gt; without ever requiring something of type &lt;code&gt;a&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t MkFoo :: Foo Int Foo Int ghci&amp;gt; :t MkFoo :: Foo Bool Foo Bool ghci&amp;gt; :t MkFoo :: Foo Either        -- requires -XPolyKinds Foo Either ghci&amp;gt; :t MkFoo :: Foo Monad         -- requires -XConstraintKinds Foo Monad&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;One use case of phantom type parameters is to prohibit certain functions on different types of values and let you be more descriptive with how your functions work together (like in &lt;a href="https://ren.zone/articles/safe-money"&gt;safe-money&lt;/a&gt;). One &amp;quot;hello world&amp;quot; use case of phantom type parameters is to tag data as &amp;quot;sanitized&amp;quot; or &amp;quot;unsanitized&amp;quot; (&lt;code&gt;UserString 'Santitized&lt;/code&gt; type vs. &lt;code&gt;UserString 'Unsanitized&lt;/code&gt;) or paths as absolute or relative (&lt;code&gt;Path 'Absolute&lt;/code&gt; vs. &lt;code&gt;Path 'Relative&lt;/code&gt;). For a simple example, let's check out a simple DSL for a type-safe door:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L12-15 data DoorState = Opened | Closed | Locked deriving (Show, Eq)&lt;/p&gt;
&lt;p&gt;data Door (s :: DoorState) = UnsafeMkDoor { doorMaterial :: String } ```&lt;/p&gt;
&lt;p&gt;A couple things going on here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Our type we are going to be playing with is a &lt;code&gt;Door&lt;/code&gt;, which contains a single field &lt;code&gt;doorMaterial&lt;/code&gt; describing, say, the material that the door is made out of. (&lt;code&gt;UnsafeMkDoor &amp;quot;Oak&amp;quot;&lt;/code&gt; would be an oak door)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We're using the &lt;code&gt;DataKinds&lt;/code&gt; extension to create both the &lt;em&gt;type&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt; as well as the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Normally, &lt;code&gt;data DoorState = Opened | Closed | Locked&lt;/code&gt; in Haskell defines the type &lt;code&gt;DoorState&lt;/code&gt; and the value constructors &lt;code&gt;Opened&lt;/code&gt;, &lt;code&gt;Closed&lt;/code&gt;, and &lt;code&gt;Locked&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;However, with &lt;code&gt;DataKinds&lt;/code&gt;, that statement also defines a new &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt;, with &lt;em&gt;type&lt;/em&gt; constructors &lt;code&gt;'Opened&lt;/code&gt;, &lt;code&gt;'Closed&lt;/code&gt;, and &lt;code&gt;'Locked&lt;/code&gt;. (note the &lt;code&gt;'&lt;/code&gt; ticks!)[^ticks]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :k 'Opened DoorState ghci&amp;gt; :k 'Locked DoorState&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;We're defining the &lt;code&gt;Door&lt;/code&gt; type with a &lt;em&gt;phantom parameter&lt;/em&gt; &lt;code&gt;s&lt;/code&gt;. It's a phantom type because we don't actually have any &lt;em&gt;values&lt;/em&gt; of type &lt;code&gt;s&lt;/code&gt; in our data type[^kind] ...the &lt;code&gt;s&lt;/code&gt; is only just there as a dummy parameter for the type.&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;UnsafeMkDoor&lt;/code&gt; without ever using anything of type &lt;code&gt;s&lt;/code&gt;. In reality, a real &lt;code&gt;Door&lt;/code&gt; type would be a bit more complicated (and the direct &lt;code&gt;UnsafeMkDoor&lt;/code&gt; constructor would be hidden).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t UnsafeMkDoor &amp;quot;Birch&amp;quot; :: Door 'Opened Door 'Opened ghci&amp;gt; :t UnsafeMkDoor &amp;quot;Iron&amp;quot; :: Door 'Locked Door 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can also use the &lt;em&gt;TypeApplications&lt;/em&gt; extension to write this in a bit more convenient way --&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t UnsafeMkDoor @'Opened &amp;quot;Birch&amp;quot; Door 'Opened ghci&amp;gt; :t UnsafeMkDoor @'Locked &amp;quot;Iron&amp;quot; Door 'Locked&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Alternatively, we can define &lt;code&gt;Door&lt;/code&gt; using &lt;a href="https://en.wikibooks.org/wiki/Haskell/GADT#Syntax"&gt;&lt;em&gt;GADT&lt;/em&gt; syntax&lt;/a&gt; (which requires the &lt;code&gt;GADTs&lt;/code&gt; extension)[^gadtnote].&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell data Door :: DoorState -&amp;gt; Type where     UnsafeMkDoor :: { doorMaterial :: String } -&amp;gt; Door s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is defining the exact same type in the alternate &amp;quot;GADT syntax&amp;quot; style of data type declaration -- here, we define types by giving the type of its constructors, &lt;code&gt;UnsafeMkDoor :: String -&amp;gt; Door s&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Door&lt;/code&gt; here is an &lt;strong&gt;indexed data type&lt;/strong&gt;, which is sometimes called a &amp;quot;type family&amp;quot; in the dependently typed programming world (which is not to be confused with type families in &lt;em&gt;GHC Haskell&lt;/em&gt;, &lt;code&gt;-XTypeFamilies&lt;/code&gt;, which is a language mechanism that is related but definitely not the same).&lt;/p&gt;
&lt;h3&gt;Phantoms in Action&lt;/h3&gt;
&lt;p&gt;At first, this seems a bit silly. Why even have the extra type parameter if you don't ever use it?&lt;/p&gt;
&lt;p&gt;Well, right off the bat, we can write functions that expect only a certain type of &lt;code&gt;Door&lt;/code&gt;, and return a specific type of &lt;code&gt;Door&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L17-18 closeDoor :: Door 'Opened -&amp;gt; Door 'Closed closeDoor (UnsafeMkDoor m) = UnsafeMkDoor m&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So, the &lt;code&gt;closeDoor&lt;/code&gt; function will &lt;em&gt;only&lt;/em&gt; take a &lt;code&gt;Door 'Opened&lt;/code&gt; (an opened door). And it will return a &lt;code&gt;Door 'Closed&lt;/code&gt; (a closed door).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; let myDoor = UnsafeMkDoor @'Opened &amp;quot;Spruce&amp;quot; ghci&amp;gt; :t myDoor Door 'Opened ghci&amp;gt; :t closeDoor myDoor Door 'Closed ghci&amp;gt; let yourDoor = UnsafeMkDoor @'Closed &amp;quot;Acacia&amp;quot; ghci&amp;gt; :t closeDoor yourDoor TYPE ERROR!  TYPE ERROR!&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can think of this as a nice way of catching &lt;em&gt;logic errors&lt;/em&gt; at compile-time. If your door type did not have its status in the type, the &lt;code&gt;closeDoor&lt;/code&gt; could have been given a closed or locked door, and you'd have to handle and reject it at &lt;em&gt;runtime&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;By adding the state of the door into its type, we can encode our pre-conditions and post-conditions directly into the type. And any opportunity to move runtime errors to compile-time errors should be celebrated with a party!&lt;/p&gt;
&lt;p&gt;This would also stop you from doing silly things like closing a door twice in a row:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t closeDoor . closeDoor TYPE ERROR!  TYPE ERROR!  TYPE ERROR!&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Do you see why?&lt;/p&gt;
&lt;p&gt;With a couple of state transitions, we can write compositions that are type-checked to all be legal:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L20-24 lockDoor :: Door 'Closed -&amp;gt; Door 'Locked lockDoor (UnsafeMkDoor m) = UnsafeMkDoor m&lt;/p&gt;
&lt;p&gt;openDoor :: Door 'Closed -&amp;gt; Door 'Opened openDoor (UnsafeMkDoor m) = (UnsafeMkDoor m) ```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t closeDoor . openDoor Door 'Closed -&amp;gt; Door 'Closed ghci&amp;gt; :t lockDoor . closeDoor . openDoor Door 'Closed -&amp;gt; Door 'Locked ghci&amp;gt; :t lockDoor . openDoor TYPE ERROR!  TYPE ERROR!  TYPE ERROR!&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Because of the type of &lt;code&gt;lockDoor&lt;/code&gt;, you &lt;em&gt;cannot&lt;/em&gt; lock an opened door! Don't even try! You'd have to close it first.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; let myDoor = UnsafeMkDoor @'Opened &amp;quot;Spruce&amp;quot; ghci&amp;gt; :t myDoor Door 'Opened ghci&amp;gt; :t lockDoor Door 'Closed -&amp;gt; Door 'Closed ghci&amp;gt; :t lockDoor myDoor TYPE ERROR!  TYPE ERROR!  TYPE ERROR! ghci&amp;gt; :t closeDoor myDoor Door 'Closed ghci&amp;gt; :t lockDoor (closeDoor myDoor) Door 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;lockDoor&lt;/code&gt; expects a &lt;code&gt;Door 'Closed&lt;/code&gt;, so if you give it a &lt;code&gt;Door 'Opened&lt;/code&gt;, that's a static compile-time type error. But, &lt;code&gt;closeDoor&lt;/code&gt; takes a &lt;code&gt;Door 'Opened&lt;/code&gt; and returns a &lt;code&gt;Door 'Closed&lt;/code&gt; -- so &lt;em&gt;that&lt;/em&gt; is something that you can call &lt;code&gt;lockDoor&lt;/code&gt; with!&lt;/p&gt;
&lt;h3&gt;The Phantom Menace&lt;/h3&gt;
&lt;p&gt;However, in standard Haskell, we quickly run into some practical problems if we program with phantom types this way.&lt;/p&gt;
&lt;p&gt;For example, how could we write a function to get the state of a door?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell doorStatus :: Door s -&amp;gt; DoorState doorStatos _ = -- ?&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(It can be done with an ad-hoc typeclass, but it's not simple, and it's prone to implementation bugs)&lt;/p&gt;
&lt;p&gt;And, perhaps even more important, how can you create a &lt;code&gt;Door&lt;/code&gt; with a given state that isn't known until runtime? If we know the type of our doors at compile-time, we can just explicitly write &lt;code&gt;UnsafeMkDoor &amp;quot;Iron&amp;quot; :: Door 'Opened&lt;/code&gt; or &lt;code&gt;UnsafeMkDoor @'Opened &amp;quot;Iron&amp;quot;&lt;/code&gt;. But what if we wanted to make a door based on a &lt;code&gt;DoorState&lt;/code&gt; &lt;em&gt;value&lt;/em&gt;? Something we might not get until runtime?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell mkDoor :: DoorState -&amp;gt; String -&amp;gt; Door s mkDoor Opened = -- ? mkDoor Closed = -- ? mkDoor Locked = -- ?&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Ah hah, you say. That's easy!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell mkDoor :: DoorState -&amp;gt; String -&amp;gt; Door s mkDoor Opened = UnsafeMkDoor mkDoor Closed = UnsafeMkDoor mkDoor Locked = UnsafeMkDoor&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, that's not how types work in Haskell. Remember that for a polymorphic type &lt;code&gt;forall s. DoorState -&amp;gt; String -&amp;gt; Door s&lt;/code&gt;, the &lt;em&gt;caller&lt;/em&gt; picks the type variable.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t mkDoor Opened &amp;quot;Acacia&amp;quot; :: Door 'Closed Door 'Closed&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Oops!&lt;/p&gt;
&lt;h3&gt;The Fundamental Issue in Haskell&lt;/h3&gt;
&lt;p&gt;We've hit upon a fundamental issue in Haskell's type system: &lt;strong&gt;type erasure&lt;/strong&gt;. In Haskell, types only exist &lt;em&gt;at compile-time&lt;/em&gt;, for help with type-checking. They are completely erased at runtime.&lt;/p&gt;
&lt;p&gt;This is usually what we want. It's great for performance, and you can bypass things like the ad-hoc runtime type checking that you have to deal with in dynamic languages like python.&lt;/p&gt;
&lt;p&gt;But in our case, it makes functions like &lt;code&gt;doorState&lt;/code&gt; fundamentally impossible. Or, does it?&lt;/p&gt;
&lt;h2&gt;The Singleton Pattern&lt;/h2&gt;
&lt;p&gt;A singleton in Haskell is a type (of kind &lt;code&gt;Type&lt;/code&gt; -- that is, &lt;code&gt;*&lt;/code&gt;) that has exactly one inhabitant. In practice (and when talking about the design pattern), it refers to a parameterized type that, for each pick of parameter, gives a type with exactly one inhabitant. It is written so that pattern matching on the &lt;em&gt;constructor&lt;/em&gt; of that value reveals the unique type parameter.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L26-29 data SingDS :: DoorState -&amp;gt; Type where     SOpened :: SingDS 'Opened     SClosed :: SingDS 'Closed     SLocked :: SingDS 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here we're using &lt;em&gt;GADT syntax&lt;/em&gt; again (but to make an actual GADT). (Also note that &lt;code&gt;Type&lt;/code&gt; is a synonym for the &lt;code&gt;*&lt;/code&gt; kind, exported from the &lt;em&gt;Data.Kind&lt;/em&gt; module) So, if we use &lt;code&gt;SOpened&lt;/code&gt;, we will get a &lt;code&gt;SingDS 'Opened&lt;/code&gt;. And if we have a &lt;code&gt;SingDS 'Opened&lt;/code&gt;, we know that it was constructed using &lt;code&gt;SOpened&lt;/code&gt;. Essentially, this gives us three values:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell SOpened :: SingDS 'Opened SClosed :: SingDS 'Closed SLocked :: SingDS 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;The Power of the Pattern Match&lt;/h3&gt;
&lt;p&gt;The power of singletons is that we can now &lt;em&gt;pattern match&lt;/em&gt; on types, essentially.&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L17-35 closeDoor :: Door 'Opened -&amp;gt; Door 'Closed&lt;/p&gt;
&lt;p&gt;lockDoor :: Door 'Closed -&amp;gt; Door 'Locked&lt;/p&gt;
&lt;p&gt;lockAnyDoor :: SingDS s -&amp;gt; Door s -&amp;gt; Door 'Locked lockAnyDoor sng door = case sng of SOpened -&amp;gt; lockDoor (closeDoor door) -- in this branch, s is 'Opened SClosed -&amp;gt; lockDoor door -- in this branch, s is 'Closed SLocked -&amp;gt; door -- in this branch, s is 'Locked ```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;lockAnyDoor&lt;/code&gt; is a function that can take a door of any state (a &lt;code&gt;Door s&lt;/code&gt; of any &lt;code&gt;s&lt;/code&gt;) and &lt;em&gt;lock&lt;/em&gt; it using a composition of &lt;code&gt;lockDoor&lt;/code&gt; or &lt;code&gt;closeDoor&lt;/code&gt; as necessary.&lt;/p&gt;
&lt;p&gt;If we have &lt;code&gt;lockAnyDoor&lt;/code&gt; take a &lt;code&gt;SingDS s&lt;/code&gt; as its input (and, importantly, make sure that the &lt;code&gt;s&lt;/code&gt; in &lt;code&gt;SingDS s&lt;/code&gt; is the same &lt;code&gt;s&lt;/code&gt; in the &lt;code&gt;Door s&lt;/code&gt;), we can &lt;em&gt;pattern match&lt;/em&gt; on the &lt;code&gt;SingDS s&lt;/code&gt; to &lt;em&gt;reveal&lt;/em&gt; what &lt;code&gt;s&lt;/code&gt; is, to the type checker. This is known as a &lt;strong&gt;dependent pattern match&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;code&gt;SingDS s&lt;/code&gt;'s pattern match goes down the &lt;code&gt;SOpened -&amp;gt;&lt;/code&gt; case, then we &lt;em&gt;know&lt;/em&gt; that &lt;code&gt;s ~ 'Opened&lt;/code&gt;[^eq]. We know that &lt;code&gt;s&lt;/code&gt; must be &lt;code&gt;'Opened&lt;/code&gt;, because &lt;code&gt;SOpened :: SingDS 'Opened&lt;/code&gt;, so there really isn't anything else the &lt;code&gt;s&lt;/code&gt; in &lt;code&gt;SingDS s&lt;/code&gt; could be!&lt;/p&gt;
&lt;p&gt;So, if we know that &lt;code&gt;s ~ 'Opened&lt;/code&gt;, that means that the &lt;code&gt;Door s&lt;/code&gt; is &lt;code&gt;Door 'Opened&lt;/code&gt;. So because &lt;code&gt;door :: Door' Opened&lt;/code&gt;, we have to &lt;code&gt;closeDoor&lt;/code&gt; it to get a &lt;code&gt;Door' Closed&lt;/code&gt;, and then &lt;code&gt;lockDoor&lt;/code&gt; it to get a &lt;code&gt;Door 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We say that &lt;code&gt;SOpened&lt;/code&gt; is a &lt;em&gt;runtime witness&lt;/em&gt; to &lt;code&gt;s&lt;/code&gt; being &lt;code&gt;'Opened&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- Note that &lt;code&gt;lockDoor . closeDoor&lt;/code&gt; will &lt;em&gt;only&lt;/em&gt; compile if given a &lt;code&gt;Door --&amp;gt; &amp;lt;!-- 'Opened&lt;/code&gt;, but because of our dependent pattern match, we &lt;em&gt;know&lt;/em&gt; we have a --&amp;gt; &amp;lt;!-- &lt;code&gt;Door 'Opened&lt;/code&gt;. --&amp;gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Same for the &lt;code&gt;SClosed -&amp;gt;&lt;/code&gt; branch -- since &lt;code&gt;SClosed :: SingDS 'Closed&lt;/code&gt;, then &lt;code&gt;s ~ 'Closed&lt;/code&gt;, so our &lt;code&gt;Door s&lt;/code&gt; must be a &lt;code&gt;Door 'Closed&lt;/code&gt;. This allows us to simply take our &lt;code&gt;door :: Door 'Closed&lt;/code&gt; and use &lt;code&gt;lockDoor&lt;/code&gt; to get a &lt;code&gt;Door 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- just write &lt;code&gt;SClosed -&amp;gt; lockDoor&lt;/code&gt;. --&amp;gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- Again, &lt;code&gt;lockDoor :: Door 'Closed -&amp;gt; Door 'Locked&lt;/code&gt;, so it would only work if --&amp;gt; &amp;lt;!-- given a &lt;code&gt;Door 'Closed&lt;/code&gt; -- which we know we have, because of the dependent --&amp;gt; &amp;lt;!-- pattern match. --&amp;gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the &lt;code&gt;SLocked -&amp;gt;&lt;/code&gt; branch, &lt;code&gt;SLocked :: SingDS 'Locked&lt;/code&gt;, so &lt;code&gt;s ~ 'Locked&lt;/code&gt;, so our &lt;code&gt;Door s&lt;/code&gt; is a &lt;code&gt;Door 'Locked&lt;/code&gt;. Our door is &amp;quot;already&amp;quot; locked, so we can just use the &lt;code&gt;door :: Door 'Locked&lt;/code&gt; that we got!&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- use &lt;code&gt;id :: Door 'Locked -&amp;gt; Door 'Locked&lt;/code&gt;. --&amp;gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- Note that &lt;code&gt;id :: Door 'Locked -&amp;gt; Door 'Locked&lt;/code&gt; would not work for any other --&amp;gt; &amp;lt;!-- branch, and would be a compile-time error. &lt;code&gt;id&lt;/code&gt; only works if you know your --&amp;gt; &amp;lt;!-- input is already &lt;code&gt;Door 'Locked&lt;/code&gt;...which we know because of the dependent --&amp;gt; &amp;lt;!-- pattern match. --&amp;gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, our singletons give us &lt;em&gt;runtime values&lt;/em&gt; that can be used as &lt;em&gt;witnesses&lt;/em&gt; for types and type variables. These values exist at runtime, so they &amp;quot;bypass&amp;quot; type erasure. Types themselves are directly erased, but we can hold on to them using these runtime tokens when we need them.&lt;/p&gt;
&lt;p&gt;Note that we can also write &lt;code&gt;lockAnyDoor&lt;/code&gt; using the &lt;em&gt;LambdaCase&lt;/em&gt; extension syntactic sugar, which I think offers a lot of extra insight:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell lockAnyDoor :: SingDS s -&amp;gt; (Door s -&amp;gt; Door 'Locked) lockAnyDoor = \case     SOpened -&amp;gt; lockDoor . closeDoor  -- in this branch, s is 'Opened     SClosed -&amp;gt; lockDoor              -- in this branch, s is 'Closed     SLocked -&amp;gt; id                    -- in this branch, s is 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here, we can see &lt;code&gt;lockAnyDoor sng&lt;/code&gt; as a partially applied function that returns a &lt;code&gt;Door s -&amp;gt; Door 'Locked&lt;/code&gt; For any &lt;code&gt;SingDS s&lt;/code&gt; you give to &lt;code&gt;lockAnyDoor&lt;/code&gt;, &lt;code&gt;lockAnyDoor&lt;/code&gt; returns a &amp;quot;locker function&amp;quot; (&lt;code&gt;Door s -&amp;gt; Door 'Locked&lt;/code&gt;) that is custom-made for your &lt;code&gt;SingDS&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;lockAnyDoor SOpened&lt;/code&gt; will return a &lt;code&gt;Door 'Opened -&amp;gt; Door 'Locked&lt;/code&gt;. Here, it has to give &lt;code&gt;lockDoor . closeDoor :: Door 'Opened -&amp;gt; Door 'Locked&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;lockAnyDoor SClosed&lt;/code&gt; will return a &lt;code&gt;Door 'Closed -&amp;gt; Door 'Locked&lt;/code&gt; -- namely &lt;code&gt;lockDoor :: Door 'Closed -&amp;gt; Door 'Locked&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;lockAnyDoor SLocked&lt;/code&gt; will return a &lt;code&gt;Door 'Locked -&amp;gt; Door 'Locked&lt;/code&gt;, which will just be &lt;code&gt;id :: Door 'Locked -&amp;gt; Door 'Locked&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that all of these functions will &lt;em&gt;only&lt;/em&gt; typecheck under the branch they fit in. If we gave &lt;code&gt;lockDoor&lt;/code&gt; for the &lt;code&gt;SOpened&lt;/code&gt; branch, or &lt;code&gt;id&lt;/code&gt; for the &lt;code&gt;SClosed&lt;/code&gt; branch, that'll be a compile-time error!&lt;/p&gt;
&lt;h4&gt;Reflection&lt;/h4&gt;
&lt;p&gt;Writing &lt;code&gt;doorStatus&lt;/code&gt; is now pretty simple --&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell doorStatus :: SingDS s -&amp;gt; Door s -&amp;gt; DoorState doorStatus SOpened _ = Opened doorStatus SClosed _ = Closed doorStatus SLocked _ = Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The benefit of the singleton again relies on the fact that the &lt;code&gt;s&lt;/code&gt; in &lt;code&gt;SingDS s&lt;/code&gt; is the same as the &lt;code&gt;s&lt;/code&gt; in &lt;code&gt;Door s&lt;/code&gt;, so if the user gives a &lt;code&gt;SingDS s&lt;/code&gt;, it &lt;em&gt;has&lt;/em&gt; to match the &lt;code&gt;s&lt;/code&gt; in the &lt;code&gt;Door s&lt;/code&gt; they give.&lt;/p&gt;
&lt;p&gt;Since we don't even care about the &lt;code&gt;door&lt;/code&gt;, we could also just write:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L37-40 fromSingDS :: SingDS s -&amp;gt; DoorState fromSingDS SOpened = Opened fromSingDS SClosed = Closed fromSingDS SLocked = Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which we can use to write a nicer &lt;code&gt;doorStatus&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L42-43 doorStatus :: SingDS s -&amp;gt; Door s -&amp;gt; DoorState doorStatus s _ = fromSingDS s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This process -- of turning a type variable (like &lt;code&gt;s&lt;/code&gt;) into a dynamic runtime value is known as &lt;strong&gt;reflection&lt;/strong&gt;. We move a value from the &lt;em&gt;type level&lt;/em&gt; to the &lt;em&gt;term level&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Recovering Implicit Passing&lt;/h3&gt;
&lt;p&gt;One downside is that we are required to manually pass in our witness. Wouldn't it be nice if we could have it be passed implicitly? We can actually leverage typeclasses to give us this ability:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L45-53 class SingDSI s where singDS :: SingDS s&lt;/p&gt;
&lt;p&gt;instance SingDSI 'Opened where singDS = SOpened instance SingDSI 'Closed where singDS = SClosed instance SingDSI 'Locked where singDS = SLocked ```&lt;/p&gt;
&lt;p&gt;(Note that &lt;em&gt;it's impossible&lt;/em&gt; to write our &lt;code&gt;SingDSI&lt;/code&gt; instances improperly! GHC checks to make sure that this is &lt;em&gt;correct&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;And so now we can do:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L55-59 lockAnyDoor_ :: SingDSI s =&amp;gt; Door s -&amp;gt; Door 'Locked lockAnyDoor_ = lockAnyDoor singDS&lt;/p&gt;
&lt;p&gt;doorStatus_ :: SingDSI s =&amp;gt; Door s -&amp;gt; DoorState doorStatus_ = doorStatus singDS ```&lt;/p&gt;
&lt;p&gt;Here, type inference will tell GHC that you want &lt;code&gt;singDS :: SingDS s&lt;/code&gt;, and it will pull out the proper singleton for the door you want to check!&lt;/p&gt;
&lt;p&gt;Now, we can call &lt;code&gt;lockAnyDoor_&lt;/code&gt; &lt;em&gt;without passing in&lt;/em&gt; a singleton, explicitly!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; let myDoor = UnsafeMkDoor @'Opened &amp;quot;Birch&amp;quot; ghci&amp;gt; :t lockAnyDoor SOpened myDoor -- our original method! Door 'Locked ghci&amp;gt; :t lockAnyDoor singDS myDoor  -- the power of type inference! Door 'Locked ghci&amp;gt; :t lockAnyDoor_ myDoor        -- no explicit singleton being passed! Door 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;The Same Power&lt;/h4&gt;
&lt;p&gt;In Haskell, a constraint &lt;code&gt;SingDSI s =&amp;gt;&lt;/code&gt; is essentially the same as passing in &lt;code&gt;SingDS s&lt;/code&gt; explicitly. Either way, you are passing in a runtime witness that your function can use. You can think of &lt;code&gt;SingDSI s =&amp;gt;&lt;/code&gt; as passing it in &lt;em&gt;implicitly&lt;/em&gt;, and &lt;code&gt;SingDS s -&amp;gt;&lt;/code&gt; as passing it in &lt;em&gt;explicitly&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, it's important to remember that &lt;code&gt;lockAnyDoor&lt;/code&gt; and &lt;code&gt;lockAnyDoor_&lt;/code&gt; are the &amp;quot;same function&amp;quot;, with the same power. They are just written in different styles -- &lt;code&gt;lockAnyDoor&lt;/code&gt; is written in explicit style, and &lt;code&gt;lockAnyDoor_&lt;/code&gt; is written in implicit style.&lt;/p&gt;
&lt;h4&gt;Going backwards&lt;/h4&gt;
&lt;p&gt;Going from &lt;code&gt;SingDSI s =&amp;gt;&lt;/code&gt; to &lt;code&gt;SingDS s -&amp;gt;&lt;/code&gt; (implicit to explicit) is very easy -- just use &lt;code&gt;singDS&lt;/code&gt; to get a &lt;code&gt;SingDS s&lt;/code&gt; if you have a &lt;code&gt;SingDSI s&lt;/code&gt; constraint available. This is what we did for &lt;code&gt;lockAnyDoor_&lt;/code&gt; and &lt;code&gt;doorStatus_&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Going from &lt;code&gt;SingDS s -&amp;gt;&lt;/code&gt; to &lt;code&gt;SingDSI s =&amp;gt;&lt;/code&gt; (explicit to implicit) in Haskell is actually a little trickier. The typical way to do this is with a CPS-like utility function:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L61-65 withSingDSI :: SingDS s -&amp;gt; (SingDSI s =&amp;gt; r) -&amp;gt; r withSingDSI sng x = case sng of     SOpened -&amp;gt; x     SClosed -&amp;gt; x     SLocked -&amp;gt; x&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;withSingDSI&lt;/code&gt; takes a &lt;code&gt;SingDS s&lt;/code&gt;, and a value (of type &lt;code&gt;r&lt;/code&gt;) that requires a &lt;code&gt;SingDSI s&lt;/code&gt; instance to be created. And it creates that value for you!&lt;/p&gt;
&lt;p&gt;To use &lt;code&gt;x&lt;/code&gt;, you must have a &lt;code&gt;SingDSI s&lt;/code&gt; instance available. This all works because in each branch, &lt;code&gt;s&lt;/code&gt; is now a &lt;em&gt;specific&lt;/em&gt;, monomorphic, &amp;quot;concrete&amp;quot; &lt;code&gt;s&lt;/code&gt;, and GHC knows that such an instance exists for every branch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the &lt;code&gt;SOpened&lt;/code&gt; branch, &lt;code&gt;s ~ 'Opened&lt;/code&gt;. We explicitly wrote an instance of &lt;code&gt;SingDSI&lt;/code&gt; for &lt;code&gt;'Opened&lt;/code&gt;, so GHC &lt;em&gt;knows&lt;/em&gt; that there is a &lt;code&gt;SingDSI 'Opened&lt;/code&gt; instance in existence, allowing you to use/create &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;In the &lt;code&gt;SClosed&lt;/code&gt; branch, &lt;code&gt;s ~ 'Closed&lt;/code&gt;, so GHC knows that there is a &lt;code&gt;SingDSI 'Closed&lt;/code&gt; instance (because we wrote one explicitly!), and gives &lt;em&gt;that&lt;/em&gt; to you -- and so you are allowed to use/create &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;In the &lt;code&gt;SLocked&lt;/code&gt; branch, &lt;code&gt;s ~ 'Locked&lt;/code&gt;, and because we wrote a &lt;code&gt;SingDSI 'Locked&lt;/code&gt; explicitly, we &lt;em&gt;know&lt;/em&gt; that a &lt;code&gt;SingDSI s&lt;/code&gt; instance is available, so we can use/create &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, we can run our implicit functions (like &lt;code&gt;lockAnyDoor_&lt;/code&gt;) by giving them explicit inputs:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L67-68 lockAnyDoor__ :: SingDS s -&amp;gt; Door s -&amp;gt; Door 'Locked lockAnyDoor__ s d = withSingDSI s (lockAnyDoor_ d)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And the cycle begins anew.&lt;/p&gt;
&lt;p&gt;One interesting thing to point out -- note that the type of &lt;code&gt;withSingDSI&lt;/code&gt; is very similar to the type of another common combinator:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell withSingDSI :: SingDS s -&amp;gt; (SingDSI s =&amp;gt; r) -&amp;gt; r flip  ($)   ::        a -&amp;gt; (        a -&amp;gt; r) -&amp;gt; r&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which is a bit of a testament to what we said earlier about how a &lt;code&gt;SingDSI s =&amp;gt; ..)&lt;/code&gt; is the same as &lt;code&gt;SingDS s -&amp;gt; ..&lt;/code&gt;. &lt;code&gt;flip ($)&lt;/code&gt; takes a value and a function and applies the function to that value. &lt;code&gt;withSingDSI&lt;/code&gt; takes a value and &amp;quot;something like a function&amp;quot; and applies the &amp;quot;something like a function&amp;quot; to that value.&lt;/p&gt;
&lt;h3&gt;Fun with Witnesses&lt;/h3&gt;
&lt;p&gt;We can write a nice version of &lt;code&gt;mkDoor&lt;/code&gt; using singletons:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs#L70-71 mkDoor :: SingDS s -&amp;gt; String -&amp;gt; Door s mkDoor _ = UnsafeMkDoor&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We take advantage of the fact that &lt;code&gt;SingDS s&lt;/code&gt; &amp;quot;locks in&amp;quot; the &lt;code&gt;s&lt;/code&gt; type variable for &lt;code&gt;Door s&lt;/code&gt;. We can call it now with values of &lt;code&gt;SingDS&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t mkDoor SOpened &amp;quot;Oak&amp;quot; Door 'Opened ghci&amp;gt; :t mkDoor SLocked &amp;quot;Spruce&amp;quot; Door 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now we can't do something silly like pass in &lt;code&gt;SLocked&lt;/code&gt; to get a &lt;code&gt;Door 'Opened&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;The Singletons Library&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;(The code for this post-singletons section is available &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs"&gt;on github&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now that we understand some of the benefits of singletons as they relate to phantom types, we can appreciate what the singletons &lt;em&gt;library&lt;/em&gt; has to offer: a fully unified, coherent system for working with singletons of almost &lt;em&gt;all&lt;/em&gt; Haskell types!&lt;/p&gt;
&lt;p&gt;First, there's Template Haskell for generating our singletons given our type:&lt;/p&gt;
&lt;p&gt;```haskell data DoorState = Opened | Closed | Locked deriving (Show, Eq)&lt;/p&gt;
&lt;p&gt;genSingletons [''DoorState]&lt;/p&gt;
&lt;p&gt;-- or -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs#L17-20 $(singletons [d| data DoorState = Opened | Closed | Locked deriving (Show, Eq) |]) ```&lt;/p&gt;
&lt;p&gt;This generates, for us:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- not the actual code, but essentially what happens data Sing :: DoorState -&amp;gt; Type where     SOpened :: Sing 'Opened     SClosed :: Sing 'Closed     SLocked :: Sing 'Locked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Sing&lt;/code&gt; is a poly-kinded type constructor (a &amp;quot;data family&amp;quot;). &lt;code&gt;STrue :: Sing 'True&lt;/code&gt; is the singleton for &lt;code&gt;'True&lt;/code&gt;, &lt;code&gt;SJust SOpened :: Sing ('Just 'Opened)&lt;/code&gt; is the singleton for &lt;code&gt;'Just 'Opened&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;It also generates us instances for &lt;code&gt;SingI&lt;/code&gt;, a poly-kinded typeclass:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell instance SingI 'Opened where     sing = SOpened instance SingI 'Closed where     sing = SClosed instance SingI 'Locked where     sing = SLocked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which is basically our &lt;code&gt;SingDSI&lt;/code&gt; typeclass, except we have instances for singletons of all kinds! (heh) There's a &lt;code&gt;SingI&lt;/code&gt; instance for &lt;code&gt;'True&lt;/code&gt;, a &lt;code&gt;SingI&lt;/code&gt; instance for &lt;code&gt;10&lt;/code&gt;, a &lt;code&gt;SingI&lt;/code&gt; instance for &lt;code&gt;'Just 'Opened&lt;/code&gt;, etc.:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; sing :: Sing 'True STrue ghci&amp;gt; sing :: Sing ('Just 'Opened) SJust SOpened&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We also have &lt;code&gt;withSingI&lt;/code&gt;, which is equivalent to our &lt;code&gt;withSingDSI&lt;/code&gt; function earlier.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell withSingI :: Sing s -&amp;gt; (forall r. SingI s =&amp;gt; r) -&amp;gt; r&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that if you have singletons for a kind &lt;code&gt;k&lt;/code&gt;, you also have instances for kind &lt;code&gt;Maybe k&lt;/code&gt;, as well. And also for &lt;code&gt;[k]&lt;/code&gt;, even! The fact that we have a unified way of working with and manipulating singletons of so many different types is a major advantage of using the &lt;em&gt;singletons&lt;/em&gt; library to manage your singletons instead of writing them yourself.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t SOpened `SCons` SClosed `SCons` SLocked `SCons` SNil Sing '[ 'Opened, 'Closed, 'Locked ] -- 'SCons is the singleton for `:` (cons), -- and 'SNil is the singleton for `[]` (nil)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(Remember that, because of &lt;code&gt;DataKinds&lt;/code&gt;, &lt;code&gt;Maybe&lt;/code&gt; is a kind constructor, who has two type constructors, the type &lt;code&gt;'Nothing&lt;/code&gt; and the type constructor &lt;code&gt;'Just :: k -&amp;gt; Maybe k&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Singletons for all kinds integrate together seamlessly, and you have mechanisms to generate them for your own type and roll it all into the system!&lt;/p&gt;
&lt;h3&gt;Extra Goodies&lt;/h3&gt;
&lt;p&gt;In addition to generating singletons for our libraries, it gives us convenient functions for working with the different &amp;quot;manifestations&amp;quot; of our types.&lt;/p&gt;
&lt;p&gt;Recall that &lt;code&gt;DoorState&lt;/code&gt; has four different things associated with it now:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;type&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt;, whose value constructors are &lt;code&gt;Opened&lt;/code&gt;, &lt;code&gt;Closed&lt;/code&gt;, and &lt;code&gt;Locked&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt;, whose type constructors are &lt;code&gt;'Opened&lt;/code&gt;, &lt;code&gt;'Closed&lt;/code&gt;, and &lt;code&gt;'Locked&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The singletons for &lt;code&gt;'Opened&lt;/code&gt;, &lt;code&gt;'Closed&lt;/code&gt;, and &lt;code&gt;'Locked&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell SOpened :: Sing 'Opened SClosed :: Sing 'Closed SLocked :: Sing 'Locked&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;SingI&lt;/code&gt; instances for &lt;code&gt;'Opened&lt;/code&gt;, &lt;code&gt;'Closed&lt;/code&gt;, and &lt;code&gt;'Locked'&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Kind of confusing, and in the future, when we have real dependent types, we can combine all of these manifestations into the &lt;em&gt;one&lt;/em&gt; thing. But for now, we do have to deal with converting between them, and for that, &lt;em&gt;singletons&lt;/em&gt; generates for us &lt;code&gt;fromSing :: Sing (s :: DoorState) -&amp;gt; DoorState&lt;/code&gt;. &lt;code&gt;fromSing&lt;/code&gt; takes us from singletons to term-level values (&lt;em&gt;reflection&lt;/em&gt;):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; fromSing SOpened Opened&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;It does this by defining a type class (actually, a &amp;quot;kind class&amp;quot;), &lt;code&gt;SingKind&lt;/code&gt;, associating each type to the corresponding datakinds-generated kind. The &lt;code&gt;SingKind&lt;/code&gt; instance for &lt;code&gt;DoorState&lt;/code&gt; links the &lt;em&gt;type&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt; to the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;DoorState&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The library also defines a neat type synonym, &lt;code&gt;type SDoorState = Sing&lt;/code&gt;, so you can do &lt;code&gt;SDoorState 'Opened&lt;/code&gt; instead of &lt;code&gt;Sing 'Opened&lt;/code&gt;, if you wish.&lt;/p&gt;
&lt;p&gt;There are definitely more useful utility functions, but we will investigate these later on in the series! For now, you can look at the &lt;a href="http://hackage.haskell.org/package/singletons/docs/Data-Singletons.html"&gt;documentation&lt;/a&gt; for the library to see more interesting utility functions!&lt;/p&gt;
&lt;h2&gt;The Singularity&lt;/h2&gt;
&lt;p&gt;In this post, at shortcomings in the usage of phantom types, and then saw how singletons could help us with these. Then, we looked at how the &lt;em&gt;singletons&lt;/em&gt; &lt;strong&gt;library&lt;/strong&gt; makes using this pattern extremely easy and smooth to integrate into your existing code.&lt;/p&gt;
&lt;p&gt;You can see all of the &amp;quot;manual singletons&amp;quot; code in this post &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/singletons/Door.hs"&gt;here&lt;/a&gt;, and then see the code re-implemented using the &lt;em&gt;singletons&lt;/em&gt; library &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, remember the question that I asked earlier, about creating a &lt;code&gt;Door&lt;/code&gt; with a given state that we don't know until runtime? So far, we are only able to create &lt;code&gt;Door&lt;/code&gt; and &lt;code&gt;SingDS&lt;/code&gt; from types we &lt;em&gt;know&lt;/em&gt; at compile-time. There is no way we have yet to convert a &lt;code&gt;DoorState&lt;/code&gt; from the value level to the type level -- so it seems that there is no way to &amp;quot;load&amp;quot; a &lt;code&gt;Door s&lt;/code&gt; with an &lt;code&gt;s&lt;/code&gt; that depends on, say, a file's contents, or user input. The fundamental issue is still &lt;em&gt;type erasure&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In Part 2, we will delve into how to overcome this and break through from the barrier of the dynamic &amp;quot;unsafe&amp;quot; runtime to the world of safe, typed, verified code, and see how the &lt;em&gt;singletons&lt;/em&gt; library gives us great tools for this. Afterwards, in Part 3, we will learn to express more complicated relationships with types and type-level functions using defunctionalization and the tools from the &lt;em&gt;singletons&lt;/em&gt; library, and finally break into the world of actual &amp;quot;type-level programming&amp;quot;.&lt;/p&gt;
&lt;p&gt;As always, let me know in the comments if you have any questions! You can also usually find me idling on the freenode &lt;code&gt;#haskell&lt;/code&gt; channel, as well, as &lt;em&gt;jle`&lt;/em&gt;. The &lt;em&gt;singletons&lt;/em&gt; &lt;a href="https://github.com/goldfirere/singletons/issues"&gt;issue tracker&lt;/a&gt; is also very active. Happy haskelling!&lt;/p&gt;
&lt;p&gt;For further reading, check out the &lt;a href="https://cs.brynmawr.edu/~rae/papers/2012/singletons/paper.pdf"&gt;original singletons paper&lt;/a&gt;! It's very readable and goes over many of the same techniques in this blog post, just written with a different perspective and tone :)&lt;/p&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Click on the links in the corner of the text boxes for solutions! (or just check out &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs"&gt;the source file&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;These should be written in the singletons library style, with &lt;code&gt;Sing&lt;/code&gt; instead of &lt;code&gt;SingDS&lt;/code&gt; and &lt;code&gt;SingI&lt;/code&gt; instead of &lt;code&gt;SingDSI&lt;/code&gt;. Review the &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs"&gt;singletons file&lt;/a&gt; for a comparison, if you are still unfamiliar.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Write a function to unlock a door, but only if the user enters an odd number (as a password).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs#L60-60 unlockDoor :: Int -&amp;gt; Door 'Locked -&amp;gt; Maybe (Door 'Closed)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;It should return a closed door in &lt;code&gt;Just&lt;/code&gt; if the caller gives an odd number, or &lt;code&gt;Nothing&lt;/code&gt; otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a function that can open any door, taking a password, in &amp;quot;implicit Sing&amp;quot; style:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/singletons/DoorSingletons.hs#L65-65 openAnyDoor :: SingI s =&amp;gt; Int -&amp;gt; Door s -&amp;gt; Maybe (Door 'Opened)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This should be written in terms of &lt;code&gt;unlockDoor&lt;/code&gt; and &lt;code&gt;openDoor&lt;/code&gt; (see above) -- that is, you &lt;strong&gt;should not&lt;/strong&gt; use &lt;code&gt;UnsafeMkDoor&lt;/code&gt; directly for &lt;code&gt;openAnyDoor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the door is already unlocked or opened, it should ignore the &lt;code&gt;Int&lt;/code&gt; input.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description><author>Justin Le</author><category>Haskell</category><guid isPermaLink="true">https://blog.jle.im/entry/introduction-to-singletons-1.html</guid><pubDate>Fri, 22 Dec 2017 18:42:07 UTC</pubDate><creator>Justin Le</creator><subject>Haskell</subject><date>2017-12-22</date></item><item><title>Hamiltonian Dynamics in Haskell</title><link>https://blog.jle.im/entry/hamiltonian-dynamics-in-haskell.html</link><description>&lt;p&gt;As promised in my &lt;a href="https://blog.jle.im/entry/introducing-the-hamilton-library.html"&gt;&lt;em&gt;hamilton&lt;/em&gt; introduction post&lt;/a&gt; (published almost exactly one year ago!), I'm going to go over implementing of the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hamilton"&gt;hamilton&lt;/a&gt;&lt;/em&gt; library using&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;DataKinds&lt;/em&gt; (with &lt;em&gt;TypeLits&lt;/em&gt;) to enforce sizes of vectors and matrices and help guide us write our code&lt;/li&gt;
&lt;li&gt;Statically-sized linear algebra with &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hmatrix"&gt;hmatrix&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Automatic differentiation with &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/ad"&gt;ad&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Statically-sized vectors with &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/vector-sized"&gt;vector-sized&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This post will be a bit heavy in some mathematics and Haskell concepts. The expected audience is intermediate Haskell programmers. Note that this is &lt;em&gt;not&lt;/em&gt; a post on dependent types, because dependent types (types that depend on runtime values) are not explicitly used.&lt;/p&gt;
&lt;p&gt;The mathematics and physics are &amp;quot;extra&amp;quot; flavor text and could potentially be skipped, but you'll get the most out of this article if you have basic familiarity with:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Basic concepts of multivariable calculus (like partial and total derivatives).&lt;/li&gt;
&lt;li&gt;Concepts of linear algebra (like dot products, matrix multiplication, and matrix inverses)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;No physics knowledge is assumed, but knowing a little bit of first semester physics would help you gain a bit more of an appreciation for the end result!&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://blog.jle.im/entry/introducing-the-hamilton-library.html"&gt;hamilton library introduction&lt;/a&gt; should be considered a &amp;quot;soft prerequisite&amp;quot; for this post, as it presents motivations, visual demonstrations, and general overviews of the methods presented here!&lt;/p&gt;
&lt;h2&gt;The Goal&lt;/h2&gt;
&lt;p&gt;At the end of this, we should be able to have Haskell &lt;em&gt;automatically generate&lt;/em&gt; &lt;strong&gt;equations of motions&lt;/strong&gt; for any arbitrary system described in arbitrary coordinate systems, and simulate that system.&lt;/p&gt;
&lt;p&gt;Normally, we'd describe a system using particles' x and y coordinates, but our goal is to be able to describe our particles' positions using any coordinate system we want (polar, distance-along-a-curved-rail, pendulum-angles, etc.) and have Haskell automatically generate equations of motions and time progressions of those coordinates.&lt;/p&gt;
&lt;p&gt;Read &lt;a href="https://blog.jle.im/entry/introducing-the-hamilton-library.html"&gt;my hamilton library introduction&lt;/a&gt; for more information and examples!&lt;/p&gt;
&lt;h2&gt;Hamiltonian Mechanics&lt;/h2&gt;
&lt;p&gt;As mentioned in the previous post, Hamiltonian mechanics is a re-imagining of dynamics and mechanics (think &amp;quot;the world post-$F = m a$&amp;quot;) that not only opened up new doors to solving problems in classical, but also ended up being the right angle of viewing the world to unlock statistical mechanics and thermodynamics, and later even quantum mechanics.&lt;/p&gt;
&lt;p&gt;Hamiltonian mechanics lets you parameterize your system's &amp;quot;position&amp;quot; in arbitrary ways (like the angle of rotation, for pendulum problems) and then posits that the full state of the system exists in something called &lt;em&gt;phase space&lt;/em&gt;, and that the system's dynamics is its motion through phase space that is dictated by the geometry of the &lt;em&gt;Hamiltonian&lt;/em&gt; of that phase space.&lt;/p&gt;
&lt;p&gt;The system's &lt;em&gt;Hamiltonian&lt;/em&gt; is a $\mathbb{R}^{2n} \rightarrow \mathbb{R}$ function from a point in $\mathbb{R}^{2n}$ phase space (where $n$ is the number of coordinates parameterizing your system) to a scalar in $\mathbb{R}$. For a time-independent system, the picture of the dynamics is pretty simple: the system moves along the &lt;em&gt;contour lines&lt;/em&gt; of the &lt;em&gt;Hamiltonian&lt;/em&gt; -- the lines of equal &amp;quot;height&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/img/entries/hamilton/contour-lines.jpg" title="Contour lines" alt="Example of contour lines of a $\mathbb{R}^2 \rightarrow \mathbb{R}$ function -- the elevation of land, from the Ordinace Survey website." /&gt;&lt;/p&gt;
&lt;p&gt;In the example above, if we imagine that phase space is the 2D location, then the &lt;em&gt;Hamiltonian&lt;/em&gt; is the mountain. And for a system dropped anywhere on the mountain, its motion would be along the contour lines. For example, if a system started somewhere along the 10 contour line, it would begin to oscillate the entire phase space along the 10 contour line.[^time-dependent]&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Every&lt;/em&gt; &lt;a href="https://www.youtube.com/watch?v=izGwDsrQ1eQ"&gt;smooth&lt;/a&gt; $\mathbb{R}^{2n} \rightarrow \mathbb{R}$ function on phase space can be used as a Hamiltonian to describe the physics of some system. So, given any &amp;quot;mountain range&amp;quot; on phase space, any &amp;quot;elevation map&amp;quot; or real-valued function on phase space, you can treat it as a description of the dynamics of some physical system.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;trick&lt;/em&gt;, then, to using Hamiltonian dynamics to model your system, is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Finding the phase space to describe your system. This can be done based on any continuous parameterization of your system (&amp;quot;generalized coordinates&amp;quot;), like angles of pendulums and so on.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finding the Hamiltonian on that phase space to describe your system.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And then Hamilton's dynamics will give you the rest! All you do is &amp;quot;follow the contour lines&amp;quot; on that Hamiltonian!&lt;/p&gt;
&lt;h3&gt;Phase Space&lt;/h3&gt;
&lt;p&gt;Hamiltonian dynamics are about systems moving around in phase space. It seems that phase space is the &amp;quot;room where it happens&amp;quot;, so to speak, so let's dig deeper into what it is. &lt;em&gt;Phase space&lt;/em&gt; is a $2n$-dimensional space parameterized by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All of the current values of the $n$ parameters (&amp;quot;generalized coordinates&amp;quot;)&lt;/li&gt;
&lt;li&gt;All of the current &amp;quot;generalized momenta&amp;quot; of those $n$ parameters&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So if you were parameterizing your pendulum system by, say, the angle of the pendulum, then a point in phase space would be the current angle of the pendulum along with the current &amp;quot;generalized momentum&amp;quot; associated with the angle of the pendulum. What exactly &lt;em&gt;is&lt;/em&gt; generalized momentum? We'll go over calculating it eventually, but what does it represent...&lt;em&gt;physically&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;The deeper answer involves the underlying Lie algebra of the Lie group associated with the generalized coordinates, but going into that would make this a completely different post. What I &lt;em&gt;can&lt;/em&gt; say is that the generalized momenta associated with (&amp;quot;conjugate to&amp;quot;) certain sets of familiar coordinates yield things that we typically call &amp;quot;momenta&amp;quot;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The momentum conjugate to normal Cartesian coordinates is just our normal run-of-the-mill &lt;em&gt;linear momentum&lt;/em&gt; (in the $\mathbf{p} = m \mathbf{v}$) from first semester physics.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The momentum conjugate to the angle $\theta$ in polar coordinates is &lt;em&gt;angular momentum&lt;/em&gt; ($L = m r^2 \dot{\theta}$) from first semester physics.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The momentum conjugate to the radial coordinate $r$ in polar coordinates is also just boring old linear momentum $p_r = m \dot{r}$, which makes sense because purely radial motion is just linear motion.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, it's our normal momentum (for linear and polar coordinates) &lt;em&gt;generalized&lt;/em&gt; to arbitrary coordinates.&lt;/p&gt;
&lt;h3&gt;Hamiltonian Dynamics&lt;/h3&gt;
&lt;p&gt;I've explained Hamiltonian dynamics for time-independent Hamiltonians as &amp;quot;follow the contour lines&amp;quot;. If you remember your basic multi-variable calculus course, you'll know that the line of &amp;quot;steepest ascent&amp;quot; is the gradient. If we call the Hamiltonian $\mathcal{H}(\mathbf{q},\mathbf{p})$ (where $\mathbf{q}$ is the vector of positions and $\mathbf{p}$ is the vector of momenta), then the direction of steepest ascent is&lt;/p&gt;
&lt;p&gt;$$ \left \langle \frac{\partial}{\partial \mathbf{q}} \mathcal{H}(\mathbf{q},\mathbf{p}), \frac{\partial}{\partial \mathbf{p}} \mathcal{H}(\mathbf{q},\mathbf{p}) \right \rangle $$&lt;/p&gt;
&lt;p&gt;But we want to move along the &lt;em&gt;contour lines&lt;/em&gt;...and these are the lines &lt;em&gt;perpendicular&lt;/em&gt; to the direction of steepest descent. The vector perpendicular to $\langle x, y \rangle$ is $\langle y, -x \rangle$,[^perp] so we just derived the actual Hamiltonian equations of motion: just move in the direction perpendicular to the steepest ascent! That is, to have things move on contour lines, $\dot{q}$ and $\dot{p}_q$ &lt;em&gt;should&lt;/em&gt; be:&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned} \dot{q} &amp;amp; = \frac{\partial}{\partial p&lt;em&gt;q} \mathcal{H}(\mathbf{q},\mathbf{p}) \ \dot{p}&lt;/em&gt;q &amp;amp; = - \frac{\partial}{\partial q} \mathcal{H}(\mathbf{q},\mathbf{p}) \end{aligned} $$&lt;/p&gt;
&lt;p&gt;This is a conclusion with one generalized coordinate $q$, but we can generalize this to systems with multiple coordinates as well, as long as this holds for &lt;em&gt;every&lt;/em&gt; $q$ and the momentum conjugate to it ($p&lt;em&gt;q$). (For the rest of this post, $\mathbf{q}$ refers to the vector of coordinates, $q$ refers to a single specific coordinate, and $p&lt;/em&gt;q$ refers to the momentum conjugate to that coordinate).&lt;/p&gt;
&lt;p&gt;Essentially, these give you &amp;quot;updating functions&amp;quot; for $q$ and $p&lt;em&gt;q$ -- given $\mathcal{H}(\mathbf{q},\mathbf{p})$, you have a way to &amp;quot;update&amp;quot; the particle's position in phase space. Just take the partial derivatives of $\mathcal{H}$ at every step in time! To update $q$, nudge it by $\frac{\partial}{\partial p&lt;/em&gt;q} \mathcal{H}(\mathbf{q},\mathbf{p})$. To update $p_q$, nudge it by $-\frac{\partial}{\partial q} \mathcal{H}(\mathbf{q},\mathbf{p})$!&lt;/p&gt;
&lt;p&gt;This picture is appealing to me in a visceral way because it sort of seems like the system is &amp;quot;surfing&amp;quot; along the Hamiltonian's contour lines. It's being &amp;quot;pushed&amp;quot; &lt;em&gt;faster&lt;/em&gt; when the Hamiltonian is steeper, and slower when it's more shallow. I can apply all my intuition as a surfer[^surfer] to Hamiltonian mechanics!&lt;/p&gt;
&lt;h2&gt;Hamiltonian Dynamics and Physical Systems&lt;/h2&gt;
&lt;p&gt;Earlier I mentioned that the two steps for applying Hamiltonian mechanics to your system was figuring out your system's conjugate momenta and the appropriate Hamiltonian. To explain this, I'm going to make a couple of simplifying assumptions that make the job easier for the purposes of this article:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Your coordinates and potential energy are time-independent.&lt;/li&gt;
&lt;li&gt;Your potential energy function only depends on &lt;em&gt;positions&lt;/em&gt;, and not &lt;em&gt;velocities&lt;/em&gt;. (So nothing like friction or wind resistance or magnetic field vector potentials)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these assumptions, I'm going to skip over discussing the &lt;a href="https://en.wikipedia.org/wiki/Lagrangian_mechanics"&gt;Lagrangian&lt;/a&gt; of the system, which is the traditional way to do this. You can think of this section as me presenting derived conclusions and skipping the derivations.&lt;/p&gt;
&lt;h3&gt;Conjugate Momenta&lt;/h3&gt;
&lt;p&gt;For systems with velocity-independent potential energies, it can be shown that the momentum conjugate to coordinate $q$ is&lt;/p&gt;
&lt;p&gt;$$ p_q = \frac{\partial}{\partial \dot{q}} KE(\mathbf{q}, \dot{\mathbf{q}}) $$&lt;/p&gt;
&lt;p&gt;Where $KE(\mathbf{q},\dot{\mathbf{q}})$ is the kinetic energy of the system, which is a function on the coordinates $\mathbf{q}$ and their rates of change, $\dot{\mathbf{q}}$. For example, for normal Cartesian coordinates in one dimension, $KE(x, \dot{x}) = \frac{1}{2} m \dot{x}^2$. So the momentum conjugate to $x$ is:&lt;/p&gt;
&lt;p&gt;$$ p_x = \frac{\partial}{\partial \dot{x}} \left[ \frac{1}{2} m \dot{x}^2 \right] = m \dot{x} $$&lt;/p&gt;
&lt;p&gt;Just linear momentum, like I claimed before.&lt;/p&gt;
&lt;p&gt;Let's generalize this to arbitrary coordinates. In general, for &lt;em&gt;Cartesian&lt;/em&gt; coordinates, the kinetic energy will always be&lt;/p&gt;
&lt;p&gt;$$ KE(\mathbf{x}, \dot{\mathbf{x}}) = \frac{1}{2} \left[ m&lt;em&gt;1 \dot{x}&lt;/em&gt;1^2 + m&lt;em&gt;2 \dot{x}&lt;/em&gt;2^2 + m&lt;em&gt;3 \dot{x}&lt;/em&gt;3^2 + \dots \right] $$&lt;/p&gt;
&lt;p&gt;Where $m$ is the inertia associated with each coordinate...for example, if $\langle x&lt;em&gt;1, x&lt;/em&gt;2 \rangle$ describes the location of an object of mass $m$, then $m&lt;em&gt;1 = m&lt;/em&gt;2 = m$.&lt;/p&gt;
&lt;p&gt;To give us nice notation and make things more convenient, we'll write this as a quadratic form over an inertia matrix:&lt;/p&gt;
&lt;p&gt;$$ KE(\dot{\mathbf{x}}) = \frac{1}{2} \dot{\mathbf{x}}^T \hat{M} \dot{\mathbf{x}} $$&lt;/p&gt;
&lt;p&gt;Where $\hat{M}$ is the &lt;a href="https://en.wikipedia.org/wiki/Diagonal_matrix"&gt;diagonal matrix&lt;/a&gt; whose entries are the masses of each coordinate, and $\dot{\mathbf{x}}$ is the column vector of all of the (Cartesian) coordinates, $\left[ \dot{x}&lt;em&gt;1\, \dot{x}&lt;/em&gt;2\, \dot{x}_3\, \dots \right]^T$.&lt;/p&gt;
&lt;p&gt;Now! How to generalize this to arbitrary coordinates? Well, if we have $n$ generalized coordinates $\mathbf{q}$ mapping to $m$-dimensional Cartesian coordinates, we can specify them as $\mathbf{x} = f(\mathbf{q})$, where $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$, taking the vector of generalized coordinates and returning a vector for the position in Cartesian space. For example, for polar coordinates, $f(r, \theta) = \left \langle r \cos(\theta), r \sin(\theta) \right \rangle$, because, for polar coordinates, $x = r \cos(\theta)$ and $y = r \sin(\theta)$.&lt;/p&gt;
&lt;p&gt;So we can get $\mathbf{x}$ from $\mathbf{q}$ with $f$, but how can we get $\dot{\mathbf{x}}$, the vector of rate of changes? Well, if $x&lt;em&gt;1 = f&lt;/em&gt;1(q&lt;em&gt;1, q&lt;/em&gt;2, q&lt;em&gt;3 \dots)$, then the $\dot{x}&lt;/em&gt;1$ is the &lt;a href="https://en.wikipedia.org/wiki/Total_derivative"&gt;total derivative&lt;/a&gt; of $x_1$ with respect to time:&lt;/p&gt;
&lt;p&gt;$$ \dot{x}&lt;em&gt;1 = \frac{\partial f&lt;/em&gt;1}{\partial q&lt;em&gt;1} \dot{q}&lt;/em&gt;1 + \frac{\partial f&lt;em&gt;1}{\partial q&lt;/em&gt;2} \dot{q}&lt;em&gt;2 + \frac{\partial f&lt;/em&gt;1}{\partial q&lt;em&gt;3} \dot{q}&lt;/em&gt;3 + \dots $$&lt;/p&gt;
&lt;p&gt;Or, in short:&lt;/p&gt;
&lt;p&gt;$$ \dot{x}&lt;em&gt;i = \sum&lt;/em&gt;{j = 1}^n \frac{\partial f&lt;em&gt;i}{\partial q&lt;/em&gt;j} \dot{q}_j $$&lt;/p&gt;
&lt;p&gt;But, hey, this looks a lot like a matrix-vector multiplication! If we make $\hat{J}&lt;em&gt;f$, an $m \times n$ matrix of partial derivatives of $f$ ($\hat{J}&lt;/em&gt;{fij} = \frac{\partial f&lt;em&gt;i}{\partial q&lt;/em&gt;j}$) at a given point (typically called the &lt;a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"&gt;Jacobian matrix of f&lt;/a&gt;, then we have a nice expression for $\dot{\mathbf{x}}$:&lt;/p&gt;
&lt;p&gt;$$ \dot{\mathbf{x}} = \hat{J}_f \dot{\mathbf{q}} $$&lt;/p&gt;
&lt;p&gt;And we can plug it in (remembering that $(A B)^T = B^T A^T$) to our kinetic energy equation to get:&lt;/p&gt;
&lt;p&gt;$$ KE(\mathbf{q},\dot{\mathbf{q}}) = \frac{1}{2} \dot{\mathbf{q}}^T \hat{J}&lt;em&gt;f^T \hat{M} \hat{J}&lt;/em&gt;f \dot{\mathbf{q}} $$&lt;/p&gt;
&lt;p&gt;And for the final step, we differentiate with respect to the $\dot{q}$s (which is just the gradient $\nabla_{\dot{\mathbf{q}}}$) to get $\mathbf{p}$, the vector of conjugate momenta:&lt;/p&gt;
&lt;p&gt;$$ \mathbf{p} = \nabla&lt;em&gt;{\dot{\mathbf{q}}} \left[ \frac{1}{2} \dot{\mathbf{q}}^T \hat{J}&lt;/em&gt;f^T \hat{M} \hat{J}&lt;em&gt;f \dot{\mathbf{q}} \right] = \hat{J}&lt;/em&gt;f^T \hat{M} \hat{J}_f \dot{\mathbf{q}} $$&lt;/p&gt;
&lt;p&gt;Now, we're going to be using $\hat{J}&lt;em&gt;f^T \hat{M} \hat{J}&lt;/em&gt;f$ a lot, so let's give it a name, $\hat{K}$. $\hat{K}$ represents some sort of coordinate-aware inertia term for our system. If the masses are all positive and $\hat{J}&lt;em&gt;f$ is full-rank[^full-rank], then $\hat{K}$ is a symmetric, positive-definite, invertible matrix (by construction). It's important to also remember that it's an explicit function of $\mathbf{q}$, because $\hat{J}&lt;/em&gt;f$ is a matrix of partial derivatives at a given $\mathbf{q}$. We now have a simple expression for the vector of conjugate momenta ($\mathbf{p} = \hat{K} \dot{\mathbf{q}}$), and also for kinetic energy ($KE = \frac{1}{2} \dot{\mathbf{q}}^T \hat{K} \dot{\mathbf{q}}$).&lt;/p&gt;
&lt;p&gt;It's going to be important for us to also be able to go backwards (to get $\dot{\mathbf{q}}$ from $\mathbf{p}$). Luckily, because we wrote the whole thing as a matrix operation, going backwards is easy -- just take the matrix inverse, which we know exists!&lt;/p&gt;
&lt;p&gt;$$ \dot{\mathbf{q}} = \hat{K}^{-1} \mathbf{p} $$&lt;/p&gt;
&lt;p&gt;The power of linear algebra!&lt;/p&gt;
&lt;h3&gt;Hamiltonians of Physical Systems&lt;/h3&gt;
&lt;p&gt;Ok, that's step one. How about step two -- finding the Hamiltonian for your system?&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;real&lt;/em&gt; Hamiltonian is actually the &lt;a href="https://en.wikipedia.org/wiki/Poisson_bracket"&gt;Poisson bracket&lt;/a&gt; of the system's &lt;a href="https://en.wikipedia.org/wiki/Lagrangian_mechanics"&gt;Lagrangian&lt;/a&gt;, but I did some of the work for you for the case of time-independent coordinates where the potential energy depends &lt;em&gt;only&lt;/em&gt; on positions (so, no friction, wind resistance, time, etc.). In such a case, the Hamiltonian of a system is precisely the system's total &lt;a href="https://en.wikipedia.org/wiki/Mechanical_energy"&gt;mechanical energy&lt;/a&gt;, or its kinetic energy plus the potential energy:&lt;/p&gt;
&lt;p&gt;$$ \mathcal{H}(\mathbf{q},\mathbf{p}) = KE(\mathbf{q},\mathbf{p}) + PE(\mathbf{q}) $$&lt;/p&gt;
&lt;p&gt;Which makes a lot of intuitive sense, because you might recall that total mechanical energy is always conserved for certain types of systems. Incidentally, Hamiltonian dynamics makes sure that the value of the system's Hamiltonian stays the same (because it moves along contour lines). So, the system's Hamiltonian always stays the same, and so its total mechanical energy stays the same, as well! Energy is conserved because the Hamiltonian stays the same!&lt;/p&gt;
&lt;p&gt;Anyway, we want to build our system's Hamiltonian from properties of the coordinate system, so plugging in our expression for $KE$, we get $\mathcal{H}(\mathbf{q},\dot{\mathbf{q}}) = \frac{1}{2} \dot{\mathbf{q}}^T \hat{K} \dot{\mathbf{q}} + PE(\mathbf{q})$.&lt;/p&gt;
&lt;p&gt;Oh, but oops, the Hamiltonian has to be a function of $\mathbf{p}$, not of $\dot{\mathbf{q}}$. Let's remember that $\dot{\mathbf{q}} = \hat{K}^{-1} \mathbf{p}$ and find the final form of our Hamiltonian (after a bit of simplification, remembering that the inverse of a symmetric matrix is also symmetric):&lt;/p&gt;
&lt;p&gt;$$ \mathcal{H}(\mathbf{q},\mathbf{p}) = \frac{1}{2} \mathbf{p}^T \hat{K}^{-1} \mathbf{p} + PE(\mathbf{q}) $$&lt;/p&gt;
&lt;h3&gt;Hamiltonian Equations&lt;/h3&gt;
&lt;p&gt;We got our Hamiltonian! Now just to find our updating functions (the partial derivatives of the Hamiltonian), and we're done with the math.&lt;/p&gt;
&lt;p&gt;Because we are assuming the case (with loss of generality) $PE$ doesn't depend on $\mathbf{p}$, the partial derivatives of $\mathcal{H}$ with respect to $\mathbf{p}$ is:&lt;/p&gt;
&lt;p&gt;$$ \nabla_{\mathbf{p}} \mathcal{H}(\mathbf{q},\mathbf{p}) = \hat{K}^{-1} \mathbf{p} $$&lt;/p&gt;
&lt;p&gt;We already can calculate $\hat{K}^{-1}$, so this wound up being easy peasy. But finding the partial derivatives with respect to $\mathbf{q}$ is a little trickier. The gradient is a linear operator, so we can break that down to just finding the gradient of the $KE$ term $\frac{1}{2} \mathbf{p}^T \hat{K}^{-1} \mathbf{p}$. Because $\mathbf{p}$ is an independent input to $\mathcal{H}$, we can just look at the gradient of $\hat{K}^{-1}$. We can simplify that even more by realizing that for any invertible matrix $A$, $\frac{\partial}{\partial q} A^{-1} = - A^{-1} \left[ \frac{\partial}{\partial q} A \right] A^{-1}$, so now we just need to find the partial derivatives of $\hat{K}$, or $\hat{J}&lt;em&gt;f^T \hat{M} \hat{J}&lt;/em&gt;f}$. $\hat{M}$ is a constant term, so, using the good ol' product rule over $\hat{J}&lt;em&gt;f^T$ and $\hat{J}&lt;/em&gt;f$, we see that, after some simplification:&lt;/p&gt;
&lt;p&gt;$$ \frac{\partial}{\partial q&lt;em&gt;i} \left[ \hat{J}&lt;/em&gt;f^T \hat{M} \hat{J}&lt;em&gt;f \right] = 2 \hat{J}&lt;/em&gt;f^T \hat{M} \left[ \frac{\partial}{\partial q&lt;em&gt;i} \hat{J}&lt;/em&gt;f \right] $$&lt;/p&gt;
&lt;p&gt;$\frac{\partial}{\partial q&lt;em&gt;i} \hat{J}&lt;/em&gt;f$ (an $m \times n$ matrix, like $\hat{J}&lt;em&gt;f$) represents the &lt;em&gt;second derivatives&lt;/em&gt; of $f$ -- the derivative (with respect to $q&lt;/em&gt;i$) of the derivatives.&lt;/p&gt;
&lt;p&gt;The collection of &amp;quot;second-order derivatives of $f$&amp;quot; is known as the &lt;a href="https://en.wikipedia.org/wiki/Hessian_matrix#Vector-valued_functions"&gt;Hessian Tensor&lt;/a&gt; (a vector-valued generalization of the Hessian matrix), which we will denote as $\hat{H}_f$.[^edwardk] We can write this in a nicer way by abusing matrix multiplication notation to get&lt;/p&gt;
&lt;p&gt;$$ \nabla&lt;em&gt;{\mathbf{q}} \left[ \hat{J}&lt;/em&gt;f^T \hat{M} \hat{J}&lt;em&gt;f \right] = 2 \hat{J}&lt;/em&gt;f^T \hat{M} \hat{H}_f $$&lt;/p&gt;
&lt;p&gt;if we use $\hat{H}&lt;em&gt;f$ as an $n \times m \times n$ tensor, whose $n$ components are the each the $m \times n$ matrices corresponding to $\frac{\partial}{\partial q&lt;/em&gt;i} \hat{J}_f$&lt;/p&gt;
&lt;p&gt;And with that, we have our final expression for $\nabla_{\mathbf{q}} \mathcal{H}(\mathbf{q},\mathbf{p})$:&lt;/p&gt;
&lt;p&gt;$$ \frac{\partial}{\partial q&lt;em&gt;i} \mathcal{H}(\mathbf{q},\mathbf{p}) = - \mathbf{p}^T \hat{K}^{-1} \hat{J}&lt;/em&gt;f^T \hat{M} \left[ \frac{\partial}{\partial q&lt;em&gt;i} \hat{J}&lt;/em&gt;f \right] \hat{K}^{-1} \mathbf{p} + \nabla_{\mathbf{q}} PE(\mathbf{q}) $$&lt;/p&gt;
&lt;p&gt;Or, to use our abuse of notation:&lt;/p&gt;
&lt;p&gt;$$ \nabla&lt;em&gt;{\mathbf{q}} \mathcal{H}(\mathbf{q},\mathbf{p}) = - \mathbf{p}^T \hat{K}^{-1} \hat{J}&lt;/em&gt;f^T \hat{M} \hat{H}&lt;em&gt;f \hat{K}^{-1} \mathbf{p} + \nabla&lt;/em&gt;{\mathbf{q}} PE(\mathbf{q}) $$&lt;/p&gt;
&lt;p&gt;And, finally, we have everything we need -- we can now construct our equations of motion! To progress through phase space ($\langle \mathbf{q}, \mathbf{p}\rangle$):&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned} \dot{\mathbf{q}} &amp;amp; = \nabla&lt;em&gt;{\mathbf{p&lt;/em&gt;q}} \mathcal{H}(\mathbf{q},\mathbf{p}) &amp;amp;&amp;amp; = \hat{K}^{-1} \mathbf{p} \ \dot{\mathbf{p}} &amp;amp; = - \nabla&lt;em&gt;{\mathbf{q}} \mathcal{H}(\mathbf{q},\mathbf{p}) &amp;amp;&amp;amp; = \mathbf{p}^T \hat{K}^{-1} \hat{J}&lt;/em&gt;f^T \hat{M} \hat{H}&lt;em&gt;f \hat{K}^{-1} \mathbf{p} - \nabla&lt;/em&gt;{\mathbf{q}} PE(\mathbf{q}) \end{aligned} $$&lt;/p&gt;
&lt;p&gt;That's it. We're done. Have a nice day, thanks for reading!&lt;/p&gt;
&lt;h2&gt;The Haskell&lt;/h2&gt;
&lt;p&gt;Just kidding, now it's time for the fun stuff :)&lt;/p&gt;
&lt;p&gt;Our final goal is to be able to simulate a &lt;em&gt;system of discrete particles&lt;/em&gt; through &lt;em&gt;arbitrary generalized coordinates&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To simplify the math, we always assume that, whatever generalized coordinates you are using ($\mathbb{R}^n$), your system &amp;quot;actually&amp;quot; exists in some real flat Cartesian coordinate system ($\mathbb{R}^m$). This allows us to take advantage of all of that math we derived in the previous section.&lt;/p&gt;
&lt;p&gt;So, in order to fully describe the system, we need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each of their masses (or inertias) in their underlying $m$ Cartesian coordinates, which we'll call $\mathbf{m}$.&lt;/li&gt;
&lt;li&gt;A function $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$ to convert the generalized coordinates ($\mathbb{R^n}$) to Cartesian coordinates ($\mathbb{R}^m$)&lt;/li&gt;
&lt;li&gt;The potential energy function $U : \mathbb{R}^n \rightarrow \mathbb{R}$ in the generalized coordinates ($\mathbb{R^n}$)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From these alone, we can derive the equations of motion for the particles in phase space as a system of first-order ODEs using the process described above. Then, given an initial phase space position, we can do numeric integration to simulate our system's motion through phase space. To &amp;quot;surf the Hamiltonian waves in phase space&amp;quot;, so to speak.&lt;/p&gt;
&lt;p&gt;But, to be explicit, we also are going to need some derivatives for these functions/vectors, too. If you've been following along, the full enumeration of functions and vectors we need is:&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned} \mathbf{m} &amp;amp; : \mathbb{R}^m \ f &amp;amp; : \mathbb{R}^n \rightarrow \mathbb{R}^m \ \hat{J}&lt;em&gt;f &amp;amp; : \mathbb{R}^n \rightarrow \mathbb{R}^{m \times n} \ \hat{H}&lt;/em&gt;f &amp;amp; : \mathbb{R}^n \rightarrow \mathbb{R}^{n \times m \times n} \ U &amp;amp; : \mathbb{R}^n \rightarrow \mathbb{R} \ \nabla_{\mathbf{q}} U &amp;amp; : \mathbb{R}^n \rightarrow \mathbb{R}^n \end{aligned} $$&lt;/p&gt;
&lt;p&gt;But, as we'll see, with libraries like &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/ad"&gt;ad&lt;/a&gt;&lt;/em&gt; in Haskell, we can really just ask the user for $\mathbf{m}$, $f$, and $U$ -- all of the derivatives can be computed automatically.&lt;/p&gt;
&lt;h3&gt;Our Data Structures&lt;/h3&gt;
&lt;p&gt;We can couple together all of these functions in a data type that fully describes the physics of our systems (the &amp;quot;shape&amp;quot; of the Hamiltonian):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L25-32 data System m n = System     { sysInertia       :: R m                         -- ^ 'm' vector     , sysCoords        :: R n -&amp;gt; R m                  -- ^ f     , sysJacobian      :: R n -&amp;gt; L m n                -- ^ J_f     , sysHessian       :: R n -&amp;gt; V.Vector n (L m n)   -- ^ H_f     , sysPotential     :: R n -&amp;gt; Double               -- ^ U     , sysPotentialGrad :: R n -&amp;gt; R n                  -- ^ grad U     }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R n&lt;/code&gt; and &lt;code&gt;L m n&lt;/code&gt; are from the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hmatrix"&gt;hmatrix&lt;/a&gt;&lt;/em&gt; library; an &lt;code&gt;R n&lt;/code&gt; represents an n-vector (For example, an &lt;code&gt;R 4&lt;/code&gt; is a 4-vector), and an &lt;code&gt;L m n&lt;/code&gt; represents an &lt;code&gt;m x n&lt;/code&gt; matrix (For example, an &lt;code&gt;L 5 3&lt;/code&gt; is a 5x3 matrix).&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;System m n&lt;/code&gt; will describe a system parameterized by &lt;code&gt;n&lt;/code&gt; generalized coordinates, taking place in an underlying &lt;code&gt;m&lt;/code&gt;-dimensional Cartesian space.&lt;/p&gt;
&lt;p&gt;It'll also be convenient to have a data type to describe the state of our system in terms of its generalized positions ($\mathbf{q}$) and generalized velocities (the rates of changes of these positions, $\dot{\mathbf{q}}$), which is sometimes called &amp;quot;configuration space&amp;quot;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L35-39 data Config n = Config     { confPositions  :: R n     , confVelocities :: R n     }   deriving Show&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And, more importantly, remember that Hamiltonian dynamics is all about surfing around on that phase space (generalized positions $\mathbf{q}$ and their conjugate momenta, $\mathbf{p_q}$). So let's make a type to describe the state of our system in phase space:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L42-46 data Phase n = Phase     { phasePositions :: R n     , phaseMomenta   :: R n     }   deriving Show&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Getting comfortable with our data types&lt;/h3&gt;
&lt;p&gt;First of all, assuming we can construct a &lt;code&gt;System&lt;/code&gt; in a sound way, let's imagine some useful functions.&lt;/p&gt;
&lt;p&gt;We can write a function &lt;code&gt;underlyingPosition&lt;/code&gt;, which allows you to give a position in generalized coordinates, and returns the position in the &amp;quot;underlying coordinate system&amp;quot;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L51-55 underlyingPosition     :: System m n     -&amp;gt; R n     -&amp;gt; R m underlyingPosition = sysCoords&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that the types in our function helps us know exactly what the function is doing --- and also helps us implement it correctly. If we have a &lt;code&gt;System&lt;/code&gt; in &lt;code&gt;n&lt;/code&gt; dimensions, over an underlying &lt;code&gt;m&lt;/code&gt;-dimensional Cartesian space, then we would need to convert an &lt;code&gt;R n&lt;/code&gt; (an n-dimensional vector of all of the positions) into an &lt;code&gt;R m&lt;/code&gt; (a vector in the underlying Cartesian space).&lt;/p&gt;
&lt;p&gt;Simple enough, but let's maybe try to calculate something more complicated: the &lt;em&gt;momenta&lt;/em&gt; of a system, given its positions and velocities (configuration).&lt;/p&gt;
&lt;p&gt;We remember that we have a nice formula for that, up above:&lt;/p&gt;
&lt;p&gt;$$ \mathbf{p} = \hat{J}&lt;em&gt;f^T \hat{M} \hat{J}&lt;/em&gt;f \dot{\mathbf{q}} $$&lt;/p&gt;
&lt;p&gt;We can translate that directly into Haskell code:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L59-67 momenta     :: (KnownNat n, KnownNat m)     =&amp;gt; System m n     -&amp;gt; Config n     -&amp;gt; R n momenta s (Config q v) = tr j #&amp;gt; mHat #&amp;gt; j #&amp;gt; v   where     j    = sysJacobian s q     mHat = diag (sysInertia s)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that, because our vectors have their size indexed in their type, this is pretty simple to write and ensure that the shapes &amp;quot;line up&amp;quot;. In fact, GHC can even help you write this function by telling you what values can go in what locations. Being able to get rid of a large class of bugs and clean up your implementation space is nice, too!&lt;/p&gt;
&lt;p&gt;(Note that &lt;em&gt;hmatrix&lt;/em&gt; requires a &lt;code&gt;KnownNat&lt;/code&gt; constraint on the size parameters of our vectors for some functions, so we add this as a constraint on our end.)&lt;/p&gt;
&lt;p&gt;With this, we can write a function to convert any state in configuration space to its coordinates in phase space:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L70-75 toPhase     :: (KnownNat n, KnownNat m)     =&amp;gt; System m n     -&amp;gt; Config n     -&amp;gt; Phase n toPhase s c = Phase (confPositions c) (momenta s c)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This function is important, because &amp;quot;configuration space&amp;quot; is how we actually directly observe our system -- in terms of positions and velocities, and not in terms of positions and momenta (and sometimes conjugate momenta might not even have meaningful physical interpretations). So, having &lt;code&gt;toPhase&lt;/code&gt; lets us &amp;quot;initialize&amp;quot; our system in terms of direct observables, and then convert it to its phase space representation, which is something that Hamiltonian mechanics can work with.&lt;/p&gt;
&lt;h3&gt;Automatic Differentiation&lt;/h3&gt;
&lt;p&gt;Now, creating a &lt;code&gt;System&lt;/code&gt; &amp;quot;from scratch&amp;quot; is not much fun, because you would have to manually differentiate your coordinate systems and potentials to generate your Jacobians and gradients.&lt;/p&gt;
&lt;p&gt;Here's where the magic comes in -- we can have Haskell generate our Jacobians and gradients &lt;em&gt;automatically&lt;/em&gt;, using the amazing &lt;a href="http://hackage.haskell.org/package/ad"&gt;ad&lt;/a&gt; library! We can just use the appropriately named &lt;code&gt;grad&lt;/code&gt;, &lt;code&gt;jacobian&lt;/code&gt;, and &lt;code&gt;hessianF&lt;/code&gt; functions.&lt;/p&gt;
&lt;h4&gt;Quick Intro to AD&lt;/h4&gt;
&lt;p&gt;At the simplest level, if we have a function from some number to some other number, we can use &lt;code&gt;diff&lt;/code&gt; to get its derivative:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell myFunc      :: RealFloat a =&amp;gt; a -&amp;gt; a diff myFunc :: RealFloat a =&amp;gt; a -&amp;gt; a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If we have a function a function from a sized vector to a scalar, we can use &lt;code&gt;grad&lt;/code&gt; to get its gradient:&lt;/p&gt;
&lt;p&gt;```haskell -- import qualified Data.Vector.Sized as V&lt;/p&gt;
&lt;p&gt;myFunc :: RealFloat a =&amp;gt; V.Vector n a -&amp;gt; a grad myFunc :: RealFloat a =&amp;gt; V.Vector n a -&amp;gt; V.Vector n a ```&lt;/p&gt;
&lt;p&gt;Where each of the components in the resulting vector corresponds to the rate of change of the output according to variations in that component.&lt;/p&gt;
&lt;p&gt;We're using &lt;strong&gt;statically sized vector&lt;/strong&gt; type from the &lt;a href="http://hackage.haskell.org/package/vector-sized"&gt;vector-sized&lt;/a&gt; package (in the &lt;a href="http://hackage.haskell.org/package/vector-sized/docs/Data-Vector-Sized.html"&gt;Data.Vector.Sized&lt;/a&gt; module), where &lt;code&gt;V.Vector n a&lt;/code&gt; is a &lt;code&gt;n&lt;/code&gt;-vector of &lt;code&gt;a&lt;/code&gt;s -- for example, a &lt;code&gt;V.Vector 3 Double&lt;/code&gt; is a vector of 3 &lt;code&gt;Double&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;We have to use &lt;code&gt;Vector&lt;/code&gt; (instead of &lt;code&gt;R&lt;/code&gt;, from &lt;em&gt;hmatrix&lt;/em&gt;) because automatic differentiation for gradients requires &lt;em&gt;some Functor&lt;/em&gt; to work. An &lt;code&gt;R 5&lt;/code&gt; is essentially a &lt;code&gt;V.Vector 5 Double&lt;/code&gt;, except the latter can contain other, non-Double things -- and therefore can be used by &lt;em&gt;ad&lt;/em&gt; to do its magic.&lt;/p&gt;
&lt;p&gt;If we have a function from a sized vector to a (differently) sized vector, we can use the &lt;code&gt;jacobian&lt;/code&gt; function to get its jacobian!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell myFunc          :: RealFloat a =&amp;gt; V.Vector n a -&amp;gt; V.Vector m a jacobian myFunc :: RealFloat a =&amp;gt; V.Vector n a -&amp;gt; V.Vector m (V.Vector n a)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Again note the usage of sized vector types, and the fact that our $m \times n$ matrix is represented by a &lt;code&gt;m&lt;/code&gt;-vector of &lt;code&gt;n&lt;/code&gt;-vectors.&lt;/p&gt;
&lt;p&gt;Finally, we can get our Hessian Tensor by using &lt;code&gt;hessianF&lt;/code&gt;:[^hessianf]&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell myFunc     :: RealFloat a =&amp;gt; V.Vector n a -&amp;gt; V.Vector m a hessianF myFunc     :: RealFloat a =&amp;gt; V.Vector n a -&amp;gt; V.Vector m (V.Vector n (V.Vector n a))&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Conversion between vector-sized and hmatrix&lt;/h4&gt;
&lt;p&gt;So some ugly things -- we need to write some functions to convert between &lt;em&gt;vector-sized&lt;/em&gt; sized vectors and &lt;em&gt;hmatrix&lt;/em&gt; vectors and matrices. These are fundamentally unsafe to write (but safe to use, after written properly):&lt;/p&gt;
&lt;p&gt;```haskell -- import qualified Data.Vector.Generic.Sized as VG -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L78-87 vec2r :: KnownNat n =&amp;gt; V.Vector n Double -&amp;gt; R n vec2r = fromJust . create . VG.fromSized . VG.convert&lt;/p&gt;
&lt;p&gt;r2vec :: KnownNat n =&amp;gt; R n -&amp;gt; V.Vector n Double r2vec = VG.convert . fromJust . VG.toSized . extract&lt;/p&gt;
&lt;p&gt;vec2l :: (KnownNat m, KnownNat n) =&amp;gt; V.Vector m (V.Vector n Double) -&amp;gt; L m n vec2l = fromJust . (\rs -&amp;gt; withRows rs exactDims) . toList . fmap vec2r ```&lt;/p&gt;
&lt;p&gt;These are necessary because &lt;em&gt;ad&lt;/em&gt; requires our vectors to be &lt;em&gt;Functors&lt;/em&gt;, but &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;L&lt;/code&gt; from &lt;em&gt;hmatrix&lt;/em&gt; are not your typical Hask Functors. One nice thing is that because they both use &lt;em&gt;TypeLits&lt;/em&gt; to get their sized parameters, we can get type-safe conversions that preserve their size information!&lt;/p&gt;
&lt;p&gt;Also, even though &lt;em&gt;ad&lt;/em&gt; gives our Hessian as an $m \times n \times n$ tensor, we really want it as a n-vector of $m \times n$ matrices -- that's how we interpreted it in our original math. So we just need to write an function to convert what &lt;em&gt;ad&lt;/em&gt; gives us to the form we expect. It's mostly just fiddling around with the internals of &lt;em&gt;hmatrix&lt;/em&gt; in a rather inelegant way. (Again, unsafe to write, but safe to use once you do)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L90-93 rehessian :: (KnownNat m, KnownNat n) =&amp;gt; V.Vector m (L n n) -&amp;gt; V.Vector n (L m n) rehessian = fmap (fromJust . (\rs -&amp;gt; withRows rs exactDims) . toList)           . sequenceA           . fmap (fromJust . V.fromList . toRows)&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Using AD to Auto-Derive Systems&lt;/h4&gt;
&lt;p&gt;Now to make a &lt;code&gt;System&lt;/code&gt; using just the mass vector, the coordinate conversion function, and the potential energy function:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L97-113 mkSystem     :: (KnownNat m, KnownNat n)     =&amp;gt; R m     -&amp;gt; (forall a. RealFloat a =&amp;gt; V.Vector n a -&amp;gt; V.Vector m a)     -&amp;gt; (forall a. RealFloat a =&amp;gt; V.Vector n a -&amp;gt; a)     -&amp;gt; System m n mkSystem m f u = System                   -- &amp;lt; convert from | actual thing | convert to &amp;gt;     { sysInertia       =                         m     , sysCoords        =      vec2r .            f . r2vec     , sysJacobian      =      vec2l .   jacobian f . r2vec     , sysHessian       = rehessian                        . fmap vec2l .   hessianF f . r2vec     , sysPotential     =                         u . r2vec     , sysPotentialGrad =      vec2r .       grad u . r2vec                   -- &amp;lt; convert from | actual thing | convert to &amp;gt;     }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now, I hesitate to call this &amp;quot;trivial&amp;quot;...but, I think it really is a straightforward direct translation of the definitions, minus some boilerplate conversions back and forth using &lt;code&gt;r2vec&lt;/code&gt;, &lt;code&gt;vec2r&lt;/code&gt;, and &lt;code&gt;vec2l&lt;/code&gt;!&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The vector of masses is just &lt;code&gt;m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The coordinate function is just &lt;code&gt;f&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The Jacobian of the coordinate function is just &lt;code&gt;jacobian f&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The Hessian Tensor of the coordinate function is just &lt;code&gt;hessianF f&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The potential energy function is just &lt;code&gt;u&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The gradient of the potential energy function is just &lt;code&gt;grad u&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;em&gt;ad&lt;/em&gt; library automatically generated all of these for us and created a perfectly well-formed &lt;code&gt;System&lt;/code&gt; with all of its gradients and Jacobians and Hessians by giving only the coordinate function and the potential energy function, and in such a clean and concise way!&lt;/p&gt;
&lt;h3&gt;Equations of Motion&lt;/h3&gt;
&lt;p&gt;At this point, we're ready to write our final equations of motion, which we found to be given by:&lt;/p&gt;
&lt;p&gt;$$ \begin{aligned} \dot{\mathbf{q}} &amp;amp; = \hat{K}^{-1} \mathbf{p} \ \dot{\mathbf{p}} &amp;amp; = \mathbf{p}^T \hat{K}^{-1} \hat{J}&lt;em&gt;f^T \hat{M} \left[ \nabla&lt;/em&gt;{\mathbf{q}} \hat{J}&lt;em&gt;f \right] \hat{K}^{-1} \mathbf{p} - \nabla&lt;/em&gt;{\mathbf{q}} PE(\mathbf{q}) \end{aligned} $$&lt;/p&gt;
&lt;p&gt;These equations aren't particularly beautiful, but it's straightforward to translate them into Haskell (using $\hat{K} = \hat{J}&lt;em&gt;f^T \hat{M} \hat{J}&lt;/em&gt;f$):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L116-133 hamilEqns     :: (KnownNat n, KnownNat m)     =&amp;gt; System m n     -&amp;gt; Phase n     -&amp;gt; (R n, R n)       -- dq/dt and dp/dt hamilEqns s (Phase q p) = (dqdt, dpdt)   where     j       = sysJacobian s q     trj     = tr j     mHat    = diag (sysInertia s)     kHat    = trj &amp;lt;&amp;gt; mHat &amp;lt;&amp;gt; j     kHatInv = inv kHat     dqdt    = kHatInv #&amp;gt; p     dpdt    = vec2r bigUglyThing - sysPotentialGrad s q       where         bigUglyThing =           fmap (\j2 -&amp;gt; -p &amp;lt;.&amp;gt; kHatInv #&amp;gt; trj #&amp;gt; mHat #&amp;gt; j2 #&amp;gt; kHatInv #&amp;gt; p)                (sysHessian s q)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Of course, there is no way to get around the big ugly math term in $\dot{p}_q$, but at least it is a direct reading of the math!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But!!&lt;/em&gt; I'd much rather write this scary Haskell than that scary math, because &lt;em&gt;ghc typechecks our math&lt;/em&gt;! When writing out those equations, we really had no idea if we were writing it correctly, and if the matrix and vector and tensor dimensions line up. If it even &lt;em&gt;made sense&lt;/em&gt; to multiply and transpose the quantities we had.&lt;/p&gt;
&lt;p&gt;However, when writing &lt;code&gt;hamilEqns&lt;/code&gt;, we let GHC &lt;em&gt;hold our hand for us&lt;/em&gt;. If any of our math is wrong, GHC will verify it for us! If any dimensions don't match up, or any transpositions don't make sense, we'll know immediately. And if we're ever lost, we can leave a &lt;em&gt;&lt;a href="https://wiki.haskell.org/GHC/Typed_holes"&gt;typed hole&lt;/a&gt;&lt;/em&gt; -- then GHC will tell you all of the values in scope that can &lt;em&gt;fit&lt;/em&gt; in that hole! Even if you don't completely understand the math, this helps you implement it in a somewhat confident way.&lt;/p&gt;
&lt;p&gt;It's admittedly difficult to convey how helpful these sized vector types are without working through trying to implement them yourself, so feel free to give it a try when you get the chance! :D&lt;/p&gt;
&lt;h3&gt;Numerical Integration&lt;/h3&gt;
&lt;p&gt;The result of &lt;code&gt;hamilEqns&lt;/code&gt; gives the rate of change of the components of our &lt;code&gt;Phase n&lt;/code&gt;. The rest of the processes then is just to &amp;quot;step&amp;quot; &lt;code&gt;Phase n&lt;/code&gt;. Gradually update it, following these rate of changes!&lt;/p&gt;
&lt;p&gt;This process is known as &lt;a href="https://en.wikipedia.org/wiki/Numerical_integration"&gt;numerical integration&lt;/a&gt;. The &amp;quot;best&amp;quot; way to do it is quite a big field, so for this article we're going to be using the extremely extremely simple &lt;a href="https://en.wikipedia.org/wiki/Euler_method"&gt;Euler method&lt;/a&gt; to progress our system through time.&lt;/p&gt;
&lt;p&gt;Disclaimer -- The Euler method is typically a &lt;strong&gt;very very bad&lt;/strong&gt; choice for numerical integration (even though, as popularized in the movie &lt;em&gt;Hidden Figures&lt;/em&gt;, it was good enough to &lt;a href="http://www.latimes.com/science/sciencenow/la-sci-sn-hidden-figures-katherine-johnson-20170109-story.html"&gt;send humans to space?&lt;/a&gt;). We are just choosing it for this article because it's very simple, conceptually!&lt;/p&gt;
&lt;p&gt;The basic idea is that you pick a time-step, $\Delta t$, and update each coordinate as:&lt;/p&gt;
&lt;p&gt;$$ x(t + \Delta t) = x(t) + \dot{x}(t) \Delta t $$&lt;/p&gt;
&lt;p&gt;Which makes sense visually if we imagine $\dot{x}$ as the &amp;quot;slope&amp;quot; of $x$ -- it just means to follow the slope another $\Delta t$ steps. If the slope stays constant, this method is perfectly accurate. The inaccuracy, of course, happens when the slope changes drastically within that $\Delta t$ (and also from the fact that small errors cause errors in the new calculations of $\dot{x}$, and so compound over time)&lt;/p&gt;
&lt;p&gt;You can understand this symbolically, as well, by remembering that the derivative can be approximated by $\dot{x}(t) \approx \frac{x(t + \Delta t) - x(t)}{\Delta t}$ for small $\Delta t$, and so we can do a little bit of symbolic manipulation to get $x(t + \Delta t) \approx \dot{x}(t) \Delta t + x(t)$.&lt;/p&gt;
&lt;p&gt;We can directly translate this into Haskell: (using &lt;code&gt;konst :: KnownNat n =&amp;gt; Double -&amp;gt; R n&lt;/code&gt;, making a constant vector, and &lt;code&gt;*&lt;/code&gt;, the component-wise product of two vectors)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L136-144 stepEuler     :: (KnownNat n, KnownNat m)     =&amp;gt; System m n       -- ^ the system     -&amp;gt; Double           -- ^ dt     -&amp;gt; Phase n          -- ^ q(t) and p(t)     -&amp;gt; Phase n          -- ^ q(t + dt) and p(t + dt) stepEuler s dt ph@(Phase q p) = Phase (q + konst dt * dq) (p + konst dt * dp)   where     (dq, dp) = hamilEqns s ph&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And repeatedly evolve this system as a lazy list:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L147-155 runSystem     :: (KnownNat n, KnownNat m)     =&amp;gt; System m n       -- ^ the system     -&amp;gt; Double           -- ^ dt     -&amp;gt; Phase n          -- ^ initial phase     -&amp;gt; [Phase n]        -- ^ progression of the system using Euler integration runSystem s dt = go   where     go p0 = p0 : go (stepEuler s dt p0)&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Running with it&lt;/h2&gt;
&lt;p&gt;And...that's it! Granted, in real life, we would be using a less naive integration method, but this is essentially the entire process!&lt;/p&gt;
&lt;p&gt;Let's generate the boring system, a 5kg particle in 2D Cartesian Coordinates under gravity --&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L158-164 simpleSystem :: System 2 2 simpleSystem = mkSystem (vec2 5 5) id pot   where     -- potential energy of a gravity field     -- U(x,y) = 9.8 * y     pot :: RealFloat a =&amp;gt; V.Vector 2 a -&amp;gt; a     pot xy = 9.8 * (xy `V.index` 1)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If we initialize the particle at position $\mathbf{q}&lt;em&gt;0 = \langle 0, 0 \rangle$ and velocity $\mathbf{v}&lt;/em&gt;0 = \langle 1, 3 \rangle$ (that is, $v&lt;em&gt;x = 1$ and $v&lt;/em&gt;y = 3$), we should see something that travels at a constant velocity in x and something that starts moving &amp;quot;upwards&amp;quot; (in positive y) and eventually reaches a peak and moves downwards.&lt;/p&gt;
&lt;p&gt;We can make our initial configuration:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L168-172 simpleConfig0 :: Config 2 simpleConfig0 = Config     { confPositions  = vec2 0 0     , confVelocities = vec2 1 3     }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And then...let it run!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L174-178 simpleMain :: IO () simpleMain =     mapM_ (disp 2 . phasePositions)  -- position with 2 digits of precision   . take 25                          -- 25 steps   $ runSystem simpleSystem 0.1 (toPhase simpleSystem simpleConfig0)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We get:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :l Hamilton.hs ghci&amp;gt; simpleMain 0     0 0.10  0.30 0.20  0.58 0.30  0.84 0.40  1.08 0.50  1.30 0.60  1.51 0.70  1.69 0.80  1.85 0.90  1.99 1.00  2.12 1.10  2.22 1.20  2.31 1.30  2.37 1.40  2.42 1.50  2.44 1.60  2.45 1.70  2.43 1.80  2.40 1.90  2.35 2.00  2.28 2.10  2.18 2.20  2.07 2.30  1.94 2.40  1.79&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Exactly what we'd expect! The &lt;code&gt;x&lt;/code&gt; positions increase steadily, and the &lt;code&gt;y&lt;/code&gt; positions increase, slow down, and start decreasing.&lt;/p&gt;
&lt;p&gt;We can try a slightly more complicated example that validates (and justifies) all of the work we've done -- let's simulate a simple pendulum. The state of a pendulum is characterized by one coordinate $\theta$, which refers to the angular (clockwise) from the equilibrium &amp;quot;hanging straight down&amp;quot; position. $\theta = 0$ corresponds to 6 o' clock, $\theta = \pi/2$ corresponds to 9 o' clock, $\theta = - \pi / 2$ corresponds to 3 o' clock, etc. For a pendulum of length $l$, we can translate that as $\langle x, y \rangle = \langle - l sin(\theta), - l cos(\theta) \rangle$.&lt;/p&gt;
&lt;p&gt;Let's set up that system! We'll put it under normal gravity potential, again ($U(x,y) = 9.8 y$). Our initial position $\theta&lt;em&gt;0$ will be at equilibrium, and our initial angular velocity $v&lt;/em&gt;{\theta 0}$ will be 0.1 radians/sec (clockwise), as we try to induce harmonic motion:&lt;/p&gt;
&lt;p&gt;``&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton1/Hamilton.hs#L181-210 -- | A pendulum system, parameterized by its angle clockwise from -- equilibrium pendulum :: System 2 1 pendulum = mkSystem (vec2 5 5) coords pot      -- 5kg particle   where     -- &amp;lt;x,y&amp;gt; = &amp;lt;-0.5 sin(theta), -0.5 cos(theta)&amp;gt;     -- pendulum of length 0.25     coords :: RealFloat a =&amp;gt; V.Vector 1 a -&amp;gt; V.Vector 2 a     coords (V.head-&amp;gt;theta) = fromJust                            . V.fromList                            $ [- 0.25 * sin theta, - 0.25 * cos theta]     -- potential energy of gravity field     -- U(x,y) = 9.8 * y     pot :: RealFloat a =&amp;gt; V.Vector 1 a -&amp;gt; a     pot q = 9.8 * (coords q&lt;/code&gt;V.index` 1)&lt;/p&gt;
&lt;p&gt;pendulumConfig0 :: Config 1 pendulumConfig0 = Config { confPositions = 0 , confVelocities = 0.1 }&lt;/p&gt;
&lt;p&gt;pendulumMain :: IO () pendulumMain = mapM_ (disp 3 . phasePositions) -- position with 2 digits of precision . take 25 -- 25 steps $ runSystem pendulum 0.1 (toPhase pendulum pendulumConfig0) ```&lt;/p&gt;
&lt;p&gt;This pendulum should wobble back and forth, ever so slightly, around equilibrium.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :l Hamilton.hs ghci&amp;gt; pendulumMain 0 0.010 0.020 0.029 0.037 0.042 0.045 0.044 0.040 0.032 0.021 0.007 -0.008 -0.023 -0.038 -0.051 -0.061 -0.068 -0.069 -0.065 -0.056 -0.041 -0.022 -0.000 0.023&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We see our $\theta$ coordinate increasing, then turning around and decreasing, swinging the other way past equilibrium, and then turning around and heading back![^phantom]&lt;/p&gt;
&lt;p&gt;We &lt;em&gt;automatically generated equations of motion for a pendulum&lt;/em&gt;. Sweet!&lt;/p&gt;
&lt;h2&gt;Wrap-Up&lt;/h2&gt;
&lt;p&gt;We traveled through the world of physics, math, Haskell, and back again to achieve something that would have initially seemed like a crazy thought experiment. But, utilizing Hamiltonian mechanics, we have a system that can automatically generate equations of motion given your coordinate system and a potential energy function. We also learned how to leverage typed vectors for more correct code and a smoother development process.&lt;/p&gt;
&lt;p&gt;See my &lt;a href="https://blog.jle.im/entry/introducing-the-hamilton-library.html"&gt;previous post&lt;/a&gt; for even crazier examples -- involving multiple objects, double pendulums, and more. And check out my &lt;a href="http://hackage.haskell.org/package/hamilton"&gt;hamilton&lt;/a&gt; library on hackage, which includes demos for exotic interesting systems, rendered graphically on your terminal.&lt;/p&gt;
&lt;p&gt;I realize that this was a lot, so if you have any questions or suggestions for clarifications, feel free to leave a comment, drop me a &lt;a href="https://twitter.com/mstk" title="Twitter"&gt;tweet&lt;/a&gt;, or find me on the freenode &lt;em&gt;#haskell&lt;/em&gt; channel (where I usually idle as &lt;em&gt;jle`&lt;/em&gt;!)&lt;/p&gt;</description><author>Justin Le</author><category>Haskell</category><guid isPermaLink="true">https://blog.jle.im/entry/hamiltonian-dynamics-in-haskell.html</guid><pubDate>Mon, 27 Nov 2017 19:46:18 UTC</pubDate><creator>Justin Le</creator><subject>Haskell</subject><date>2017-11-27</date></item><item><title>Fixed-Length Vector Types in Haskell (an Update for 2017)</title><link>https://blog.jle.im/entry/fixed-length-vector-types-in-haskell.html</link><description>&lt;p&gt;This post is a follow-up to my &lt;a href="https://blog.jle.im/entry/fixed-length-vector-types-in-haskell-2015.html"&gt;fixed-length vectors in haskell in 2015&lt;/a&gt; post! When I was writing the post originally, I was new to the whole type-level game in Haskell; I didn't know what I was talking about, and that post was a way for me to push myself to learn more. Immediately after it was posted, of course, people taught me where I went wrong in the idioms I explained, and better and more idiomatic ways to do things. And that's great! Learning is awesome!&lt;/p&gt;
&lt;p&gt;Unfortunately, however, to my horror, I began noticing people referring to the post in a canonical/authoritative way...so the post became an immediate source of guilt to me. I tried correcting things with my &lt;a href="https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html"&gt;practical dependent types in haskell&lt;/a&gt; series the next year. But I still saw my 2015 post being used as a reference even after that post, so I figured that writing a direct replacement/follow-up as the only way I would ever fix this!&lt;/p&gt;
&lt;p&gt;So here we are in 2017. GHC 8.2 is here, and &lt;em&gt;base&lt;/em&gt; is in version &lt;em&gt;4.10&lt;/em&gt;. What's the &amp;quot;right&amp;quot; way to do fixed-length vectors in Haskell?&lt;/p&gt;
&lt;p&gt;This post doesn't attempt to present anything groundbreaking or new, but is meant to be a sort of &lt;em&gt;reference/introduction&lt;/em&gt; to fixed-length vectors in Haskell, as of GHC 8.2 and the 2017 Haskell ecosystem.&lt;/p&gt;
&lt;p&gt;We'll be looking at two methods here: The first one we will be looking at is a &lt;em&gt;performant&lt;/em&gt; fixed-length vector that you will probably be using for any code that requires a fixed-length container --- especially for tight numeric code and situations where performance matters. We'll see how to implement them using the universal native &lt;code&gt;KnownNat&lt;/code&gt; mechanisms, and also how we can implement them using &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/singletons"&gt;singletons&lt;/a&gt;&lt;/em&gt; to help us make things a bit smoother and more well-integrated. For most people, this is all they actually need. (I claim the canonical haskell ecosystem source to be the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/vector-sized"&gt;vector-sized&lt;/a&gt;&lt;/em&gt; library)&lt;/p&gt;
&lt;p&gt;The second method is a &lt;em&gt;structural&lt;/em&gt; fixed-length inductive vector. It's...actually more like a fixed-length (lazily linked) &lt;em&gt;list&lt;/em&gt; than a vector. The length of the list is enforced by the very structure of the data type. This type is more useful as a streaming data type, and also in situations where you want take advantage of the structural characteristics of lengths in the context of a dependently typed program. (I claim the canonical haskell ecosystem source to be the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/type-combinators"&gt;type-combinators&lt;/a&gt;&lt;/em&gt; library)&lt;/p&gt;
&lt;h2&gt;The Non-Structural Way&lt;/h2&gt;
&lt;p&gt;(Code for this section (as well as exercise solutions) are &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs"&gt;available here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In most situations, if you use vectors, you want some sort of constant-time indexed data structure. The best way to do this in Haskell is to wrap the heavily optimized &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/vector"&gt;vector&lt;/a&gt;&lt;/em&gt; library with a newtype wrapper that contains its length as a phantom type parameter.&lt;/p&gt;
&lt;p&gt;```haskell import qualified Data.Vector as V import GHC.TypeNats&lt;/p&gt;
&lt;p&gt;-- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L19-20 data Vec (n :: Nat) a = UnsafeMkVec { getVector :: V.Vector a } deriving Show ```&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;Vec n a&lt;/code&gt; will represent an &lt;code&gt;n&lt;/code&gt;-element vector of &lt;code&gt;a&lt;/code&gt;s. So, a &lt;code&gt;Vec 5 Int&lt;/code&gt; will be a vector of five &lt;code&gt;Int&lt;/code&gt;s, a &lt;code&gt;Vec 10 String&lt;/code&gt; is a vector of 10 &lt;code&gt;String&lt;/code&gt;s, etc.&lt;/p&gt;
&lt;p&gt;For our numeric types, we're using the fancy &amp;quot;type literals&amp;quot; that GHC offers us with the &lt;code&gt;DataKinds&lt;/code&gt; extension. Basically, alongside the normal kinds &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;* -&amp;gt; *&lt;/code&gt;, etc., we also have the &lt;code&gt;Nat&lt;/code&gt; kind; type literals &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;5&lt;/code&gt;, &lt;code&gt;100&lt;/code&gt;, etc. are all &lt;em&gt;types&lt;/em&gt; with the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;Nat&lt;/code&gt;, from the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base/docs/GHC-TypeNats.html"&gt;GHC.TypeNats&lt;/a&gt;&lt;/em&gt; module[^typelits].&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :k 5 Nat ghci&amp;gt; :k Vec Vec :: Nat -&amp;gt; * -&amp;gt; *&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can &amp;quot;reflect&amp;quot; the type-level numeral as a value using the &lt;code&gt;KnownNat&lt;/code&gt; typeclass, provided by GHC, which lets you gain back the number as a run-time value using &lt;code&gt;natVal&lt;/code&gt;: (This process is called &amp;quot;reflection&amp;quot;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell natVal :: KnownNat n =&amp;gt; p n -&amp;gt; Natural&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(Where &lt;code&gt;Natural&lt;/code&gt;, from &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base/docs/Numeric-Natural.html"&gt;Numeric.Natural&lt;/a&gt;&lt;/em&gt;, is a non-negative &lt;code&gt;Integer&lt;/code&gt; type.)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; natVal (Proxy @10)   -- or, natVal (Proxy :: Proxy 10) 10 ghci&amp;gt; natVal (Proxy @7) 7&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Super low-level utility functions for the &lt;code&gt;Nat&lt;/code&gt; kind (like &lt;code&gt;natVal&lt;/code&gt;) are found in the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base/docs/GHC-TypeNats.html"&gt;GHC.TypeNats&lt;/a&gt;&lt;/em&gt; module (and also in &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base/docs/GHC-TypeLits.html"&gt;GHC.TypeLits&lt;/a&gt;&lt;/em&gt; for a slightly different API)&lt;/p&gt;
&lt;h3&gt;The Smart Constructor&lt;/h3&gt;
&lt;p&gt;We can use &lt;code&gt;natVal&lt;/code&gt; with the &lt;code&gt;KnownNat&lt;/code&gt; typeclass to write a &amp;quot;smart constructor&amp;quot; for our type -- make a &lt;code&gt;Vec&lt;/code&gt; from a &lt;code&gt;Vector&lt;/code&gt;, but only if the length is the correct type:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L22-26 mkVec :: forall n a. KnownNat n =&amp;gt; V.Vector a -&amp;gt; Maybe (Vec n a) mkVec v | V.length v == l = Just (UnsafeMkVec v)         | otherwise       = Nothing   where     l = fromIntegral (natVal (Proxy @n))&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here, we use &lt;code&gt;ScopedTypeVariables&lt;/code&gt; so we can refer to the &lt;code&gt;n&lt;/code&gt; in the type signature in the function body (for &lt;code&gt;natVal (Proxy @n)&lt;/code&gt;), and we need to use an explicit forall, then, to bring the &lt;code&gt;n&lt;/code&gt; into scope.&lt;/p&gt;
&lt;p&gt;We also use &lt;code&gt;TypeApplications&lt;/code&gt; syntax (&lt;code&gt;Proxy @n&lt;/code&gt;) so we can say &amp;quot;We want a &lt;code&gt;Proxy :: Proxy n&lt;/code&gt;&amp;quot; and tell &lt;code&gt;natVal&lt;/code&gt; that we want the &lt;code&gt;Natural&lt;/code&gt; for &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Utilizing type-level guarantees&lt;/h3&gt;
&lt;p&gt;Another operation we might want to do with vectors is do things with them that might change their length in a predetermined way. We might want the type of our vectors to describe the nature of the operations they are undergoing. For example, if you saw a function:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell someFunc :: (a -&amp;gt; b) -&amp;gt; Vec n a -&amp;gt; Vec n b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can see that it takes a function and a vector of length &lt;code&gt;n&lt;/code&gt;, and returns another vector of length &lt;code&gt;n&lt;/code&gt;. Clearly, this function might be a &amp;quot;map&amp;quot; function, which applies the function to all of the values in the &lt;code&gt;Vec&lt;/code&gt;! We know that it must have the same length, so it can't drop or add items. (However, it could still be shuffling or duplicating or permuting the items, as long as the resulting length is the same)&lt;/p&gt;
&lt;p&gt;In this situation, we can write such a mapping function in an &amp;quot;unsafe&amp;quot; way, and then give it our type signature as a form of documentation:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L28-29 mapVec :: (a -&amp;gt; b) -&amp;gt; Vec n a -&amp;gt; Vec n b mapVec f v = UnsafeMkVec $ V.map f (getVector v)&lt;/p&gt;
&lt;p&gt;-- just for fun -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L31-32 instance Functor (Vec n) where fmap = mapVec ```&lt;/p&gt;
&lt;p&gt;The compiler didn't help us write this function, and we have to be pretty careful that the guarantees we specify in our types are reflected in the actual unsafe operations. This is because our types don't &lt;em&gt;structurally&lt;/em&gt; enforce their type-level lengths.&lt;/p&gt;
&lt;p&gt;So, why bother? For us, here, our fixed-length vector types basically act as &amp;quot;active documentation&amp;quot;, in a way. Compare:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- | Maps the function over the items in the vector, returning a vector of the -- same length V.map :: (a -&amp;gt; b) -&amp;gt; V.Vector a -&amp;gt; V.Vector b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Which is the type signature we'd have to write for a map of an &amp;quot;unsized&amp;quot; vector (like from the &lt;em&gt;Data.Vector&lt;/em&gt; module)&lt;/p&gt;
&lt;p&gt;We have to rely on the documentation to &lt;em&gt;tell&lt;/em&gt; us what the length of the final resulting vector is, even though it can be known statically if you know the length of the input vectors. The vectors have a &lt;em&gt;static relationship&lt;/em&gt; in their length, but this isn't specified in a way that the compiler can take advantage of.&lt;/p&gt;
&lt;p&gt;By having our &lt;code&gt;mapVec :: (a -&amp;gt; b) -&amp;gt; Vec n a -&amp;gt; Vec n b&lt;/code&gt;, the relationship between the input lengths and output length is right there in the types, when you &lt;em&gt;use&lt;/em&gt; &lt;code&gt;mapVec&lt;/code&gt;, GHC is aware of the relationships and can give you help in the form of typed hole suggestions and informative type errors. You can even catch errors in logic at compile-time instead of runtime!&lt;/p&gt;
&lt;p&gt;```haskell -- the resulting vector's length is the sum of the input vectors' lengths -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L34-35 (++) :: Vec n a -&amp;gt; Vec m a -&amp;gt; Vec (n + m) a UnsafeMkVec xs ++ UnsafeMkVec ys = UnsafeMkVec (xs V.++ ys)&lt;/p&gt;
&lt;p&gt;-- you must zip two vectors of the same length -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L37-38 zipVec :: Vec n a -&amp;gt; Vec n b -&amp;gt; Vec n (a, b) zipVec (UnsafeMkVec xs) (UnsafeMkVec ys) = UnsafeMkVec (V.zip xs ys)&lt;/p&gt;
&lt;p&gt;-- type-level arithmetic to let us 'take' -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L40-43 takeVec :: forall n m a. KnownNat n =&amp;gt; Vec (n + m) a -&amp;gt; Vec n a takeVec (UnsafeMkVec xs) = UnsafeMkVec (V.take l xs) where l = fromIntegral (natVal (Proxy @n))&lt;/p&gt;
&lt;p&gt;-- splitAt, as well -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L45-49 splitVec :: forall n m a. KnownNat n =&amp;gt; Vec (n + m) a -&amp;gt; (Vec n a, Vec m a) splitVec (UnsafeMkVec xs) = (UnsafeMkVec ys, UnsafeMkVec zs) where l = fromIntegral (natVal (Proxy @n)) (ys, zs) = V.splitAt l xs ```&lt;/p&gt;
&lt;p&gt;Here, &lt;code&gt;(+)&lt;/code&gt; comes from &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base/docs/GHC-TypeNats.html"&gt;GHC.TypeNats&lt;/a&gt;&lt;/em&gt;, which provides it as a type family (type-level function) we can use, with proper meaning and semantics.&lt;/p&gt;
&lt;p&gt;Note the usage of &lt;code&gt;(+)&lt;/code&gt; for &lt;code&gt;takeVec&lt;/code&gt; and &lt;code&gt;splitVec&lt;/code&gt; to let the function ensure that the input vector has &amp;quot;enough&amp;quot; (at least &lt;code&gt;n&lt;/code&gt;) elements to do the taking.&lt;/p&gt;
&lt;h4&gt;Notes on the typechecker&lt;/h4&gt;
&lt;p&gt;GHC's typechecker works very well with concrete, monomorphic &lt;code&gt;Nat&lt;/code&gt;s. For example, &lt;code&gt;5 + 3&lt;/code&gt; will always typecheck as &lt;code&gt;8&lt;/code&gt;, so you don't have to worry at all about &lt;code&gt;takeVec&lt;/code&gt;, &lt;code&gt;(++)&lt;/code&gt;, and &lt;code&gt;splitVec&lt;/code&gt;'s usage of &lt;code&gt;(+)&lt;/code&gt; if you work with monomorphic, specific &lt;code&gt;Nat&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;However, GHC treats &lt;code&gt;(+)&lt;/code&gt; &amp;quot;opaquely&amp;quot; when using using it with polymorphic type variables. That means that &lt;code&gt;n + (m + o)&lt;/code&gt; is seen as a completely different type to GHC than &lt;code&gt;(n + m) + o&lt;/code&gt; -- GHC doesn't reduce &lt;code&gt;+&lt;/code&gt;, and to it, they both just look like different trees. Remember that one is &lt;code&gt;(+) n ((+) m o)&lt;/code&gt;, and the other is &lt;code&gt;(+) ((+) n m) o&lt;/code&gt;. Completely different structure!&lt;/p&gt;
&lt;p&gt;This comes up as an issue when you start doing non-trivial things, so it sometimes helps to augment GHC's typechecker.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/ghc-typelits-natnormalise"&gt;ghc-typelits-natnormalise&lt;/a&gt;&lt;/em&gt; package provides such a plugin. If we pass it as a flag to GHC (as &lt;code&gt;-fplugin GHC.TypeLits.NatNormalise&lt;/code&gt;) or as a pragma:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell {-# OPTIONS_GHC -fplugin GHC.TypeLits.Normalise #-}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then GHC will be able to recognize the fact that &lt;code&gt;n + (m + o)&lt;/code&gt; and &lt;code&gt;(n + m) + o&lt;/code&gt; are the same, and will unify them during typechecking. It also provides normalization/unification for many other situations that we &amp;quot;expect&amp;quot; to work when using &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;+&lt;/code&gt; on type variables.&lt;/p&gt;
&lt;h3&gt;Indexing&lt;/h3&gt;
&lt;p&gt;We need an appropriate type for indexing our vectors, but we'd like a type where indexing is &amp;quot;safe&amp;quot; -- that is, that you can't compile a program that will result in an index error.&lt;/p&gt;
&lt;p&gt;For this, we can use the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/finite-typelits"&gt;finite-typelits&lt;/a&gt;&lt;/em&gt; package, which provides the &lt;code&gt;Finite n&lt;/code&gt; type.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;Finite n&lt;/code&gt; type is a type with exactly &lt;code&gt;n&lt;/code&gt; distinct inhabitants/values. For example, &lt;code&gt;Finite 4&lt;/code&gt; contains four &amp;quot;anonymous&amp;quot; inhabitants. For convenience, sometimes we like to name them 0, 1, 2, and 3. In general, we sometimes refer to the values of type &lt;code&gt;Finite n&lt;/code&gt; as 0 ... (n - 1).&lt;/p&gt;
&lt;p&gt;So, we can imagine that &lt;code&gt;Finite 6&lt;/code&gt; has inhabitants corresponding to 0, 1, 2, 3, 4, and 5. We can convert back and forth between a &lt;code&gt;Finite n&lt;/code&gt; and its &lt;code&gt;Integer&lt;/code&gt; representation using &lt;code&gt;packFinite&lt;/code&gt; and &lt;code&gt;getFinite&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell packFinite :: KnownNat n =&amp;gt; Integer  -&amp;gt; Maybe (Finite n) getFinite  ::               Finite n -&amp;gt; Integer&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; map packFinite [0..3] :: [Maybe (Finite 3)] [Just (finite 0), Just (finite 1), Just (finite 2), Nothing] ghci&amp;gt; getFinite (finite 2 :: Finite 5) 2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can use a &lt;code&gt;Finite n&lt;/code&gt; to &amp;quot;index&amp;quot; a &lt;code&gt;Vector n a&lt;/code&gt;. A &lt;code&gt;Vector n a&lt;/code&gt; has exactly &lt;code&gt;n&lt;/code&gt; slots, and a &lt;code&gt;Finite n&lt;/code&gt; has &lt;code&gt;n&lt;/code&gt; possible values. Clearly, &lt;code&gt;Finite n&lt;/code&gt; only contains valid indices into our vector!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L51-52 index :: Vec n a -&amp;gt; Finite n -&amp;gt; a index v i = getVector v V.! fromIntegral (getFinite i)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;index&lt;/code&gt; will never fail at runtime due to a bad index --- do you see why? Valid indices of a &lt;code&gt;Vector 5 a&lt;/code&gt; are the integers 0 to 4, and that is precisely the exact things that &lt;code&gt;Finite 5&lt;/code&gt; can store!&lt;/p&gt;
&lt;h3&gt;Generating&lt;/h3&gt;
&lt;p&gt;We can directly generate these vectors in interesting ways. Using return-type polymorphism, we can have the user &lt;em&gt;directly&lt;/em&gt; request a vector length, &lt;em&gt;just&lt;/em&gt; by using type inference or a type annotation. (kind of like with &lt;code&gt;read&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;For example, we can write a version of &lt;code&gt;replicate&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L54-57 replicate :: forall n a. KnownNat n =&amp;gt; a -&amp;gt; Vec n a replicate x = UnsafeMkVec $ V.replicate l x   where     l = fromIntegral (natVal (Proxy @n))&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; replicate 'a' :: Vec 5 Char UnsafeMkVec (V.fromList ['a','a','a','a','a'])&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that normally, &lt;code&gt;replicate&lt;/code&gt; takes an &lt;code&gt;Int&lt;/code&gt; argument so that the user can give how long the resulting vector needs to be. However, with our new &lt;code&gt;replicate&lt;/code&gt;, we don't need that &lt;code&gt;Int&lt;/code&gt; argument --- the size of the vector we want can more often than not be inferred auto-magically using type inference!&lt;/p&gt;
&lt;p&gt;With this new cleaner type signature, we can actually see that &lt;code&gt;replicate&lt;/code&gt;'s type is something very familiar. Look at it carefully:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell replicate :: KnownNat n =&amp;gt; a -&amp;gt; Vec n a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You might recognize it as very similar to haskellism &lt;code&gt;pure&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell pure :: Applicative f =&amp;gt; a -&amp;gt; f a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;replicate&lt;/code&gt; is actually &lt;code&gt;pure&lt;/code&gt; for the Applicative instance of &lt;code&gt;Vec n&lt;/code&gt;! As an extra challenge, what would &lt;code&gt;&amp;lt;*&amp;gt;&lt;/code&gt; be? See &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L59-61"&gt;the solution&lt;/a&gt; if you want to check your answer!&lt;/p&gt;
&lt;h4&gt;Generating with indices&lt;/h4&gt;
&lt;p&gt;We can be a little more fancy with &lt;code&gt;replicate&lt;/code&gt;, to get what we normally call &lt;code&gt;generate&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L63-66 generate :: forall n a. KnownNat n =&amp;gt; (Finite n -&amp;gt; a) -&amp;gt; Vec n a generate f = UnsafeMkVec $ V.generate l (f . fromIntegral)   where     l = fromIntegral (natVal (Proxy @n))&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Type-Safety and positives and negatives&lt;/h4&gt;
&lt;p&gt;I think it's an interesting point that we're using &lt;code&gt;Finite n&lt;/code&gt; in a different sense here than in &lt;code&gt;index&lt;/code&gt;, and for different reasons. In &lt;code&gt;index&lt;/code&gt;, &lt;code&gt;Finite&lt;/code&gt; is in the &amp;quot;negative&amp;quot; position --- it's something that the function &amp;quot;takes&amp;quot;. In &lt;code&gt;generate&lt;/code&gt;, &lt;code&gt;Finite&lt;/code&gt; is in the &amp;quot;positive&amp;quot; position --- it's something that the function &amp;quot;gives&amp;quot; (to the &lt;code&gt;f&lt;/code&gt; in &lt;code&gt;generate f&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In the negative position, &lt;code&gt;Finite n&lt;/code&gt; and type-safety is useful because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It tells the user what sort of values that the function expects. The user &lt;em&gt;knows&lt;/em&gt;, just from the type, that indexing a &lt;code&gt;Vec 5 a&lt;/code&gt; requires a &lt;code&gt;Finite 5&lt;/code&gt;, or a number between 0 and 4.&lt;/li&gt;
&lt;li&gt;It guarantees that whatever &lt;code&gt;Finite n&lt;/code&gt; index you give to &lt;code&gt;index&lt;/code&gt; is a &lt;em&gt;valid one&lt;/em&gt;. It's impossible to give &lt;code&gt;index&lt;/code&gt; an &amp;quot;invalid index&amp;quot;, so &lt;code&gt;index&lt;/code&gt; is allowed to use &amp;quot;unsafe indexing&amp;quot; in its implementation, knowing that nothing bad can be given.&lt;/li&gt;
&lt;li&gt;It lets you develop code in &amp;quot;typed-hole&amp;quot; style: if a function requires a &lt;code&gt;Finite 4&lt;/code&gt;, put an underscore there, and GHC will tell you about all the &lt;code&gt;Finite 4&lt;/code&gt;s you have in scope. It can help you write your code for you!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the positive position, &lt;code&gt;Finite n&lt;/code&gt; and the type-safety have different uses and advantages: it tells the user what sort of values the function can return, and also also the type of values that the user has to be expected to handle. For example, in &lt;code&gt;generate&lt;/code&gt;, the fact that the user has to provide a &lt;code&gt;Finite n -&amp;gt; a&lt;/code&gt; tells the user that they have to handle every number between 0 and n-1, and nothing else.&lt;/p&gt;
&lt;h3&gt;Moving between Sized and Unsized&lt;/h3&gt;
&lt;p&gt;One key part of our API is missing: how to convert between &amp;quot;sized&amp;quot; and &amp;quot;unsized&amp;quot; vectors.&lt;/p&gt;
&lt;p&gt;Converting from sized to unsized is easy, and we already have it:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell getVector :: Vec n a -&amp;gt; V.Vector a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Converting from unsized to sized is harder. We already saw a &amp;quot;shoe-horning&amp;quot; method, if we know the size we want at compile-time:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell mkVec :: forall n. KnownNat n =&amp;gt; V.Vector a -&amp;gt; Maybe (Vec n a)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;But what if we don't know what size &lt;code&gt;n&lt;/code&gt; we want? What if we want &lt;code&gt;n&lt;/code&gt; to be whatever the actual size of the input vector is?&lt;/p&gt;
&lt;p&gt;In general we can't predict the size of our input vector at compile-time, so we can't just directly put in a size we want. What we want is a method to return a &lt;code&gt;Vec n&lt;/code&gt;, where &lt;code&gt;n&lt;/code&gt; is the length of the input vector, determined at runtime.&lt;/p&gt;
&lt;p&gt;I'm going to try to convince you that a plausible API is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell withVec     :: V.Vector a     -&amp;gt; (forall n. KnownNat n =&amp;gt; Vec n a -&amp;gt; r)     -&amp;gt; r&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(Note: this does require &lt;code&gt;RankNTypes&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;People familiar with dependent types might recognize that &lt;code&gt;withVec&lt;/code&gt; is a function that takes an unsized vector and returns an &lt;em&gt;existentially quantified&lt;/em&gt; sized vector, in CPS-style. Basically, give the function a vector, and a way to &amp;quot;handle&amp;quot; a &lt;code&gt;Vec n&lt;/code&gt; of &lt;em&gt;any possible size&lt;/em&gt;. The function will then give your handler a &lt;code&gt;Vec n&lt;/code&gt; of the proper type/size. The &lt;em&gt;function&lt;/em&gt; gets to chose the &lt;code&gt;n&lt;/code&gt; that you must handle.&lt;/p&gt;
&lt;p&gt;Within your continuation/handler, you can take advantage of the size type, and do take advantage of all of the type-level guarantees and benefits of a length-indexed vector. In a way, it is its own &amp;quot;world&amp;quot; where your vector has a fixed size. However, the caveat is that you have to treat the size &lt;em&gt;universally&lt;/em&gt; --- you have to be able to handle any possible size given to you, in a parametrically polymorphic way.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; myVector = V.fromList [10,5,8] :: V.Vector Int ghci&amp;gt; withVec myVector $ \(v :: Vec n Int) -&amp;gt;           -- in this function body, `v :: Vec 3 Int`, and `n ~ 3`           -- whatever I return here will be the return value of the entire line           case packFinite 1 :: Maybe (Finite n) of      -- Finite 3             Nothing -&amp;gt; 0             Just i  -&amp;gt; v `index` i 5&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We could write, say, a function to always safely get the third item:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L68-69 getThird :: V.Vector a -&amp;gt; Maybe a getThird v = withVec v $ \v' -&amp;gt; fmap (v' `index`) (packFinite 2)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And we can run it:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; getThird $ V.fromList [1,2,3] Just 3 ghci&amp;gt; getThird $ V.fromList [1,2] Nothing&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can even do something silly like convert an unsized vector to a sized vector and then back again:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L71-72 vectorToVector :: V.Vector a -&amp;gt; V.Vector a vectorToVector v = withVec v getVector&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now that I've (hopefully) convinced you that this function really does convert an unsized vector into a sized vector that you can use, let's see how we can implement it!&lt;/p&gt;
&lt;p&gt;To do this, we can take advantage of the &lt;code&gt;someNatVal&lt;/code&gt; function (from &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base/docs/GHC-TypeNats.html"&gt;GHC.TypeNats&lt;/a&gt;&lt;/em&gt;):&lt;/p&gt;
&lt;p&gt;```haskell data SomeNat = forall n. KnownNat n =&amp;gt; SomeNat (Proxy n)&lt;/p&gt;
&lt;p&gt;someNatVal :: Natural -&amp;gt; SomeNat ```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SomeNat&lt;/code&gt; contains what we call an existentially quantified type, &lt;code&gt;n&lt;/code&gt;. Basically, a value of &lt;code&gt;SomeNat&lt;/code&gt; contains a &lt;code&gt;Proxy n&lt;/code&gt; with &lt;em&gt;some specific &lt;code&gt;n&lt;/code&gt;&lt;/em&gt;, that is hidden &amp;quot;inside&amp;quot; the constructor. The only way to figure it out is to pattern match on the constructor and use it in a generic and parametrically polymorphic way. Sound familiar?&lt;/p&gt;
&lt;p&gt;&lt;code&gt;someNatVal&lt;/code&gt; converts &lt;code&gt;Natural&lt;/code&gt; (a non-negative Integer type) into a &lt;code&gt;SomeNat&lt;/code&gt; --- it &amp;quot;picks&amp;quot; the right &lt;code&gt;n&lt;/code&gt; (the one that corresponds to that &lt;code&gt;Natural&lt;/code&gt;) and stuffs/hides it into &lt;code&gt;SomeNat&lt;/code&gt;. We can leverage this to write our &lt;code&gt;withVec&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L74-76 withVec :: V.Vector a -&amp;gt; (forall n. KnownNat n =&amp;gt; Vec n a -&amp;gt; r) -&amp;gt; r withVec v f = case someNatVal (fromIntegral (V.length v)) of     SomeNat (Proxy :: Proxy m) -&amp;gt; f (UnsafeMkVec @m v)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(The &lt;code&gt;TypeApplications&lt;/code&gt; syntax &lt;code&gt;@m&lt;/code&gt; is used with &lt;code&gt;UnsafeMkVec&lt;/code&gt; to specify that we want a &lt;code&gt;Vec m a&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;This process is actually called &amp;quot;reification&amp;quot; -- we take a value-level runtime property (the length) and &amp;quot;reify&amp;quot; it, bringing it up to the type-level.&lt;/p&gt;
&lt;p&gt;And now, we have both of our conversion functions! We can convert from sized to unsized using &lt;code&gt;getVector&lt;/code&gt;, and from unsized to sized using &lt;code&gt;withVec&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Verifying Properties&lt;/h3&gt;
&lt;p&gt;The final useful API aspect we will be looking at is how to verify properties of our vector lengths at the type level, and let us use those properties.&lt;/p&gt;
&lt;p&gt;One common thing we might want to do is ensure that two vectors have the same length. This might happen when we use &lt;code&gt;withVec&lt;/code&gt; from two different vectors, and we get a &lt;code&gt;Vec n a&lt;/code&gt; and &lt;code&gt;Vec m a&lt;/code&gt; of two (potentially) different lengths.&lt;/p&gt;
&lt;p&gt;We can do this using &lt;code&gt;sameNat&lt;/code&gt; from &lt;em&gt;[GHC.TypeNats]&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;``&lt;code&gt;haskell --&lt;/code&gt;Type` is just a synonym for * from Data.Kind -- from the module Data.Type.Equality data (:~:) :: k -&amp;gt; k -&amp;gt; Type where Refl :: x :~: x&lt;/p&gt;
&lt;p&gt;sameNat :: (KnownNat n, KnownNat m) =&amp;gt; Proxy n -&amp;gt; Proxy m -&amp;gt; Maybe (n :~: m) ```&lt;/p&gt;
&lt;p&gt;The only way we can have a non-bottom value of type &lt;code&gt;n :~: m&lt;/code&gt; is with the &lt;code&gt;Refl&lt;/code&gt; constructor, which can only be used in the case that &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;m&lt;/code&gt; are equal. &lt;code&gt;sameNat&lt;/code&gt; gives us that &lt;code&gt;Refl&lt;/code&gt;, if possible --- that is, if &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;m&lt;/code&gt; are equal. If not, it gives us &lt;code&gt;Nothing&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, we can write:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L78-81 exactLength :: forall n m a. (KnownNat n, KnownNat m) =&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLength v = case sameNat (Proxy @n) (Proxy @m) of     Just Refl -&amp;gt; Just v     -- here, n ~ m, so a `Vec n a` is a `Vec m a`, too     Nothing   -&amp;gt; Nothing&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(We could also write this by using &lt;code&gt;getVector&lt;/code&gt; and &lt;code&gt;mkVec&lt;/code&gt;, which wraps and unwraps, but let's pretend it is expensive to construct and re-construct).&lt;/p&gt;
&lt;p&gt;Now we can do:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L37-37 zipVec :: Vec n a -&amp;gt; Vec n b -&amp;gt; Vec n (a, b)&lt;/p&gt;
&lt;p&gt;-- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrapped.hs#L83-89 zipSame :: forall a b. V.Vector a -&amp;gt; V.Vector b -&amp;gt; Maybe (V.Vector (a, b)) zipSame v1 v2 = withVec v1 $ (v1' :: Vec n a) -&amp;gt; withVec v2 $ (v2' :: Vec m b) -&amp;gt; case exactLength v1' of Just v1Same -&amp;gt; Just $ getVector (zipVec v1Same v2') -- v1' has the same length as v2' Nothing -&amp;gt; Nothing ```&lt;/p&gt;
&lt;p&gt;Which will zip two unsized vectors, but only if their lengths are the same.&lt;/p&gt;
&lt;p&gt;Now, &amp;quot;checking that the length is a certain length&amp;quot; is literally the least interesting property we can test about our vectors. There are definitely more interesting properties we can test, like whether or not our lengths are even or odd, if they are greater than a certain number, etc.; for these, the process is essentially the same: find some way, &lt;em&gt;at runtime&lt;/em&gt;, to get some sort of witness for the property you want. In our case, our witness was &lt;code&gt;n :~: m&lt;/code&gt;, which witnessed the fact that &lt;code&gt;n ~ m&lt;/code&gt;. Different libraries might provide different witnesses that might be useful. But the general process is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find a way to get your witness, using some runtime function (that will probably return &lt;code&gt;Maybe&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Pattern match on your witness, and see that property realized and usable by GHC/the type checker!&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Help from singletons&lt;/h3&gt;
&lt;p&gt;You have probably heard that &lt;code&gt;TypeNats&lt;/code&gt; provides a very bare-bones and primitive interface. This is true. Its interface also sometimes doesn't play well with other type-level mechanisms you might want to try. To prepare you for the real world, let's re-implement these things using the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/singletons"&gt;singletons&lt;/a&gt;&lt;/em&gt; library, which provides a unified interface for type-level programming in general.&lt;/p&gt;
&lt;p&gt;Instead of &lt;code&gt;KnownNat&lt;/code&gt;, &lt;code&gt;Proxy&lt;/code&gt;, &lt;code&gt;natVal&lt;/code&gt;, &lt;code&gt;SomeNat&lt;/code&gt;, and &lt;code&gt;someNatVal&lt;/code&gt;, we can use the singletons equivalents, &lt;code&gt;Sing&lt;/code&gt;, &lt;code&gt;fromSing&lt;/code&gt;, &lt;code&gt;SomeSing&lt;/code&gt;, and &lt;code&gt;toSing&lt;/code&gt;:[^singnat]&lt;/p&gt;
&lt;p&gt;```haskell -- TypeNats style natVal :: KnownNat n =&amp;gt; p n -&amp;gt; Natural&lt;/p&gt;
&lt;p&gt;-- Singletons style sing :: KnownNat n =&amp;gt; Sing n fromSing :: Sing n -&amp;gt; Natural -- (for n :: Nat)&lt;/p&gt;
&lt;p&gt;-- TypeNats style data SomeNat = forall n. KnownNat n =&amp;gt; SomeNat (Proxy n) someNatVal :: Natural -&amp;gt; SomeNat&lt;/p&gt;
&lt;p&gt;-- Singletons style data SomeSing Nat = forall n. SomeSing (Sing n) toSing :: Natural -&amp;gt; SomeSing Nat&lt;/p&gt;
&lt;p&gt;withSomeSing :: Natural -&amp;gt; (forall n. Sing n -&amp;gt; r) -&amp;gt; r&lt;/p&gt;
&lt;p&gt;-- TypeNats style sameNat :: (KnownNat n, KnownNat m) =&amp;gt; Proxy n -&amp;gt; Proxy m -&amp;gt; Maybe (n :~: m)&lt;/p&gt;
&lt;p&gt;-- Singletons style -- from Data.Singletons.Decide -- for our purposes, Decision is basically a fancy Maybe data Decision a = Proved a | Disproved (a -&amp;gt; Void) (%~) :: Sing n -&amp;gt; Sing m -&amp;gt; Decision (n :~: m) ```&lt;/p&gt;
&lt;p&gt;Hopefully the above should give you a nice &amp;quot;key&amp;quot; for translating between the two styles. But here are some practical translations:&lt;/p&gt;
&lt;p&gt;```haskell -- &amp;quot;explicit Sing&amp;quot; style -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L20-24 mkVec_ :: Sing n -&amp;gt; V.Vector a -&amp;gt; Maybe (Vec n a) mkVec_ s v | V.length v == l = Just (UnsafeMkVec v) | otherwise = Nothing where l = fromIntegral (fromSing s)&lt;/p&gt;
&lt;p&gt;-- &amp;quot;implicit&amp;quot; style -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L26-30 mkVec :: forall n a. KnownNat n =&amp;gt; V.Vector a -&amp;gt; Maybe (Vec n a) mkVec v | V.length v == l = Just (UnsafeMkVec v) | otherwise = Nothing where l = fromIntegral (fromSing (sing :: Sing n))&lt;/p&gt;
&lt;p&gt;-- alternatively, re-using &lt;code&gt;mkVec_&lt;/code&gt; mkVec :: KnownNat n =&amp;gt; V.Vector a -&amp;gt; Maybe (Vec n a) mkVec = mkVec_ sing ```&lt;/p&gt;
&lt;p&gt;As you can see, in singletons, we have the luxury of defining our functions in &amp;quot;explicit&amp;quot; style (where the user passes in a &lt;code&gt;Sing&lt;/code&gt; token which reveals what length they want) or &amp;quot;implicit&amp;quot; style (where the length is inferred from the return type, requiring a &lt;code&gt;KnownNat n =&amp;gt;&lt;/code&gt; constraint), like we have been writing up to this point. &lt;code&gt;Sing n -&amp;gt;&lt;/code&gt; and &lt;code&gt;KnownNat n =&amp;gt;&lt;/code&gt; really have the same power. You can think of &lt;code&gt;Sing n&lt;/code&gt; as a token that carries around &lt;code&gt;KnownNat n =&amp;gt;&lt;/code&gt;, in a way.&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L32-42 replicate_ :: Sing n -&amp;gt; a -&amp;gt; Vec n a replicate_ s x = UnsafeMkVec $ V.replicate l x where l = fromIntegral (fromSing s)&lt;/p&gt;
&lt;p&gt;replicate :: KnownNat n =&amp;gt; a -&amp;gt; Vec n a replicate = replicate_ sing&lt;/p&gt;
&lt;p&gt;withVec :: V.Vector a -&amp;gt; (forall n. Sing n -&amp;gt; Vec n a -&amp;gt; r) -&amp;gt; r withVec v f = case toSing (fromIntegral (V.length v)) of SomeSing s -&amp;gt; f s (UnsafeMkVec v)&lt;/p&gt;
&lt;p&gt;-- alternatively, skipping &lt;code&gt;SomeSing&lt;/code&gt; altogether: -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L44-54 withVec' :: V.Vector a -&amp;gt; (forall n. Sing n -&amp;gt; Vec n a -&amp;gt; r) -&amp;gt; r withVec' v0 f = withSomeSing (fromIntegral (V.length v0)) $ \s -&amp;gt; f s (UnsafeMkVec v0)&lt;/p&gt;
&lt;p&gt;exactLength_ :: Sing m -&amp;gt; Sing n -&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLength_ sM sN v = case sM %~ sN of Proved Refl -&amp;gt; Just v Disproved _ -&amp;gt; Nothing&lt;/p&gt;
&lt;p&gt;exactLength :: (KnownNat m, KnownNat n) =&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLength = exactLength_ sing sing ```&lt;/p&gt;
&lt;p&gt;Note that you &lt;em&gt;aren't&lt;/em&gt; required to implement both a &lt;code&gt;replicate_&lt;/code&gt; and &lt;code&gt;replicate&lt;/code&gt; --- I'm just including them here to show that both API's (implicit and explicit) are possible. (You can always just directly use &lt;code&gt;sing&lt;/code&gt; right away before getting started to get the &lt;code&gt;Sing n&lt;/code&gt; that those functions use, and so skip &lt;code&gt;replicate_&lt;/code&gt; and other explicit variants)&lt;/p&gt;
&lt;p&gt;One slight bit of friction comes when using libraries that work with &lt;code&gt;KnownNat&lt;/code&gt;, like &lt;em&gt;finite-typelits&lt;/em&gt; and the &lt;code&gt;Finite&lt;/code&gt; type. But we can convert between the two using &lt;code&gt;SNat&lt;/code&gt; or &lt;code&gt;withKnownNat&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;``&lt;code&gt;haskell -- SNat can be used to construct a&lt;/code&gt;Sing&lt;code&gt;if we have a&lt;/code&gt;KnownNat&lt;code&gt;constraint -- It can also be pattern matched on to reveal a&lt;/code&gt;KnownNat constraint` SNat :: KnownNat n =&amp;gt; Sing n&lt;/p&gt;
&lt;p&gt;-- we can give a &lt;code&gt;Sing n&lt;/code&gt; and be able to execute something in the context where -- that &lt;code&gt;n&lt;/code&gt; has a &lt;code&gt;KnownNat&lt;/code&gt; constraint withKnownNat :: Sing n -&amp;gt; (KnownNat n =&amp;gt; r) -&amp;gt; r ```&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L56-60 generate_ :: Sing n -&amp;gt; (Finite n -&amp;gt; a) -&amp;gt; Vec n a generate_ s f = withKnownNat s $ UnsafeMkVec $ V.generate l (f . fromIntegral) where l = fromIntegral (fromSing s)&lt;/p&gt;
&lt;p&gt;-- alternatively, via pattern matching: -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L63-66 generate'_ :: Sing n -&amp;gt; (Finite n -&amp;gt; a) -&amp;gt; Vec n a generate'_ s@SNat f = UnsafeMkVec $ V.generate l (f . fromIntegral) where l = fromIntegral (fromSing s)&lt;/p&gt;
&lt;p&gt;-- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs#L68-69 generate :: KnownNat n =&amp;gt; (Finite n -&amp;gt; a) -&amp;gt; Vec n a generate = generate_ sing ```&lt;/p&gt;
&lt;p&gt;You can see most of our original code (with pure &lt;code&gt;KnownNat&lt;/code&gt;) rewritten to work with singletons in &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecWrappedSingletons.hs"&gt;this file&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Why Singletons?&lt;/h4&gt;
&lt;p&gt;As you can see, singletons-style programming completely subsumes programming with &lt;code&gt;TypeNats&lt;/code&gt; and &lt;code&gt;KnownNat&lt;/code&gt;. What we don't see here is that singletons style integrates very well with the rest of the singletons ecosystem...so you might just have to take my word for it :)&lt;/p&gt;
&lt;p&gt;What we have just witnessed is the bridge between the singletons ecosystem and the rest of the Haskell ecosystem's usage of &lt;code&gt;GHC.TypeNats&lt;/code&gt;. &lt;code&gt;KnownNat&lt;/code&gt;, because it is provided by GHC itself, is universal. However, I recommend any new projects or libraries you write that do &lt;em&gt;anything&lt;/em&gt; more than the most trivial of usages of &lt;code&gt;KnownNat&lt;/code&gt; should take a look at doing things singletons-style.&lt;/p&gt;
&lt;p&gt;Working with just &lt;code&gt;GHC.TypeNats&lt;/code&gt; and &lt;code&gt;KnownNat&lt;/code&gt;, you run into limitations very quickly unless you stick to very basic things. And, if you ever work with any other type-level stuff, &lt;em&gt;singletons&lt;/em&gt; integrates very well and very smoothly with everything else type-level you do. If you plan on doing other type-level things besides just the most basic, you will not regret starting singletons-style from the beginning.&lt;/p&gt;
&lt;h3&gt;Real-World Examples&lt;/h3&gt;
&lt;p&gt;This exact pattern is used in many real-world libraries. The canonical fixed-length vector library implemented in this style is &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/vector-sized"&gt;vector-sized&lt;/a&gt;&lt;/em&gt;, which more or less re-exports the entire &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/vector"&gt;vector&lt;/a&gt;&lt;/em&gt; library, but with a statically-sized interface. This is the library I use for all my my modern sized-vector needs.&lt;/p&gt;
&lt;p&gt;It's also used to great benefit by the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hmatrix"&gt;hmatrix&lt;/a&gt;&lt;/em&gt; library, which I take advantage of in my &lt;a href="https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html"&gt;dependently typed neural networks&lt;/a&gt; tutorial series.&lt;/p&gt;
&lt;p&gt;It's also provided in the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/linear-1.20.7/docs/Linear-V.html"&gt;linear&lt;/a&gt;&lt;/em&gt; library, which was one of the first major libraries to adopt this style. However, it offers an incomplete API, and requires lens --- its main purpose is for integration with the rest of the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/linear-1.20.7/docs/Linear-V.html"&gt;linear&lt;/a&gt;&lt;/em&gt; library, which it does very well.&lt;/p&gt;
&lt;p&gt;Anyway, if all you really wanted was performant fixed-size containers, feel free to stop reading now (or jump to the conclusion). But if you want to explore a bit deeper into the world of inductive dependent types ... continue on :)&lt;/p&gt;
&lt;h2&gt;The Structural Way&lt;/h2&gt;
&lt;p&gt;So, the (a?) problem with &lt;code&gt;TypeNats&lt;/code&gt; from GHC is that it has no internal structure. It's basically the same as the &lt;code&gt;Integer&lt;/code&gt; or &lt;code&gt;Natural&lt;/code&gt; type --- every single value (constructor) is completely structurally unrelated to the next.&lt;/p&gt;
&lt;p&gt;Just like we can imagine&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell data Int = .. -2 | -1 | 0 | 1 | 2 ...&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can also think of &lt;code&gt;Nat&lt;/code&gt; as just &lt;code&gt;0 | 1 | 2 | 3 | 4 ...&lt;/code&gt;. Each constructor is completely distinct.&lt;/p&gt;
&lt;p&gt;This is useful for most practical applications. However, when we want to use our fixed-length types in a more subtle and nuanced way, it might help to work with a length type that is more...structurally aware.&lt;/p&gt;
&lt;p&gt;We've also noticed that the structure of our &lt;code&gt;Vec&lt;/code&gt; and the structure of our &lt;code&gt;Nat&lt;/code&gt; have nothing in common, so we can't take advantage of any shared structure to help us with type-safety in our implementation...everything we wrote was pretty much implemented using &amp;quot;unsafe&amp;quot; functions.&lt;/p&gt;
&lt;p&gt;So, enough of this non-structural blasphemy. We are proper dependent type programmers, dangit! We want structural verification! Compiler verification from the very bottom!&lt;/p&gt;
&lt;p&gt;For this, we'll dig into &lt;em&gt;inductive&lt;/em&gt; type-level nats.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell data Nat = Z | S Nat   deriving Eq&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We're using the &lt;code&gt;DataKinds&lt;/code&gt; extension, so not only does that define the &lt;em&gt;type&lt;/em&gt; &lt;code&gt;Nat&lt;/code&gt; with the &lt;em&gt;values&lt;/em&gt; &lt;code&gt;Z&lt;/code&gt; and &lt;code&gt;S :: Nat -&amp;gt; Nat&lt;/code&gt;, it also defines the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;Nat&lt;/code&gt; with the &lt;em&gt;types&lt;/em&gt; &lt;code&gt;'Z&lt;/code&gt; and &lt;code&gt;'S :: Nat -&amp;gt; Nat&lt;/code&gt;! (note the backticks)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t S Z Nat ghci&amp;gt; :k 'S 'Z Nat&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So &lt;code&gt;'Z&lt;/code&gt; represents 0, and &lt;code&gt;'S&lt;/code&gt; represents the &amp;quot;successor&amp;quot; function: one plus whatever number it contains. &lt;code&gt;'S 'Z&lt;/code&gt; represents 1, &lt;code&gt;'S ('S 'Z)&lt;/code&gt; represents 2, etc.&lt;/p&gt;
&lt;p&gt;And now we can define a fixed-length &lt;em&gt;list&lt;/em&gt;, which is basically a normal haskell list &amp;quot;zipped&amp;quot; with &lt;code&gt;S&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L26-30 data Vec :: Nat -&amp;gt; Type -&amp;gt; Type where VNil :: Vec 'Z a (:+) :: a -&amp;gt; Vec n a -&amp;gt; Vec ('S n) a&lt;/p&gt;
&lt;p&gt;infixr 5 :+ ```&lt;/p&gt;
&lt;p&gt;Here, we're using &lt;code&gt;GADT&lt;/code&gt; syntax to define our type using its constructors: the &lt;code&gt;VNil&lt;/code&gt; constructor (which creates a &lt;code&gt;Vec 'Z a&lt;/code&gt;, or the empty vector, like &lt;code&gt;[]&lt;/code&gt;) and the &lt;code&gt;(:+)&lt;/code&gt; constructor (like cons, or &lt;code&gt;(:)&lt;/code&gt;), which conses an item to a &lt;code&gt;Vec n a&lt;/code&gt; to get a &lt;code&gt;Vec ('S n) a&lt;/code&gt;, or a vector with one more element.&lt;/p&gt;
&lt;p&gt;Basically, all usage of nil and cons (&lt;code&gt;VNil&lt;/code&gt; and &lt;code&gt;:+&lt;/code&gt;) keeps track of the current &amp;quot;length&amp;quot; of the vectors in its type. Observe that the only way to construct a &lt;code&gt;Vec ('S ('S 'Z)) a&lt;/code&gt; is by using two &lt;code&gt;:+&lt;/code&gt;s and a &lt;code&gt;VNil&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; :t VNil Vec 'Z a ghci&amp;gt; :t True :+ VNil Vec ('S 'Z) Bool ghci&amp;gt; :t False :+ True :+ VNil Vec ('S ('S 'Z)) Bool&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Type-level Guarantees are Structurally Free&lt;/h3&gt;
&lt;p&gt;One nice thing about this is that there is no &amp;quot;unsafe&amp;quot; way to construct a &lt;code&gt;Vec&lt;/code&gt;. Any &lt;code&gt;Vec&lt;/code&gt; is &lt;em&gt;inherently of the correct size&lt;/em&gt;. The very act of constructing it enforces its length.&lt;/p&gt;
&lt;p&gt;Remember our &amp;quot;unsafe&amp;quot; &lt;code&gt;mapVec&lt;/code&gt;? We had to implement it unsafely, and trust that our implementation is correct. Even worse --- our &lt;em&gt;users&lt;/em&gt; have to trust that our implementation is correct!&lt;/p&gt;
&lt;p&gt;But writing such a &lt;code&gt;mapVec&lt;/code&gt; function using &lt;code&gt;Vec&lt;/code&gt; is guaranteed to preserve the lengths:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L32-35 mapVec :: (a -&amp;gt; b) -&amp;gt; Vec n a -&amp;gt; Vec n b mapVec f = \case VNil -&amp;gt; VNil x :+ xs -&amp;gt; f x :+ mapVec f xs&lt;/p&gt;
&lt;p&gt;-- compare to map :: (a -&amp;gt; b) -&amp;gt; [a] -&amp;gt; [b] map f = \case [] -&amp;gt; [] x:xs -&amp;gt; f x : map f xs ```&lt;/p&gt;
&lt;p&gt;Our implementation is guaranteed to have the correct length. Neat! We get all of the documentation benefits described in our previous discussion of &lt;code&gt;mapVec&lt;/code&gt;, plus more.&lt;/p&gt;
&lt;p&gt;We can write &lt;code&gt;zip&lt;/code&gt; too:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L37-42 zipVec :: Vec n a -&amp;gt; Vec n b -&amp;gt; Vec n (a, b) zipVec = \case     VNil -&amp;gt; \case       VNil -&amp;gt; VNil     x :+ xs -&amp;gt; \case       y :+ ys -&amp;gt; (x,y) :+ zipVec xs ys&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Isn't it neat how the code reads exactly like the code for map/zip for &lt;em&gt;lists&lt;/em&gt;? Because their structure is identical, their only real difference is the type-level tag. All of the functions we write are the same.&lt;/p&gt;
&lt;h4&gt;Type-Level Arithmentic&lt;/h4&gt;
&lt;p&gt;GHC provided our &lt;code&gt;+&lt;/code&gt; before, so we have to write it ourselves if we want to be able to use it for our &lt;code&gt;Nat&lt;/code&gt;s. We can write it as a type family:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L54-61 type family (n :: Nat) + (m :: Nat) :: Nat where 'Z + m = m 'S n + m = 'S (n + m)&lt;/p&gt;
&lt;p&gt;(++) :: Vec n a -&amp;gt; Vec m a -&amp;gt; Vec (n + m) a (++) = \case VNil -&amp;gt; \ys -&amp;gt; ys x :+ xs -&amp;gt; \ys -&amp;gt; x :+ (xs ++ ys) ```&lt;/p&gt;
&lt;p&gt;This works! However, we have to be careful that GHC can verify that the final vector &lt;em&gt;really does&lt;/em&gt; have the length &lt;code&gt;n + m&lt;/code&gt;. GHC can do this automatically only in very simple situations. In our situation, it is possible because &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;++&lt;/code&gt; have the &lt;em&gt;exact same structure&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Take a moment to stare at the definition of &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;++&lt;/code&gt; very closely, and then squint really hard. You can see that &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;++&lt;/code&gt; really describe the &amp;quot;same function&amp;quot;, using the exact same structure. First, if the first item is a Z-y thing, return the second item as-is. If the first item is a consy thing, return the second item consed with the rest of the first item. Roughly speaking, of course.&lt;/p&gt;
&lt;p&gt;This is a part of what we mean when we say that we can take advantage of the &lt;em&gt;structure&lt;/em&gt; of the length type. Here, the structure of &lt;code&gt;Nat&lt;/code&gt; aligns so well with the structure of &lt;code&gt;Vec&lt;/code&gt; what we can prove structural properties about &lt;code&gt;Nat&lt;/code&gt; and the &lt;code&gt;Vec&lt;/code&gt; together by exploiting their shared inductive structure.&lt;/p&gt;
&lt;p&gt;Unfortunately, for examples where the function we write doesn't exactly match the structure as the type family we write, this won't work. And sometimes, the structural properties might get in the way of what we are trying to prove/produce. An example here would be a &lt;code&gt;snoc&lt;/code&gt; function (cons to the end of a list). If you try writing it, you'll see that the structure of &lt;code&gt;Nat&lt;/code&gt; and &lt;code&gt;Vec&lt;/code&gt; fight back against you pretty hard. So, exploiting structure isn't universally useful, but it definitely helps in many situations! Handling tricky cases like this is a subject for a whole other blog post.&lt;/p&gt;
&lt;h3&gt;Indexing&lt;/h3&gt;
&lt;p&gt;To index our previous type, we used some abstract &lt;code&gt;Finite&lt;/code&gt; type, where &lt;code&gt;Finite n&lt;/code&gt; conveniently represented the type of all possible indices to a &lt;code&gt;Vec n a&lt;/code&gt;. We can do something similar, inductively, as well:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L63-67 data Fin :: Nat -&amp;gt; Type where FZ :: Fin ('S n) FS :: Fin n -&amp;gt; Fin ('S n)&lt;/p&gt;
&lt;p&gt;deriving instance Show (Fin n) ```&lt;/p&gt;
&lt;p&gt;I always thought of this inductive definition of &lt;code&gt;Fin&lt;/code&gt; as a cute trick, because I don't think there was any way I could have thought of it on my own. But if you play around it enough, you might be able to convince yourself that there are exactly &lt;code&gt;n&lt;/code&gt; inhabitants of &lt;code&gt;Fin n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, for &lt;code&gt;Fin ('S 'Z)&lt;/code&gt; (indices for a one-item vector), there should be only one inhabitant. And there is! It's &lt;code&gt;FZ&lt;/code&gt;. &lt;code&gt;FS FZ&lt;/code&gt; is not a valid inhabitant, because it has type &lt;code&gt;Fin ('S ('S m))&lt;/code&gt; for some &lt;code&gt;m&lt;/code&gt;, so cannot possibly have the type &lt;code&gt;Fin ('S 'Z)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's see the inhabitants of &lt;code&gt;Fin ('S ('S ('S 'Z)))&lt;/code&gt; (indices for three-item vectors):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; FZ              :: Fin ('S ('S ('S 'Z))) FZ ghci&amp;gt; FS FZ           :: Fin ('S ('S ('S 'Z))) FS FZ ghci&amp;gt; FS (FS FZ)      :: Fin ('S ('S ('S 'Z))) FS (FS FZ) ghci&amp;gt; FS (FS (FS FZ)) :: Fin ('S ('S ('S 'Z))) TYPE ERROR!  TYPE ERROR!  TYPE ERROR!&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As GHC informs us, &lt;code&gt;FS (FS (FS FZ))&lt;/code&gt; is not an inhabitant of &lt;code&gt;Fin ('S ('S ('S 'Z)))&lt;/code&gt;, which is exactly the behavior we wanted. This is because &lt;code&gt;FS (FS (FS FZ))&lt;/code&gt; has type &lt;code&gt;Fin ('S ('S ('S ('S m))))&lt;/code&gt; for some &lt;code&gt;m&lt;/code&gt;, and this can't fit &lt;code&gt;Fin ('S ('S ('S 'Z)))&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Also, note that there are no inhabitants of &lt;code&gt;Fin 'Z&lt;/code&gt;. There is no constructor or combination of constructors that can yield a value of that type.&lt;/p&gt;
&lt;p&gt;Armed with this handy &lt;code&gt;Fin&lt;/code&gt; type, we can do structural type-safe indexing:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L69-74 index :: Fin n -&amp;gt; Vec n a -&amp;gt; a index = \case     FZ -&amp;gt; \case       x :+ _ -&amp;gt; x     FS i -&amp;gt; \case       _ :+ xs -&amp;gt; index i xs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note that our &lt;code&gt;Fin&lt;/code&gt; type structurally precludes us from being able to index into a &lt;code&gt;Vec 'Z a&lt;/code&gt; (an empty vector), because to do that, we would have to pass in a &lt;code&gt;Fin 'Z&lt;/code&gt;...but there is no such value with that type!&lt;/p&gt;
&lt;h3&gt;Generating&lt;/h3&gt;
&lt;p&gt;Now, generating these requires some more thought. Naively writing a &lt;code&gt;replicate :: a -&amp;gt; Vec n a&lt;/code&gt; is not possible; ideally, we'd want to &amp;quot;pattern match&amp;quot; on our length &lt;code&gt;n&lt;/code&gt;, and use &lt;code&gt;VNil&lt;/code&gt; if it's &lt;code&gt;'Z&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;However, we can't pattern match on types in Haskell, because types are &lt;em&gt;erased&lt;/em&gt; at runtime. They're just used by the compiler to verify your code, but they don't exist at runtime. So, you can't just say &amp;quot;do this if &lt;code&gt;n&lt;/code&gt; is &lt;code&gt;'Z&lt;/code&gt;, otherwise do this&amp;quot;.&lt;/p&gt;
&lt;p&gt;Recall that, in our previous vector type, we needed to use a &lt;code&gt;KnownNat n&lt;/code&gt; constraint to be able to &lt;em&gt;reflect&lt;/em&gt; a &lt;code&gt;n&lt;/code&gt; type down to the value level. We can do something similar using the &lt;em&gt;singletons&lt;/em&gt; machinery!&lt;/p&gt;
&lt;p&gt;First, we need to get singletons for our &lt;code&gt;Nat&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L21-24 $(singletons [d| data Nat = Z | S Nat deriving Eq |])&lt;/p&gt;
&lt;p&gt;-- this creates: data instance Sing :: Nat -&amp;gt; Type where SZ :: Sing 'Z SS :: Sing n -&amp;gt; Sing ('S n) ```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Sing n&lt;/code&gt; is a singleton for our &lt;code&gt;Nat&lt;/code&gt;, in that there is only one &lt;code&gt;Sing n&lt;/code&gt; for every &lt;code&gt;n&lt;/code&gt;. So, if we receive a value of type &lt;code&gt;Sing n&lt;/code&gt;, we can pattern match on it to figure out what &lt;code&gt;n&lt;/code&gt; is. Essentially, we can &lt;em&gt;pattern match&lt;/em&gt; on &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L76-83 singSize :: Sing (n :: Nat) -&amp;gt; String singSize = \case     -- here, n is 'Z     SZ        -&amp;gt; &amp;quot;Size of zero!&amp;quot;     -- here, n is ('S 'Z)     SS SZ     -&amp;gt; &amp;quot;Size of one!&amp;quot;     -- here, n is ('S ('S n))     SS (SS _) -&amp;gt; &amp;quot;Wow, so big!&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We can now branch depending on what &lt;code&gt;n&lt;/code&gt; is!&lt;/p&gt;
&lt;p&gt;Basically, &lt;em&gt;we can use a singleton&lt;/em&gt; if we ever want to &amp;quot;pattern match&amp;quot; or branch our program's output based on the type. This is a general rule you will observe as we continue on this article.&lt;/p&gt;
&lt;p&gt;Note that because of the inductive nature of our original &lt;code&gt;Nat&lt;/code&gt; type, the singletons are also inductive, as well. This is handy, because then our whole ecosystem remains inductive.&lt;/p&gt;
&lt;p&gt;Now, to write &lt;code&gt;replicate&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L90-93 replicate_ :: Sing n -&amp;gt; a -&amp;gt; Vec n a replicate_ = \case     SZ   -&amp;gt; \_ -&amp;gt; VNil     SS l -&amp;gt; \x -&amp;gt; x :+ replicate_ l x&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And we can recover our original &amp;quot;implicit&amp;quot; style, with type-inference-driven lengths, using &lt;code&gt;SingI&lt;/code&gt; and &lt;code&gt;sing :: SingI n =&amp;gt; Sing n&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L95-96 replicate :: SingI n =&amp;gt; a -&amp;gt; Vec n a replicate = replicate_ sing&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can think of &lt;code&gt;SingI&lt;/code&gt; as the &amp;quot;generic singletons&amp;quot; equivalent of &lt;code&gt;KnownNat&lt;/code&gt;. &lt;code&gt;KnownNat&lt;/code&gt; lets us reflect out a &lt;code&gt;GHC.TypeNats.Nat&lt;/code&gt; to a &lt;code&gt;Sing&lt;/code&gt;...&lt;code&gt;SingI&lt;/code&gt; lets us reflect any type that has singletons defined to its corresponding &lt;code&gt;Sing&lt;/code&gt;. Since our new &lt;code&gt;Nat&lt;/code&gt; type has singletons, we basically get a free &amp;quot;&lt;code&gt;KnownNat&lt;/code&gt; equivalent&amp;quot;!&lt;/p&gt;
&lt;p&gt;See how useful the whole singletons ecosystem is? :)&lt;/p&gt;
&lt;h4&gt;Generating with indices&lt;/h4&gt;
&lt;p&gt;Writing &lt;code&gt;generate&lt;/code&gt; using the inductive &lt;code&gt;Fin&lt;/code&gt; and &lt;code&gt;Nat&lt;/code&gt; is an interesting challenge. It's actually a fairly standard pattern that comes up when working with inductive types like these. I'm going to leave it as an exercise to the reader -- click the link at the top corner of the text box to see the solution, and see how it compares to your own :)&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L98-104 generate_ :: Sing n -&amp;gt; (Fin n -&amp;gt; a) -&amp;gt; Vec n a&lt;/p&gt;
&lt;p&gt;generate :: SingI n =&amp;gt; (Fin n -&amp;gt; a) -&amp;gt; Vec n a generate = generate_ sing ```&lt;/p&gt;
&lt;p&gt;The one thing I will point out is that it is very useful that GHC verifies our code for us, and that we have typed holes to help us develop our code. If we ever don't know something, we can just use a typed hole &lt;code&gt;_&lt;/code&gt;, and GHC will tell us what type it expects, and what values in scope have that type. It is infinitely useful for situations like this, especially when you are new to this sort of dependently typed inductive programming!&lt;/p&gt;
&lt;p&gt;If you ever get stuck, try throwing in a &lt;code&gt;_&lt;/code&gt; and seeing what types GHC expects...these clues will help you get your bearings!&lt;/p&gt;
&lt;h3&gt;Between Sized and Unsized&lt;/h3&gt;
&lt;p&gt;Converting from sized to unsized vectors (to lists) is something that is pretty straightforward, and can be done by just pattern matching on the vector and recursing on the tail. I've &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L85-88"&gt;left it as an excercise&lt;/a&gt; to write &lt;code&gt;Vec n a -&amp;gt; [a]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More interesting is the other way around; our the API of converting unsized to sized vectors will be the same:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L106-106 withVec :: [a] -&amp;gt; (forall n. Sing n -&amp;gt; Vec n a -&amp;gt; r) -&amp;gt; r&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;But implementing it inductively is also an interesting challenge. See my tip above about typed holes (&lt;code&gt;_&lt;/code&gt;). I recommend taking a break here to try to solve it yourself.&lt;/p&gt;
&lt;p&gt;Ready?&lt;/p&gt;
&lt;p&gt;Welcome back! Hope you had a fun time :) Here's the solution!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L106-110 withVec :: [a] -&amp;gt; (forall n. Sing n -&amp;gt; Vec n a -&amp;gt; r) -&amp;gt; r withVec = \case     []   -&amp;gt; \f -&amp;gt; f SZ VNil     x:xs -&amp;gt; \f -&amp;gt; withVec xs $ \l ys -&amp;gt;         f (SS l) (x :+ ys)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To handle the empty list, we just return immediately, giving &lt;code&gt;f&lt;/code&gt; the proper singleton and vector (&lt;code&gt;SZ&lt;/code&gt; and &lt;code&gt;VNil&lt;/code&gt;). For the non-empty list, first we convert the tail &lt;code&gt;xs&lt;/code&gt; into a vector (&lt;code&gt;ys&lt;/code&gt;) and its corresponding length-singleton (&lt;code&gt;l&lt;/code&gt;), and then we give &lt;code&gt;f&lt;/code&gt; the &amp;quot;correct&amp;quot; length singleton of our complete vector (&lt;code&gt;SS l&lt;/code&gt;) and the correct complete vector (&lt;code&gt;x :+ ys&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;One nice property where (in contrast with our previous non-structural &lt;code&gt;withVec&lt;/code&gt;) is that GHC ensures that the length of the vector we give to &lt;code&gt;f&lt;/code&gt; is actually what we claim it is.&lt;/p&gt;
&lt;h3&gt;Verifying properties&lt;/h3&gt;
&lt;p&gt;We can create some corresponding example of &lt;code&gt;exactLength&lt;/code&gt; using the exact same process we did before&lt;/p&gt;
&lt;p&gt;First, it'd be nice to get a witness for the length of a given vector just from the vector itself:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L112-115 vecLength :: Vec n a -&amp;gt; Sing n vecLength = \case     VNil    -&amp;gt; SZ     _ :+ xs -&amp;gt; SS (vecLength xs)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The type of &lt;code&gt;vecLength :: Vec n a -&amp;gt; Sing n&lt;/code&gt; says that it is possible, from the structure of the vector given alone, to get a witness to its length. And, because the structure of the vector and the structure of the length type are so similar, this is possible! (Note that this is not possible for our non-structural &amp;quot;wrapped&amp;quot; &lt;code&gt;Vec&lt;/code&gt;, without some unsafe operations)&lt;/p&gt;
&lt;p&gt;Now, our code will be identical to the code for our wrapped/non-structural vectors, using &lt;code&gt;%~&lt;/code&gt; and &lt;code&gt;Decision&lt;/code&gt; and &lt;code&gt;Refl&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L117-123 exactLength_ :: Sing m -&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLength_ sM v = case sM %~ vecLength v of Proved Refl -&amp;gt; Just v Disproved _ -&amp;gt; Nothing&lt;/p&gt;
&lt;p&gt;exactLength :: SingI m =&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLength = exactLength_ sing ```&lt;/p&gt;
&lt;p&gt;It's nice that this is exactly the same as before, and that's a testament to how useful the singletons library is at unifying all of these distinct type-level stuffs.&lt;/p&gt;
&lt;p&gt;We could also write &lt;code&gt;exactLength&lt;/code&gt; in a cute way by inducting on the length we want and the vector, so it might be fun to look at this version instead --&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L125-135 exactLengthInductive_ :: Sing m -&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLengthInductive_ = \case SZ -&amp;gt; \case VNil -&amp;gt; Just VNil _ :+ _ -&amp;gt; Nothing SS l -&amp;gt; \case VNil -&amp;gt; Nothing x :+ xs -&amp;gt; (x :+) &amp;lt;$&amp;gt; exactLengthInductive_ l xs&lt;/p&gt;
&lt;p&gt;exactLengthInductive :: SingI m =&amp;gt; Vec n a -&amp;gt; Maybe (Vec m a) exactLengthInductive = exactLengthInductive_ sing ```&lt;/p&gt;
&lt;p&gt;This is another way you can take advantage of the &lt;em&gt;structure&lt;/em&gt; of the length type. Here, we explicitly take advantage of the inductive structure of the &lt;code&gt;Nat&lt;/code&gt; type and how it matches with the structure of the &lt;code&gt;Vec&lt;/code&gt; type, and do bold things with it![^eli]&lt;/p&gt;
&lt;p&gt;But I digress. Like in the last section, checking for a given length is literally the least interesting property you can check for. But, again, the same process is usable here: find a way to get your witness, and then pattern match on that witness.&lt;/p&gt;
&lt;p&gt;For example, we can make a witness that &lt;code&gt;n&lt;/code&gt; is less than or equal to &lt;code&gt;m&lt;/code&gt;, as well as a way to construct such a witness:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L137-149 data LTE :: Nat -&amp;gt; Nat -&amp;gt; Type where LEZ :: LTE 'Z n LES :: LTE n m -&amp;gt; LTE ('S n) ('S m)&lt;/p&gt;
&lt;p&gt;isLTE :: Sing n -&amp;gt; Sing m -&amp;gt; Decision (LTE n m) isLTE = \case SZ -&amp;gt; _ -&amp;gt; Proved LEZ SS n -&amp;gt; \case SZ -&amp;gt; Disproved $ \case -- EmptyCase SS m -&amp;gt; case isLTE n m of Proved l -&amp;gt; Proved $ LES l Disproved p -&amp;gt; Disproved $ \case LES l -&amp;gt; p l ```&lt;/p&gt;
&lt;p&gt;So, it is impossible to construct an &lt;code&gt;LTE n m&lt;/code&gt; if &lt;code&gt;n&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; less than or equal to &lt;code&gt;m&lt;/code&gt;. I dare you to try!&lt;/p&gt;
&lt;p&gt;We can write code to check for this property in our vectors:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L151-157 atLeast_ :: Sing n -&amp;gt; Vec m a -&amp;gt; Maybe (LTE n m, Vec m a) atLeast_ sN v = case isLTE sN (vecLength v) of Proved l -&amp;gt; Just (l, v) Disproved _ -&amp;gt; Nothing&lt;/p&gt;
&lt;p&gt;atLeast :: SingI n =&amp;gt; Vec m a -&amp;gt; Maybe (LTE n m, Vec m a) atLeast = atLeast_ sing ```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;atLeast_ sN&lt;/code&gt; will only return our vector if its length is &lt;em&gt;at least&lt;/em&gt; the length of the length indicated by &lt;code&gt;sN&lt;/code&gt;. Basically, we check if our vector is &amp;quot;at least&amp;quot; a certain length.&lt;/p&gt;
&lt;p&gt;We can write a function that can &amp;quot;take&amp;quot; an arbitrary amount from a vector, given (via proof) that the vector has at least that many elements:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L159-163 takeVec :: LTE n m -&amp;gt; Vec m a -&amp;gt; Vec n a takeVec = \case     LEZ   -&amp;gt; \_ -&amp;gt; VNil     LES l -&amp;gt; \case       x :+ xs -&amp;gt; x :+ takeVec l xs&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And, we can combine that with our &lt;code&gt;atLeast&lt;/code&gt; function, to be able to take (maybe)[^maybe] from any vector:&lt;/p&gt;
&lt;p&gt;```haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/fixvec-2/VecInductive.hs#L165-169 takeVecMaybe_ :: Sing n -&amp;gt; Vec m a -&amp;gt; Maybe (Vec n a) takeVecMaybe_ sN v = uncurry takeVec &amp;lt;$&amp;gt; atLeast_ sN v&lt;/p&gt;
&lt;p&gt;takeVecMaybe :: SingI n =&amp;gt; Vec m a -&amp;gt; Maybe (Vec n a) takeVecMaybe v = uncurry takeVec &amp;lt;$&amp;gt; atLeast v ```&lt;/p&gt;
&lt;h3&gt;In the Real World&lt;/h3&gt;
&lt;p&gt;This type is more like a list than a vector, so it's in a bit of an awkward position, utility-wise. You usually chose a list over a vector in Haskell when you want some sort of lazy streaming, but the cases where you want to lazily stream something &lt;em&gt;and&lt;/em&gt; you know exactly how many items you want to stream are admittedly a bit rare. GHC can't handle infinite &lt;code&gt;Vec&lt;/code&gt;s, so there's that, too. For &amp;quot;containers&amp;quot;, &lt;em&gt;vector&lt;/em&gt; is great, so the non-structural &lt;code&gt;Vec&lt;/code&gt; is seen a lot more.&lt;/p&gt;
&lt;p&gt;However, if you are working with a lot of other inductive types, &lt;code&gt;Vec&lt;/code&gt; works very naturally alongside them. It makes sense, then, that a &amp;quot;canonical&amp;quot; package offering &lt;code&gt;Vec&lt;/code&gt; is &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/type-combinators"&gt;type-combinators&lt;/a&gt;&lt;/em&gt;, an actively maintained library with loads of useful inductive types for type-level programming, exporting its own &lt;code&gt;Nat&lt;/code&gt; and &lt;code&gt;Sing&lt;/code&gt;-equivalents. If I am doing the sort of type-level programming that &lt;code&gt;Vec&lt;/code&gt; is useful for, chances are I already have &lt;em&gt;type-combinators&lt;/em&gt; imported. This is the library that I personally suggest if you want to use this &lt;code&gt;Vec&lt;/code&gt; in the real world.&lt;/p&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;There's obviously more to look at, and much more we can do with fixed-length vectors and inductive types. And, there will definitely be more issues that come up when you start working with these in the real world, with real applications.&lt;/p&gt;
&lt;p&gt;If you plan on moving into learning about dependent types, I hope that guide would be a good launching point. But if all you wanted to do was learn how to use fixed-length vectors effectively in Haskell...hopefully after reading this, you have confidence to work with these things directly, and to know what to google if anything else comes up :)&lt;/p&gt;
&lt;p&gt;Feel free as always to leave a comment or a &lt;a href="https://twitter.com/mstk"&gt;tweet&lt;/a&gt;, or find me the freenode &lt;code&gt;#haskell&lt;/code&gt; channel, as &lt;em&gt;jle`&lt;/em&gt;. I always welcome feedback, suggestions, or questions!&lt;/p&gt;</description><author>Justin Le</author><category>Haskell</category><category>Tutorials</category><category>Reference</category><guid isPermaLink="true">https://blog.jle.im/entry/fixed-length-vector-types-in-haskell.html</guid><pubDate>Fri, 25 Aug 2017 20:37:10 UTC</pubDate><creator>Justin Le</creator><subject>Haskell, Tutorials, Reference</subject><date>2017-08-25</date></item><item><title>Verify your Typeclass Instances in Haskell Today!</title><link>https://blog.jle.im/entry/verified-instances-in-haskell.html</link><description>&lt;p&gt;One of the most common gripes people have when learning Haskell is the fact that typeclass &amp;quot;laws&amp;quot; are only laws by convention, and aren't enforced by the language and compiler. When asked why, the typical response is &amp;quot;Haskell can't do that&amp;quot;, followed by a well-intentioned redirection to quickcheck or some other fuzzing library.&lt;/p&gt;
&lt;p&gt;But, to any experienced Haskeller, &amp;quot;Haskell's type system can't express X&amp;quot; is always interpreted as a (personal) challenge.&lt;/p&gt;
&lt;p&gt;GHC Haskell's type system has been advanced enough to provide verified typeclasses for a long time, since the introduction of data kinds and associated types. And with the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/singletons"&gt;singletons&lt;/a&gt;&lt;/em&gt; library, it's now as easy as ever.&lt;/p&gt;
&lt;p&gt;(The code for this post is available &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/verified-instances/VerifiedInstances.hs"&gt;here&lt;/a&gt; if you want to follow along! Some of the examples here involving &lt;code&gt;Demote&lt;/code&gt; and relying on its injectivity will only work with &lt;a href="https://github.com/goldfirere/singletons"&gt;singletons HEAD&lt;/a&gt;, even though the necessary patches were made seven months ago[^shackage])&lt;/p&gt;
&lt;h2&gt;Semigroups&lt;/h2&gt;
&lt;p&gt;Let's start simple -- everyone's favorite structural addition to magmas, &lt;a href="http://hackage.haskell.org/package/base-4.9.1.0/docs/Data-Semigroup.html"&gt;semigroups&lt;/a&gt;. A semigroup is a type with an associative binary operation, &lt;code&gt;(&amp;lt;&amp;gt;)&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell class Semigroup a where     (&amp;lt;&amp;gt;) :: a -&amp;gt; a -&amp;gt; a&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Its one law is associativity:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell (x &amp;lt;&amp;gt; y) &amp;lt;&amp;gt; z = x &amp;lt;&amp;gt; (y &amp;lt;&amp;gt; z)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;But, this class stinks, because it's super easy to write bad instances:&lt;/p&gt;
&lt;p&gt;```haskell data List a = Nil | Cons a (List a) deriving Show&lt;/p&gt;
&lt;p&gt;infixr 5 &lt;code&gt;Cons&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;instance Semigroup (List a) where Nil &amp;lt;&amp;gt; ys = ys Cons x xs &amp;lt;&amp;gt; ys = Cons x (ys &amp;lt;&amp;gt; xs) ```&lt;/p&gt;
&lt;p&gt;This instance isn't associative:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; ((1 `Cons` 2 `Cons` Nil) &amp;lt;&amp;gt; (3 `Cons` 4 `Cons` Nil)) &amp;lt;&amp;gt; (5 `Cons` 6 `Cons` Nil) 1 `Cons` 5 `Cons` 3 `Cons` 6 `Cons` 2 `Cons` 4 `Cons` Nil ghci&amp;gt; (1 `Cons` 2 `Cons` Nil) &amp;lt;&amp;gt; ((3 `Cons` 4 `Cons` Nil) &amp;lt;&amp;gt; (5 `Cons` 6 `Cons` Nil)) 1 `Cons` 3 `Cons` 2 `Cons` 5 `Cons` 4 `Cons` 6 `Cons` Nil&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;But if you try to compile it, GHC doesn't complain at all. Is this an error on the part of Haskell? Not quite; it's an error on the part of the &lt;code&gt;Semigroup&lt;/code&gt; typeclass not requiring proofs that the instance is indeed associative.&lt;/p&gt;
&lt;p&gt;Let's try again.&lt;/p&gt;
&lt;h3&gt;Verify me, Captain&lt;/h3&gt;
&lt;p&gt;We will now define &lt;code&gt;Semigroup&lt;/code&gt; on the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;List&lt;/code&gt;, using &lt;code&gt;-XDataKinds&lt;/code&gt;, instead of the type.&lt;/p&gt;
&lt;p&gt;```haskell class Semigroup a where type (x :: a) &amp;lt;&amp;gt; (y :: a) :: a&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(%&amp;lt;&amp;gt;) :: Sing (x :: a) -&amp;gt; Sing (y :: a) -&amp;gt; Sing (x &amp;lt;&amp;gt; y)

appendAssoc
    :: Sing (x :: a)
    -&amp;gt; Sing (y :: a)
    -&amp;gt; Sing (z :: a)
    -&amp;gt; ((x &amp;lt;&amp;gt; y) &amp;lt;&amp;gt; z) :~: (x &amp;lt;&amp;gt; (y &amp;lt;&amp;gt; z))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;Now, &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; exists not as a function on &lt;em&gt;values&lt;/em&gt;, but as a function on &lt;em&gt;types&lt;/em&gt;. &lt;code&gt;%&amp;lt;&amp;gt;&lt;/code&gt; is a function that performs &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; at the value level, written to work with singletons representing the input types, so that GHC can verify that it is identical to the type family &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;. (it's 100% boilerplate and should pretty much exactly match the &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; type family).[^apdf] Finally, &lt;code&gt;appendAssoc&lt;/code&gt; is a proof that the type family &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; is associative, using &lt;code&gt;:~:&lt;/code&gt; (type equality witness) from &lt;code&gt;Data.Type.Equality&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This means that, if a type is an instance of &lt;code&gt;Semigroup&lt;/code&gt;, it not only has to provide &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;/&lt;code&gt;%&amp;lt;&amp;gt;&lt;/code&gt;, but also a &lt;em&gt;proof that they are associative&lt;/em&gt;. You can't write the full instance without it!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Semigroup&lt;/code&gt; is a &amp;quot;kind-class&amp;quot;, because it is a bunch of methods and types associated with a certain kind. Which &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; is dispatched when you do something like &lt;code&gt;x &amp;lt;&amp;gt; y&lt;/code&gt; depends on the &lt;em&gt;kind&lt;/em&gt; of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. GHC does &amp;quot;kind inference&amp;quot; and uses the &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; corresponding to the kinds of &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;SingKind&lt;/code&gt; typeclass from the singletons library, we can move back and forth from &lt;code&gt;Sing x&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;, and get our original (value-level) &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; back:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell (&amp;lt;&amp;gt;)     :: (SingKind m, Semigroup m)     =&amp;gt; Demote m     -&amp;gt; Demote m     -&amp;gt; Demote m x &amp;lt;&amp;gt; y = withSomeSing x $ \sX -&amp;gt;            withSomeSing y $ \sY -&amp;gt;              fromSing (sX %&amp;lt;&amp;gt; sY)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(This works best with singletons HEAD at the moment, because &lt;code&gt;Demote&lt;/code&gt; is injective. On 2.2 or lower, using this would require an explicit type application or annotation at any place you use &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; or &lt;code&gt;%&amp;lt;&amp;gt;&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Now, let's write the instance for &lt;code&gt;List&lt;/code&gt;. First, we need to define the singletons:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell data instance Sing (xs :: List a) where     SNil  :: Sing Nil     SCons :: Sing x -&amp;gt; Sing xs -&amp;gt; Sing (Cons x xs)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then, we can define the instance, using the traditional &lt;code&gt;(++)&lt;/code&gt; appending that lists famously have:&lt;/p&gt;
&lt;p&gt;```haskell instance Semigroup (List a) where type Nil &amp;lt;&amp;gt; ys = ys type Cons x xs &amp;lt;&amp;gt; ys = Cons x (xs &amp;lt;&amp;gt; ys)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SNil       %&amp;lt;&amp;gt; ys = ys
SCons x xs %&amp;lt;&amp;gt; ys = SCons x (xs %&amp;lt;&amp;gt; ys)

appendAssoc = \case
  SNil       -&amp;gt; \_ _ -&amp;gt; Refl
  SCons x xs -&amp;gt; \ys zs -&amp;gt;
    case appendAssoc xs ys zs of
      Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;Like I promised, &lt;code&gt;%&amp;lt;&amp;gt;&lt;/code&gt; is a boilerplate re-implementation of &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;, to manipulate value-level witnesses. &lt;code&gt;appendAssoc&lt;/code&gt; is the interesting bit: It's our proof. It reads like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If the first list is &lt;code&gt;Nil&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- left hand side (Nil &amp;lt;&amp;gt; ys) &amp;lt;&amp;gt; zs   = ys &amp;lt;&amp;gt; zs        -- definition of `(Nil &amp;lt;&amp;gt;)` -- right hand side Nil &amp;lt;&amp;gt; (ys &amp;lt;&amp;gt; zs)   = ys &amp;lt;&amp;gt; zs        -- definition of `(Nil &amp;lt;&amp;gt;)`&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So, no work needed. QED! (Or, as we say in Haskell, &lt;code&gt;Refl&lt;/code&gt;!)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the first list is &lt;code&gt;Cons x xs&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell -- left hand side (Cons x xs &amp;lt;&amp;gt; ys) &amp;lt;&amp;gt; zs   = (Cons x (xs &amp;lt;&amp;gt; ys)) &amp;lt;&amp;gt; zs   -- definition of `(Cons x xs &amp;lt;&amp;gt;)`   = Cons x ((xs &amp;lt;&amp;gt; ys) &amp;lt;&amp;gt; zs)   -- definition of `(Cons x xs &amp;lt;&amp;gt;)` -- right hand side Cons x xs &amp;lt;&amp;gt; (ys &amp;lt;&amp;gt; zs)   = Cons x (xs &amp;lt;&amp;gt; (ys &amp;lt;&amp;gt; zs))   -- definition of `(Cons x xs &amp;lt;&amp;gt;)`&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So, the problem reduces to proving that &lt;code&gt;(xs &amp;lt;&amp;gt; ys) &amp;lt;&amp;gt; zs&lt;/code&gt; is equal to &lt;code&gt;xs &amp;lt;&amp;gt; (ys &amp;lt;&amp;gt; zs)&lt;/code&gt;. If we can do that, then we can prove that the whole things are equal. We generate that proof using &lt;code&gt;appendAssoc xs ys zs&lt;/code&gt;, and, wit that proof in scope...QED!&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And, we're done!&lt;/p&gt;
&lt;p&gt;Note that if you had tried any &lt;em&gt;non-associative&lt;/em&gt; implementation of &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; (and &lt;code&gt;%&amp;lt;&amp;gt;&lt;/code&gt;), GHC would reject it because you wouldn't have been able to write the proof!&lt;/p&gt;
&lt;h4&gt;Automatic Singletons&lt;/h4&gt;
&lt;p&gt;Deriving &lt;code&gt;Sing&lt;/code&gt; and &lt;code&gt;SingKind&lt;/code&gt; and both versions of &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; is kind of tedious, so it's useful to use template haskell to do it all for us:&lt;/p&gt;
&lt;p&gt;```haskell $(singletons [d| data List a = Nil | Cons a (List a) deriving (Show)&lt;/p&gt;
&lt;p&gt;infixr 5 &lt;code&gt;Cons&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;appendList :: List a -&amp;gt; List a -&amp;gt; List a appendList Nil ys = ys appendList (Cons x xs) ys = Cons x (appendList xs ys) |])&lt;/p&gt;
&lt;p&gt;instance Semigroup (List a) where type xs &amp;lt;&amp;gt; ys = AppendList xs ys (%&amp;lt;&amp;gt;) = sAppendList&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;appendAssoc = \case
  SNil       -&amp;gt; \_ _ -&amp;gt; Refl
  SCons _ xs -&amp;gt; \ys zs -&amp;gt;
    case appendAssoc xs ys zs of
      Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;The boilerplate of re-defining &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; as &lt;code&gt;%&amp;lt;&amp;gt;&lt;/code&gt; goes away!&lt;/p&gt;
&lt;p&gt;And now, we we can do:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; print $ ((1::Integer) `Cons` 2 `Cons` Nil) &amp;lt;&amp;gt; (3 `Cons` 4 `Cons` Nil) 1 `Cons` 2 `Cons` 3 `Cons` 4 `Cons` Nil&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Ta dah!&lt;/p&gt;
&lt;h3&gt;Naturally, Maybe&lt;/h3&gt;
&lt;p&gt;Now that we have our basic infrastructure, let's implement some other famous semigroups:&lt;/p&gt;
&lt;p&gt;First, the inductive nats, &lt;code&gt;data N = Z | S N:&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;```haskell $(singletons [d| data N = Z | S N deriving (Show)&lt;/p&gt;
&lt;p&gt;plus :: N -&amp;gt; N -&amp;gt; N plus Z y = y plus (S x) y = S (plus x y) |])&lt;/p&gt;
&lt;p&gt;instance Semigroup N where type xs &amp;lt;&amp;gt; ys = Plus xs ys (%&amp;lt;&amp;gt;) = sPlus&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;appendAssoc = \case
  SZ -&amp;gt; \_ _ -&amp;gt; Refl
  SS x -&amp;gt; \y z -&amp;gt;
    case appendAssoc x y z of
      Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;And the standard instance for &lt;code&gt;Maybe&lt;/code&gt;, which lifts the underlying semigroup:&lt;/p&gt;
&lt;p&gt;```haskell $(singletons [d| data Option a = None | Some a deriving (Show) |])&lt;/p&gt;
&lt;p&gt;instance Semigroup a =&amp;gt; Semigroup (Option a) where type None &amp;lt;&amp;gt; y = y type x &amp;lt;&amp;gt; None = x type Some x &amp;lt;&amp;gt; Some y = Some (x &amp;lt;&amp;gt; y)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SNone   %&amp;lt;&amp;gt; y       = y
x       %&amp;lt;&amp;gt; SNone   = x
SSome x %&amp;lt;&amp;gt; SSome y = SSome (x %&amp;lt;&amp;gt; y)

appendAssoc = \case
    SNone   -&amp;gt; \_ _ -&amp;gt; Refl
    SSome x -&amp;gt; \case
      SNone -&amp;gt; \_ -&amp;gt; Refl
      SSome y -&amp;gt; \case
        SNone -&amp;gt; Refl
        SSome z -&amp;gt;
          case appendAssoc x y z of
            Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell ghci&amp;gt; print $ S (S Z) &amp;lt;&amp;gt; S Z S (S (S Z)) ghci&amp;gt; print $ Some (S Z) &amp;lt;&amp;gt; Some (S (S (S Z))) Some (S (S (S (S Z)))) ghci&amp;gt; print $ None       &amp;lt;&amp;gt; Some (S (S (S Z))) Some (S (S (S Z)))&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Going Monoidal&lt;/h2&gt;
&lt;p&gt;Of course, we can now introduce the &lt;code&gt;Monoid&lt;/code&gt; typeclass, which introduces a new element &lt;code&gt;empty&lt;/code&gt;, along with the laws that appending with empty leaves things unchanged:&lt;/p&gt;
&lt;p&gt;```haskell class Semigroup a =&amp;gt; Monoid a where type Empty a :: a&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sEmpty :: Sing (Empty a)

emptyIdentLeft
    :: Sing x
    -&amp;gt; (Empty a &amp;lt;&amp;gt; x) :~: x

emptyIdentRight
    :: Sing x
    -&amp;gt; (x &amp;lt;&amp;gt; Empty a) :~: x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;empty :: (SingKind m, Monoid m) =&amp;gt; Demote m empty = fromSing sEmpty ```&lt;/p&gt;
&lt;p&gt;Because working implicitly return-type polymorphism at the type level can be annoying sometimes, we have &lt;code&gt;Empty&lt;/code&gt; take the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;a&lt;/code&gt; as a parameter, instead of having it be inferred through kind inference like we did for &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;. That is, &lt;code&gt;Empty (List a)&lt;/code&gt; is &lt;code&gt;Empty&lt;/code&gt; for the &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;List a&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As usual in Haskell, the instances write themselves!&lt;/p&gt;
&lt;p&gt;```haskell instance Monoid (List a) where type Empty (List a) = Nil&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sEmpty = SNil
emptyIdentLeft _ = Refl
emptyIdentRight  = \case
  SNil -&amp;gt; Refl
  SCons _ xs -&amp;gt;
    case emptyIdentRight xs of
      Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;instance Monoid N where type Empty N = Z&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sEmpty = SZ
emptyIdentLeft _ = Refl
emptyIdentRight  = \case
  SZ -&amp;gt; Refl
  SS x -&amp;gt; case emptyIdentRight x of
    Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;instance Semigroup a =&amp;gt; Monoid (Option a) where type Empty (Option a) = None&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sEmpty = SNone
emptyIdentLeft  _ = Refl
emptyIdentRight _ = Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;h2&gt;Play that Funcy Music&lt;/h2&gt;
&lt;p&gt;How about some higher-kinded typeclasses?&lt;/p&gt;
&lt;p&gt;```haskell class Functor f where type Fmap a b (g :: a ~&amp;gt; b) (x :: f a) :: f b&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sFmap
    :: Sing (g            :: a ~&amp;gt; b)
    -&amp;gt; Sing (x            :: f a   )
    -&amp;gt; Sing (Fmap a b g x :: f b   )

-- | fmap id x == x
fmapId
    :: Sing (x :: f a)
    -&amp;gt; Fmap a a IdSym0 x :~: x

-- | fmap f (fmap g x) = fmap (f . g) x
fmapCompose
    :: Sing (g :: b ~&amp;gt; c)
    -&amp;gt; Sing (h :: a ~&amp;gt; b)
    -&amp;gt; Sing (x :: f a   )
    -&amp;gt; Fmap b c g (Fmap a b h x) :~: Fmap a c (((:.$) @@ g) @@ h) x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Fmap a b g x&lt;/code&gt; maps the &lt;em&gt;type-level function&lt;/em&gt; &lt;code&gt;g :: a ~&amp;gt; b&lt;/code&gt; over &lt;code&gt;x :: f a&lt;/code&gt;, and returns a type of kind &lt;code&gt;f b&lt;/code&gt;. Like with &lt;code&gt;Empty&lt;/code&gt;, to help with kind inference, we have &lt;code&gt;Fmap&lt;/code&gt; explicitly requre the &lt;em&gt;kinds&lt;/em&gt; of the input and results of &lt;code&gt;g&lt;/code&gt; (&lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;) so GHC doesn't have to struggle to infer it implicitly.&lt;/p&gt;
&lt;p&gt;And, of course, along with &lt;code&gt;sFmap&lt;/code&gt; (the singleton mirror of &lt;code&gt;Fmap&lt;/code&gt;), we have our laws: &lt;code&gt;fmap id x = x&lt;/code&gt;, and &lt;code&gt;fmap g (fmap h) x = fmap (g . h) x&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But, what are &lt;code&gt;a ~&amp;gt; b&lt;/code&gt;, &lt;code&gt;IdSym0&lt;/code&gt;, &lt;code&gt;:.$&lt;/code&gt;, and &lt;code&gt;@@&lt;/code&gt;? They're a part of the &lt;em&gt;defunctionalization&lt;/em&gt; system that the singletons library uses. A &lt;code&gt;g :: a ~&amp;gt; b&lt;/code&gt; means that &lt;code&gt;g&lt;/code&gt; represents a type-level function taking a type of kind &lt;code&gt;a&lt;/code&gt; to a type of kind &lt;code&gt;b&lt;/code&gt;, but, importantly, encodes it in a way that makes Haskell happy. This hack is required because you can't partially apply type families in Haskell. If &lt;code&gt;g&lt;/code&gt; was a regular old &lt;code&gt;a -&amp;gt; b&lt;/code&gt; type family, you wouldn't be able to pass just &lt;code&gt;g&lt;/code&gt; into &lt;code&gt;Fmap a b g&lt;/code&gt; (because it'd be partially applied, and type families always have to appear fully saturated).&lt;/p&gt;
&lt;p&gt;You can convert a &lt;code&gt;g :: a ~&amp;gt; b&lt;/code&gt; back into a regular old &lt;code&gt;g :: a -&amp;gt; b&lt;/code&gt; using &lt;code&gt;Apply&lt;/code&gt;, or its convenient infix synonym &lt;code&gt;@@&lt;/code&gt;, like &lt;code&gt;g @@ (x :: a) :: b&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The singletons library provides &lt;code&gt;type family Id a where Id a = a&lt;/code&gt;, but we can't pass in &lt;code&gt;Id&lt;/code&gt; directly into &lt;code&gt;Fmap&lt;/code&gt;. We have to pass in its &amp;quot;defunctionalized&amp;quot; encoding, &lt;code&gt;IdSym0 :: a ~&amp;gt; a&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the composition law, we use &lt;code&gt;(:.$)&lt;/code&gt; (which is a defunctionalized type-level &lt;code&gt;.&lt;/code&gt;) and apply it to &lt;code&gt;g&lt;/code&gt; and &lt;code&gt;h&lt;/code&gt; to get, essentially, &lt;code&gt;g :. h&lt;/code&gt;, where &lt;code&gt;:.&lt;/code&gt; is type-level function composition.&lt;/p&gt;
&lt;p&gt;Now we Haskell.&lt;/p&gt;
&lt;p&gt;```haskell $(singletons [d| mapOption :: (a -&amp;gt; b) -&amp;gt; Option a -&amp;gt; Option b mapOption _ None = None mapOption f (Some x) = Some (f x)&lt;/p&gt;
&lt;p&gt;mapList :: (a -&amp;gt; b) -&amp;gt; List a -&amp;gt; List b mapList _ Nil = Nil mapList f (Cons x xs) = Cons (f x) (mapList f xs) |])&lt;/p&gt;
&lt;p&gt;instance Functor Option where type Fmap a b g x = MapOption g x&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sFmap = sMapOption
fmapId = \case
  SNone   -&amp;gt; Refl
  SSome _ -&amp;gt; Refl

fmapCompose _ _ = \case
  SNone   -&amp;gt; Refl
  SSome _ -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;instance Functor List where type Fmap a b g x = MapList g x&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sFmap = sMapList
fmapId = \case
  SNil       -&amp;gt; Refl
  SCons _ xs -&amp;gt;
    case fmapId xs of
      Refl -&amp;gt; Refl

fmapCompose g h = \case
  SNil -&amp;gt; Refl
  SCons _ xs -&amp;gt;
    case fmapCompose g h xs of
      Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;p&gt;And there you have it. A verified &lt;code&gt;Functor&lt;/code&gt; typeclass, ensuring that all instances are lawful. Never tell me that Haskell's type system can't do anything ever again!&lt;/p&gt;
&lt;p&gt;Note that any mistakes in implementation (like, for example, having &lt;code&gt;mapOption _ _ = None&lt;/code&gt;) will cause a compile-time error now, because the proofs are impossible to provide.&lt;/p&gt;
&lt;p&gt;As a side note, I'm not quite sure how to implement the value-level &lt;code&gt;fmap&lt;/code&gt; from this, since I can't figure out how to promote functions nicely. Using &lt;code&gt;sFmap&lt;/code&gt; is the only way to work with this at the value level that I can see, but it's probably because of my own lack of understanding. If anyone knows how to do this, please let me know!&lt;/p&gt;
&lt;p&gt;Anyway, what an exciting journey and a wonderful conclusion. I hope you enjoyed this and will begin using this in your normal day-to-day Haskell. Goodbye, until next time!&lt;/p&gt;
&lt;h2&gt;Just one more&lt;/h2&gt;
&lt;p&gt;Hah! Of course we aren't done. I wouldn't let you down like that. I know that you probably saw that the entire last section's only purpose was to build up to the piÃ¨ce de rÃ©sistance: the crown jewel of every Haskell article, the Monad.&lt;/p&gt;
&lt;p&gt;```haskell class Functor f =&amp;gt; Monad f where type Return a (x :: a) :: f a type Bind a b (m :: f a) (g :: a ~&amp;gt; f b) :: f b&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sReturn
    :: Sing (x :: a)
    -&amp;gt; Sing (Return a x :: f a)

sBind
    :: Sing (m :: f a)
    -&amp;gt; Sing (g :: a ~&amp;gt; f b)
    -&amp;gt; Sing (Bind a b m g)

-- | (return x &amp;gt;&amp;gt;= f) == f x
returnIdentLeft
    :: Sing (x :: a)
    -&amp;gt; Sing (g :: a ~&amp;gt; f b)
    -&amp;gt; Bind a b (Return a x) g :~: (g @@ x)

-- | (m &amp;gt;&amp;gt;= return) == m
returnIdentRight
    :: Sing (m :: f a)
    -&amp;gt; Bind a a m ReturnSym0 :~: m

-- | m &amp;gt;&amp;gt;= (\x -&amp;gt; f x &amp;gt;&amp;gt;= h) == (m &amp;gt;&amp;gt;= f) &amp;gt;&amp;gt;= h
bindCompose
    :: Sing (m :: f a)
    -&amp;gt; Sing (g :: a ~&amp;gt; f b)
    -&amp;gt; Sing (h :: b ~&amp;gt; f c)
    -&amp;gt; Bind a c m (KCompSym2 a b c g h) :~: Bind b c (Bind a b m g) h&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;data ReturnSym0 :: a ~&amp;gt; f a type instance Apply (ReturnSym0 :: a ~&amp;gt; f a) (x :: a) = Return a x&lt;/p&gt;
&lt;p&gt;type KComp a b c (g :: a ~&amp;gt; f b) (h :: b ~&amp;gt; f c) (x :: a) = Bind b c (g @@ x) h data KCompSym2 a b c g h :: (a ~&amp;gt; f c) type instance Apply (KCompSym2 a b c g h :: a ~&amp;gt; f c) (x :: a) = KComp a b c g h x&lt;/p&gt;
&lt;p&gt;return :: (SingKind a, SingKind (f a), Monad f) =&amp;gt; Demote a -&amp;gt; Demote (f a) return x = withSomeSing x $ \sX -&amp;gt; fromSing (sReturn sX) ```&lt;/p&gt;
&lt;p&gt;To help with kind inference, again, we provide explicit kind arguments for &lt;code&gt;Return&lt;/code&gt; (the kind of the thing that is being lifted) and &lt;code&gt;Bind&lt;/code&gt; (the original &lt;code&gt;a&lt;/code&gt; and the resulting &lt;code&gt;b&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Some boilerplate exists there at the bottom --- it's the plumbing for the defunctionalization system. &lt;code&gt;returnIdentRight&lt;/code&gt; requires a defunctionalized version of &lt;code&gt;Return&lt;/code&gt;, so we can provide that by defining &lt;code&gt;ReturnSym0&lt;/code&gt;, and writing an &lt;code&gt;Apply&lt;/code&gt; instance for it (which &amp;quot;applies&amp;quot; it the parameter &lt;code&gt;x&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We introduce &lt;code&gt;KComp&lt;/code&gt; (kleisli composition) and its defunctionalized version in order to express the third law, because we don't yet have type-level lambdas in Haskell. The actual function it is expressing is &lt;code&gt;\x -&amp;gt; f x &amp;gt;&amp;gt;= g&lt;/code&gt;, and that definition is given on the &lt;code&gt;type KComp a b c ... = Bind ...&lt;/code&gt; line. &lt;code&gt;KCompSym2&lt;/code&gt; is the defunctioanlized version, which is not a &lt;code&gt;a -&amp;gt; f c&lt;/code&gt; but rather an &lt;code&gt;a ~&amp;gt; f c&lt;/code&gt;, which allows it to be partially applied (like we do for &lt;code&gt;composeBind&lt;/code&gt;). And, finally, to hook all of this up into the defunctionalization system, we write an &lt;code&gt;Apply&lt;/code&gt; instance yet again.&lt;/p&gt;
&lt;p&gt;And, again, if anyone knows how I can write a value-level &lt;code&gt;Bind&lt;/code&gt;, I'd definitely appreciate hearing!&lt;/p&gt;
&lt;p&gt;Let's see some sample implementations.&lt;/p&gt;
&lt;p&gt;```haskell $(singletons [d| bindOption :: Option a -&amp;gt; (a -&amp;gt; Option b) -&amp;gt; Option b bindOption None _ = None bindOption (Some x) f = f x&lt;/p&gt;
&lt;p&gt;concatMapList :: (a -&amp;gt; List b) -&amp;gt; List a -&amp;gt; List b concatMapList _ Nil = Nil concatMapList f (Cons x xs) = f x &lt;code&gt;appendList&lt;/code&gt; concatMapList f xs |])&lt;/p&gt;
&lt;p&gt;instance Monad Option where type Return a x = Some x type Bind a b m g = BindOption m g&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sReturn = SSome
sBind   = sBindOption

returnIdentLeft _ _ = Refl
returnIdentRight = \case
  SNone   -&amp;gt; Refl
  SSome x -&amp;gt; case sReturn x of
    SSome _ -&amp;gt; Refl
bindCompose = \case
  SNone   -&amp;gt; \_ _ -&amp;gt; Refl
  SSome _ -&amp;gt; \_ _ -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;instance Monad List where type Return a x = PureList x type Bind a b m g = ConcatMapList g m&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sReturn   = sPureList
sBind x f = sConcatMapList f x

returnIdentLeft x g = case sReturn x of
  SCons y SNil -&amp;gt; case emptyIdentRight (unSingFun1 Proxy g y) of
    Refl -&amp;gt; Refl

returnIdentRight = \case
  SNil       -&amp;gt; Refl
  SCons _ xs -&amp;gt; case returnIdentRight xs of
    Refl -&amp;gt; Refl

bindCompose = \case
  SNil       -&amp;gt; \_ _ -&amp;gt; Refl
  SCons x xs -&amp;gt; \g h -&amp;gt; case bindCompose xs g h of
    Refl -&amp;gt; case unSingFun1 Proxy g x of
      SNil       -&amp;gt; Refl
      SCons y ys -&amp;gt;
        let gxs  = sConcatMapList g xs
            hgxs = sConcatMapList h gxs
            hy   = unSingFun1 Proxy h y
            hys  = sConcatMapList h ys
        in  case distribConcatMap h ys gxs of
              Refl -&amp;gt; case appendAssoc hy hys hgxs of
                Refl -&amp;gt; Refl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;-- | Proving that concatMap distributes over &amp;lt;&amp;gt; distribConcatMap :: Sing (g :: a ~&amp;gt; List b) -&amp;gt; Sing (xs :: List a) -&amp;gt; Sing (ys :: List a) -&amp;gt; ConcatMapList g (xs &amp;lt;&amp;gt; ys) :~: (ConcatMapList g xs &amp;lt;&amp;gt; ConcatMapList g ys) distribConcatMap g = \case SNil -&amp;gt; _ -&amp;gt; Refl SCons x xs -&amp;gt; \ys -&amp;gt; case distribConcatMap g xs ys of Refl -&amp;gt; let gx = unSingFun1 Proxy g x cmgxs = sConcatMapList g xs cmgys = sConcatMapList g ys in case appendAssoc gx cmgxs cmgys of Refl -&amp;gt; Refl ```&lt;/p&gt;
&lt;p&gt;Here we use &lt;code&gt;unSingFun1&lt;/code&gt;, which converts a singleton of a type-level function into a value-level function on singletons:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;haskell unSingFun1     :: Proxy (f      :: a ~&amp;gt; b)     -&amp;gt; Sing  (f      :: a ~&amp;gt; b)     -&amp;gt; Sing  (x      :: a)     -&amp;gt; Sing  (f @@ x :: b)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Proxy&lt;/code&gt; argument only exists for historical reasons, I believe. But, the crux is that, given a &lt;code&gt;Sing (f :: a ~&amp;gt; b)&lt;/code&gt; and a &lt;code&gt;Sing (x :: a)&lt;/code&gt;, we can &amp;quot;apply&amp;quot; them to get &lt;code&gt;Sing (f @@ x :: b)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The proofs for the list instance is admittedly ugly to write, due to the fact that &lt;code&gt;List&lt;/code&gt; is a recursive type. It's also tricky because Haskell has poor to little support for theorem proving and no real tools to help you write them efficiently. But, the proofs for &lt;code&gt;Option&lt;/code&gt; are really something, aren't they? It's kind of amazing how much GHC can do on its own without requiring any manual proving on the part of the user.&lt;/p&gt;
&lt;h2&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;Don't do this in actual code, please (&lt;a href="https://twitter.com/mstk/status/848677244478279680"&gt;why?&lt;/a&gt;). This post started off as an April Fools joke that accidentally compiled correctly for reasons which I cannot explain.&lt;/p&gt;
&lt;p&gt;While I don't recommend that you do this in actual code, but definitely do recommend that you do it for fun! The code in this post is available &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/verified-instances/VerifiedInstances.hs"&gt;here&lt;/a&gt; if you want to play around!&lt;/p&gt;</description><author>Justin Le</author><category>Haskell</category><guid isPermaLink="true">https://blog.jle.im/entry/verified-instances-in-haskell.html</guid><pubDate>Sat,  1 Apr 2017 20:25:04 UTC</pubDate><creator>Justin Le</creator><subject>Haskell</subject><date>2017-04-01</date></item><item><title>Introducing the Hamilton library</title><link>https://blog.jle.im/entry/introducing-the-hamilton-library.html</link><description>&lt;p&gt;&lt;a href="http://i.imgur.com/Vaaa2EC.gifv"&gt;&lt;img src="/img/entries/hamilton/double-pendulum.gif" title="My name is William Rowan Hamilton" alt="My name is William Rowan Hamilton" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hamilton&lt;/strong&gt;: &lt;a href="https://github.com/mstksg/hamilton#readme"&gt;README&lt;/a&gt; / &lt;a href="http://hackage.haskell.org/package/hamilton"&gt;hackage&lt;/a&gt; / &lt;a href="https://github.com/mstksg/hamilton"&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hamilton"&gt;hamilton&lt;/a&gt;&lt;/em&gt; library is on hackage! It was mostly a proof-of-concept toy experiment to simulate motion on bezier curves, but it became usable enough and accurate enough (to my surprise, admittedly) that I finished up some final touches to make it complete and put it on hackage as a general-purpose physics simulator.&lt;/p&gt;
&lt;p&gt;The library is, in short, a way to simulate a physical system by stating nothing more than an arbitrary parameterization of a system (a &amp;quot;generalized coordinate&amp;quot;) and a potential energy function.&lt;/p&gt;
&lt;p&gt;I was going to write a Haskell post on the implementation, which was what interested me at first. I wanted to go over --&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Using &lt;a href="https://hackage.haskell.org/package/ad"&gt;automatic differentiation&lt;/a&gt; to automatically compute momentum and the hamilton equations, which are solutions of differential equations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using type-indexed vectors and dependent types in a seamless way to encode the dimensionality of the generalized coordinate systems and to encode invariants the types of functions.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And fun stuff like that. But that post might be a bit of a while away, so I'm just going to write a post about the usage of the library. (Fair warning, most of this information is also found in the &lt;a href="https://github.com/mstksg/hamilton#readme"&gt;readme&lt;/a&gt;.)&lt;/p&gt;
&lt;h3&gt;Hamiltonian Mechanics&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(This section goes briefly over some relevant part of the physics behind Hamiltonian dynamics, but feel free to skip it if you want to go straight to the Haskell)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Hamiltonian_mechanics"&gt;Hamiltonian mechanics&lt;/a&gt; is a brilliant, radical, and beautiful re-imagination of the physics of mechanics and dynamics by &lt;a href="https://www.youtube.com/watch?v=SZXHoWwBcDc"&gt;William Rowan Hamilton&lt;/a&gt;. It was adapted for statistical mechanics and thermodynamics, and it was through the lens of Hamiltonian mechanics that Schroedinger and Heisenberg independently found insight that unlocked the secrets of quantum mechanics. While Newton's interpretation of mechanics (in terms of forces and accelerations) was cute, it simply didn't generalize to quantum mechanics. Hamiltonian's interpretation of mechanics &lt;em&gt;did&lt;/em&gt;, and we have a century of physics revolutions to thank for it. Hamiltonian mechanics also generalize without any extra work to relativity -- another case where newtonian mechanics tends to fall apart.&lt;/p&gt;
&lt;p&gt;Hamiltonian mechanics, in a classical sense, imagines that the state of the system exists as a point in &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_space"&gt;phase space&lt;/a&gt;&lt;/em&gt;, and that the system evolves based on geometric properties of the system's &lt;em&gt;Hamiltonian&lt;/em&gt; over that phase space.&lt;/p&gt;
&lt;p&gt;&lt;img src="/img/entries/hamilton/phase-space.gif" alt="Animation of particles traveling in phase space (top) over time, from Wikipedia" /&gt;&lt;/p&gt;
&lt;p&gt;In other words, define the Hamiltonian of the system, and you see the step-by-step evolution and dynamics of the system. You can imagine mechanics as a series of streams of flow over phase space...and the state of the system just goes along for the ride.&lt;/p&gt;
&lt;p&gt;One nice thing about phase space is that it can be stated in terms of any arbitrary parameterization/coordinate system of your system. For example, for a &lt;a href="https://en.wikipedia.org/wiki/Double_pendulum"&gt;double pendulum&lt;/a&gt; system, you can imagine the system as traveling about in the phase space of the angles of the bobs (instead of their actual positions in cartesian space). If you can find &lt;em&gt;any&lt;/em&gt; way to parameterize your system, in any sort of type of coordinates, then Hamiltonian mechanics will describe how it evolves in those coordinates.&lt;/p&gt;
&lt;p&gt;State some fundamental geometric properties about your coordinate system, and the Hamiltonian figures out the rest. It's the key to unlocking the dynamical properties of the system.&lt;/p&gt;
&lt;p&gt;I could go into more details, but this isn't a post about Hamiltonian mechanics! Armed with this, let's look into modeling an actual double pendulum system in terms of the angles of the bobs.&lt;/p&gt;
&lt;h3&gt;Examples&lt;/h3&gt;
&lt;h4&gt;The Double Pendulum&lt;/h4&gt;
&lt;p&gt;So, if we're going to be simulating a double pendulum system using &lt;em&gt;hamilton&lt;/em&gt;, we need three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A statement of our parameterized coordinates and how they relate to the underlying cartesian coordinates of our system&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The inertias (in our case, masses) of each of those underlying coordinates.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A potential energy function (in our case, just the potential energy induced by gravity)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We have two coordinates here ($\theta&lt;em&gt;1$ and $\theta&lt;/em&gt;2$), which will be encoding the positions of the two pendulums:&lt;/p&gt;
&lt;p&gt;$$ \langle x&lt;em&gt;1, y&lt;/em&gt;1 \rangle = \left\langle \sin (\theta&lt;em&gt;1), - \cos (\theta&lt;/em&gt;1) \right\rangle $$&lt;/p&gt;
&lt;p&gt;$$ \langle x&lt;em&gt;2, y&lt;/em&gt;2 \rangle = \left\langle \sin (\theta&lt;em&gt;1) + \frac{1}{2} \sin (\theta&lt;/em&gt;2), - \cos (\theta&lt;em&gt;1) - \frac{1}{2} \cos (\theta&lt;/em&gt;2) \right\rangle $$&lt;/p&gt;
&lt;p&gt;(Assuming that the first pendulum has length 1 and the second pendulum has length $\frac{1}{2}$)&lt;/p&gt;
&lt;p&gt;The inertias of $x&lt;em&gt;1$, $y&lt;/em&gt;1$, $x&lt;em&gt;2$, and $y&lt;/em&gt;2$ are the &amp;quot;masses&amp;quot; attached to them. Let's pick that the first bob has mass $1$ and the second bob has mass $2$, so then our masses are $\langle 1, 1, 2, 2 \rangle$.&lt;/p&gt;
&lt;p&gt;Finally, the potential energy of our system is just the potential energy of gravity, $m \times g \times y$ for each of our points:&lt;/p&gt;
&lt;p&gt;$$ U(x&lt;em&gt;1, y&lt;/em&gt;1, x&lt;em&gt;2, y&lt;/em&gt;2) = ( y&lt;em&gt;1 + 2 y&lt;/em&gt;2 ) g $$&lt;/p&gt;
&lt;p&gt;Turns out that this is a complete enough description of our system to let &lt;em&gt;hamilton&lt;/em&gt; do the rest!&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L10-25 doublePendulum :: System 4 2 doublePendulum = mkSystem' masses coordinates potential where masses :: R 4 masses = vec4 1 1 2 2 coordinates :: Floating a =&amp;gt; V.Vector 2 a -&amp;gt; V.Vector 4 a coordinates (V2 Î¸1 Î¸2) = V4 (sin Î¸1) (-cos Î¸1) (sin Î¸1 + sin Î¸2/2) (-cos Î¸1 - cos Î¸2/2) potential :: Num a =&amp;gt; V.Vector 4 a -&amp;gt; a potential (V4 _ y1 _ y2) = (y1 + 2 * y2) * 5 -- assuming g = 5 ~~~&lt;/p&gt;
&lt;p&gt;(with some &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L27-35"&gt;helper patterns&lt;/a&gt; defined here -- &lt;code&gt;V2&lt;/code&gt; and &lt;code&gt;V4&lt;/code&gt; -- that lets us pattern match on and construct sized &lt;code&gt;Vector&lt;/code&gt;s and their 2 (or 4) elements)&lt;/p&gt;
&lt;p&gt;Ta dah. That's literally all we need.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;System m n&lt;/code&gt; represents a description of a physical system (without its state) described with &lt;code&gt;n&lt;/code&gt; parameters/generalized coordinates. The &lt;code&gt;m&lt;/code&gt; represents the dimension of its underlying cartesian coordinate system (&lt;code&gt;4&lt;/code&gt; for us, with $\langle x&lt;em&gt;1, y&lt;/em&gt;1, x&lt;em&gt;2, y&lt;/em&gt;2 \rangle$). The &lt;code&gt;m&lt;/code&gt; should be more or less irrelevant to the actual &lt;em&gt;usage&lt;/em&gt; of &lt;code&gt;System m n&lt;/code&gt; and the &lt;em&gt;hamilton&lt;/em&gt; api...but it's mostly useful only if we eventually want to plot the system in normal cartesian space.&lt;/p&gt;
&lt;p&gt;Now, let's run the simulation. First we have to pick a starting configuration:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L37-39 config0 :: Config 2 config0 = Cfg (vec2 1 0 ) -- initial positions (vec2 0 0.5) -- initial velocities ~~~&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;Config n&lt;/code&gt; represents the state of the system, represented in configuration-space. But, remember, Hamiltonian dynamics is about simulating the path of the particle through &lt;em&gt;phase space&lt;/em&gt;. So we can convert our configuration-space state into a phase-space state:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L41-42 phase0 :: Phase 2 phase0 = toPhase doublePendulum config0 ~~~&lt;/p&gt;
&lt;p&gt;And now we can ask for the state of our system at any amount of points in time:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L44-45 evolution :: [Phase 2] evolution = evolveHam' doublePendulum phase0 [0,0.1 .. 1] ~~~&lt;/p&gt;
&lt;p&gt;The result there will be the state of the system at times 0, 0.01, 0.02, 0.03 ... etc.&lt;/p&gt;
&lt;p&gt;Or, if you want to run the system step-by-step:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L47-48 evolution' :: [Phase 2] evolution' = iterate (stepHam 0.1 doublePendulum) phase0 ~~~&lt;/p&gt;
&lt;p&gt;And you can get the position of the coordinates as:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L50-51 positions :: [R 2] positions = phsPositions &amp;lt;$&amp;gt; evolution' ~~~&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;phsPositions :: Phase n -&amp;gt; R n&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And the position in the underlying cartesian space as:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L53-54 positions' :: [R 4] positions' = underlyingPos doublePendulum &amp;lt;$&amp;gt; positions ~~~&lt;/p&gt;
&lt;p&gt;Where &lt;code&gt;underlyingPos :: System m n -&amp;gt; Phase n -&amp;gt; R m&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let's ignore the underlying position for now, and print out now the full progression of the system's positions:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/DoublePendulum.hs#L56-57 main :: IO () main = withRows (take 25 positions) (disp 4) ~~~&lt;/p&gt;
&lt;p&gt;(&lt;code&gt;withRows&lt;/code&gt; is from &lt;em&gt;hmatrix&lt;/em&gt;, which treats a list of vectors as a matrix with each vector as a row, and &lt;code&gt;disp 5&lt;/code&gt; from &lt;em&gt;hmatrix&lt;/em&gt; pretty-prints our matrix with 5 decimal places of precision)&lt;/p&gt;
&lt;p&gt;~~~ L 25 2 1.0000 0.0000 0.9727 0.0800 0.8848 0.2345 0.7164 0.5129 0.4849 0.8725 0.2878 1.0648 0.1223 1.0801 -0.0165 0.9388 -0.1099 0.6400 -0.1161 0.1447 -0.0539 -0.4882 -0.0795 -0.9212 -0.1689 -1.1797 -0.2860 -1.2970 -0.4146 -1.2803 -0.5562 -1.1238 -0.7249 -0.8079 -0.8762 -0.4505 -0.9442 -0.2075 -0.9416 -0.0516 -0.8793 0.0312 -0.7596 0.0265 -0.5728 -0.1086 -0.3001 -0.4237 -0.0381 -0.6640 ~~~&lt;/p&gt;
&lt;p&gt;Neat! We see that the first coordinate ($\theta&lt;em&gt;1$) starts at 1 like we asked, and then begins decreasing and falling... And then we see the second coordinate ($\theta&lt;/em&gt;2$) starting at 0 and then &amp;quot;swinging&amp;quot; to the right. The &lt;a href="http://i.imgur.com/Vaaa2EC.gifv"&gt;image the top of this post&lt;/a&gt; is an animation of such a system (albeit with $m_2 = 1$).&lt;/p&gt;
&lt;h4&gt;Two-body system&lt;/h4&gt;
&lt;p&gt;Here's one more situation where generalized coordinates describe things in a lot nicer way than cartesian coordinates: the classic two-body problem.&lt;/p&gt;
&lt;p&gt;Really, you can describe the state of a two-body system with only two parameters: the distance between the two bodies, and their current angle of rotation.&lt;/p&gt;
&lt;p&gt;In this framework, Kepler tells us that for bodies in orbit, the distance will grow smaller and larger again over time, and that the angle of rotation will constantly increase...and increase at a faster rate when the distance is smaller (which is &lt;a href="https://en.wikipedia.org/wiki/Kepler&amp;#39;s_laws_of_planetary_motion#Second_law"&gt;Kepler's second law&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If we assume that the center of mass of the system is at $\langle 0, 0 \rangle$, then we can state these coordinates as&lt;/p&gt;
&lt;p&gt;$$ \langle x&lt;em&gt;1, y&lt;/em&gt;1 \rangle = \langle r&lt;em&gt;1 \cos (\theta), r&lt;/em&gt;1 \sin (\theta) \rangle $$&lt;/p&gt;
&lt;p&gt;$$ \langle x&lt;em&gt;2, y&lt;/em&gt;2 \rangle = \langle r&lt;em&gt;2 \cos (\theta), r&lt;/em&gt;2 \sin (\theta) \rangle $$&lt;/p&gt;
&lt;p&gt;Where $r&lt;em&gt;1 = \frac{m&lt;/em&gt;2}{m&lt;em&gt;1 + m&lt;/em&gt;2}$ and $r&lt;em&gt;2 = - \frac{m&lt;/em&gt;1}{m&lt;em&gt;1 + m&lt;/em&gt;2}$ (solving from the center of mass).[^com]&lt;/p&gt;
&lt;p&gt;Our potential energy function is Newton's famous &lt;a href="https://en.wikipedia.org/wiki/Newton&amp;#39;s_law_of_universal_gravitation"&gt;law of universal gravitation&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;$$ U(r, \theta) = - \frac{G m&lt;em&gt;1 m&lt;/em&gt;2}{r} $$&lt;/p&gt;
&lt;p&gt;And, this should be enough to go for &lt;em&gt;hamilton&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&amp;quot;But wait,&amp;quot; I hear you say. &amp;quot;If we're doing a change-of-coordinate-system into polar coordinates, don't we have to account for artifacts like centrifugal acceleration from the fact that $d \theta$ is non-uniform and depends on $r$?&amp;quot;&lt;/p&gt;
&lt;p&gt;Well, I'm glad you asked! And the answer is, nope. We don't have to account for any weird interplay from non-uniform coordinate systems because &lt;em&gt;hamilton&lt;/em&gt; arrives at the proper solution simply from the geometry of the generalized coordinates. (And it does this using &lt;a href="https://hackage.haskell.org/package/ad"&gt;ad&lt;/a&gt;, but more on that for a later post!)&lt;/p&gt;
&lt;p&gt;Anyway, here we go:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/TwoBody.hs#L10-42 twoBody :: System 4 2 twoBody = mkSystem masses coordinates potential where masses :: R 4 masses = vec4 10 10 1 1 coordinates :: Floating a =&amp;gt; V.Vector 2 a -&amp;gt; V.Vector 4 a coordinates (V2 r Î¸) = V4 (r1 * cos Î¸) (r1 * sin Î¸) (r2 * cos Î¸) (r2 * sin Î¸) where r1 = r * 1 / 11 r2 = - r * 10 / 11 potential :: Fractional a =&amp;gt; V.Vector 2 a -&amp;gt; a potential (V2 r _) = - 10 / r -- G = 1&lt;/p&gt;
&lt;p&gt;config0 :: Config 2 config0 = Cfg (vec2 2 0) -- initial positions (vec2 0 0.5) -- initial velocities ~~~&lt;/p&gt;
&lt;p&gt;(we use &lt;code&gt;mkSystem&lt;/code&gt; instead of &lt;code&gt;mkSystem'&lt;/code&gt; because we want to state the potential energy in terms of our generalized coordinates $r$ and $\theta$)&lt;/p&gt;
&lt;p&gt;Let's take a peek:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/hamilton/TwoBody.hs#L44-60 phase0 :: Phase 2 phase0 = toPhase twoBody config0&lt;/p&gt;
&lt;p&gt;evolution' :: [Phase 2] evolution' = iterate (stepHam 0.1 twoBody) phase0&lt;/p&gt;
&lt;p&gt;positions :: [R 2] positions = phsPositions &amp;lt;$&amp;gt; evolution'&lt;/p&gt;
&lt;p&gt;main :: IO () main = withRows (take 25 positions) (disp 4) ~~~&lt;/p&gt;
&lt;p&gt;~~~ L 25 2 2.0000 0.0000 1.9887 0.0502 1.9547 0.1015 1.8972 0.1554 1.8149 0.2133 1.7058 0.2777 1.5669 0.3523 1.3933 0.4435 1.1774 0.5647 0.9057 0.7503 0.5516 1.1413 0.2057 3.4946 0.6092 5.2275 0.9490 5.5664 1.2115 5.7386 1.4207 5.8542 1.5889 5.9424 1.7233 6.0152 1.8283 6.0785 1.9069 6.1358 1.9610 6.1892 1.9917 6.2403 1.9998 6.2904 1.9852 6.3407 1.9479 6.3923 ~~~&lt;/p&gt;
&lt;p&gt;Neat! We see that $r$ starts big and gets smaller, and then gets big again. And it's clear that when $r$ is smallest, $\theta$ changes the fastest. Look at it go!&lt;/p&gt;
&lt;p&gt;Here's an animation of the same situation with some different masses:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://i.imgur.com/TDEHTcb.gifv"&gt;&lt;img src="/img/entries/hamilton/two-body.gif" alt="The two-body solution" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Just you wait&lt;/h3&gt;
&lt;p&gt;Now, this isn't all just useful for physics. You can state a lot of animation/dynamics problems as motion along coordinates that aren't always trivial. This project started, after all, as a way to simulate constant-velocity motion along a bezier curve. (In that case, the single coordinate is the non-uniform time parameter to the bezier curve.)&lt;/p&gt;
&lt;p&gt;I've included more examples in the &lt;a href="https://github.com/mstksg/hamilton#example-app-runner"&gt;example app launcher&lt;/a&gt; included in the library (which generated those animations you see above), including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A spring hanging from a block sliding along a horizontal rail (a favorite of many physics students, of course)&lt;/li&gt;
&lt;li&gt;A ball bouncing around a room, showing that you can represent bouncy walls as potential energy functions&lt;/li&gt;
&lt;li&gt;The bezier curve example.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let me know in the comments if you think of any interesting systems to apply this to, or if you have any interesting applications in physical or non-physical ways! I'd love to hear :D&lt;/p&gt;
&lt;p&gt;And if you're interested in the implementation using some of those Haskell tricks I mentioned above, stay tuned :)&lt;/p&gt;</description><author>Justin Le</author><category>Haskell</category><category>Projects</category><guid isPermaLink="true">https://blog.jle.im/entry/introducing-the-hamilton-library.html</guid><pubDate>Mon, 28 Nov 2016 17:28:32 UTC</pubDate><creator>Justin Le</creator><subject>Haskell, Projects</subject><date>2016-11-28</date></item><item><title>Practical Dependent Types in Haskell 2: Existential Neural Networks and Types at
Runtime</title><link>https://blog.jle.im/entry/practical-dependent-types-in-haskell-2.html</link><description>&lt;p&gt;We're back to continue on &lt;a href="https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html"&gt;our journey&lt;/a&gt; in using dependent types to write type-safe neural networks! In &lt;a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html"&gt;Part 1&lt;/a&gt;, we wrote things out in normal, untyped Haskell, and looked at red flags and general design principles that nudged us in the direction of adding dependent types to our program. We learned to appreciate what dependent types offered in terms of guiding us in writing our code, helping the compiler check our correctness, providing a better interface for users, and more.&lt;/p&gt;
&lt;p&gt;We also learned how to use singletons to work around some of Haskell's fundamental limitations to let us &amp;quot;pattern match&amp;quot; on the structure of types, and how to use typeclasses to generate singletons reflecting the structure of types we are dealing with.&lt;/p&gt;
&lt;p&gt;(If you read &lt;a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html"&gt;Part 1&lt;/a&gt; &lt;em&gt;before&lt;/em&gt; the singletons section was re-written to use the &lt;a href="https://hackage.haskell.org/package/singletons"&gt;singletons&lt;/a&gt; library, &lt;a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html#singletons-and-induction"&gt;here's a link to the section&lt;/a&gt; in specific. This tutorial will assume familiarity with what is discussed there!)&lt;/p&gt;
&lt;p&gt;All of what we've dealt with so far has essentially been with types that are fixed at compile-time. All the networks we've made have had &amp;quot;static&amp;quot; types, with their sizes in their types indicated directly in the source code. In this post, we're going to dive into the world of types that &lt;em&gt;depend&lt;/em&gt; on factors unknown until runtime, and see how dependent types in a strongly typed language like Haskell helps us write safer, more correct, and more maintainable code.&lt;/p&gt;
&lt;p&gt;This post was written for GHC 8 on stackage snapshot &lt;a href="https://www.stackage.org/nightly-2016-06-28"&gt;nightly-2016-06-28&lt;/a&gt;, but should work with GHC 7.10 for the most part. All of the set-up instructions and caveats (like the &lt;em&gt;singletons-2.0.1&lt;/em&gt; bug affecting GHC 7.10 users and the unreleased &lt;em&gt;hmatrix&lt;/em&gt; version) are the same as for &lt;a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html#setup"&gt;part 1's setup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All of the code in this post is &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs"&gt;downloadable as a standalone source file&lt;/a&gt; so you can follow along!&lt;/p&gt;
&lt;p&gt;A fair disclosure: a lot of this post doesn't actually directly deal with machine learning or neural networks. Most of it will be learning general principles for working with dependent types through implementing things you'd want to do with neural networks. More stuff with ML will come in the next posts!&lt;/p&gt;
&lt;h2&gt;Types at Runtime&lt;/h2&gt;
&lt;p&gt;Recall the type we had for our neural networks:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; :k Network Network :: Nat -&amp;gt; [Nat] -&amp;gt; Nat -&amp;gt; * ~~~&lt;/p&gt;
&lt;p&gt;They're of the form &lt;code&gt;Network i hs o&lt;/code&gt;, where &lt;code&gt;i&lt;/code&gt; is the size of the input vector it expects, &lt;code&gt;hs&lt;/code&gt; is the list of hidden layer sizes, and &lt;code&gt;o&lt;/code&gt; is the size of the output vector it produces. Something of type &lt;code&gt;Network 10 '[6, 4] 3&lt;/code&gt; is a network with 10 input nodes, two input layers of size 6 and 4, and 3 output nodes.&lt;/p&gt;
&lt;p&gt;This is great and all, but there's an severe limitation to this: Haskell is a statically typed language, right? So doesn't this mean that using a network requires that you know the entire structure of the network at compile-time?&lt;/p&gt;
&lt;p&gt;It's conceivable that you might be able to have the input and output sizes known at compile-time, but it's possible that you don't care or know your hidden layer structure. You might load it from a configuration file, or have it depend on user input. You might even want to receive one over a network channel without knowing what the internal structure is. But can a type really depend on things that you can't know until runtime?&lt;/p&gt;
&lt;p&gt;To illustrate more clearly:&lt;/p&gt;
&lt;p&gt;~~~haskell main :: IO () main = do putStrLn &amp;quot;What hidden layer structure do you want?&amp;quot; hs &amp;lt;- readLn :: IO [Integer] net &amp;lt;- randomNet :: IO (Network 10 ??? 3) -- what is ??? -- ...? ~~~&lt;/p&gt;
&lt;p&gt;We &lt;em&gt;want&lt;/em&gt; to put &lt;code&gt;hs&lt;/code&gt; there where &lt;code&gt;???&lt;/code&gt; is, but...&lt;code&gt;???&lt;/code&gt; has to be a type (of kind &lt;code&gt;[Nat]&lt;/code&gt;). &lt;code&gt;hs&lt;/code&gt; is a value (of type &lt;code&gt;[Integer]&lt;/code&gt;). It's clear here that the &lt;em&gt;type&lt;/em&gt; of our network depends on something we can't write down or decide until runtime.&lt;/p&gt;
&lt;h3&gt;An Existential Crisis&lt;/h3&gt;
&lt;p&gt;There are two main ways to go about solving this issue in Haskell. We'll look at both, and then see that they are really actually just two styles of doing the same thing.&lt;/p&gt;
&lt;h4&gt;Types hiding behind constructors&lt;/h4&gt;
&lt;p&gt;Now, having the entire structure of your neural network in the type is nice and all for cool tricks like &lt;code&gt;randomNet&lt;/code&gt;...but do you &lt;em&gt;really&lt;/em&gt; want to work with this directly? After all, from the user's perspective, the user really only ever needs to know &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;o&lt;/code&gt;: What vectors the network &lt;em&gt;expects&lt;/em&gt; and what vectors the network &lt;em&gt;outputs&lt;/em&gt;. In the end, a (feed-forward) Neural Network is really just a fancy &lt;code&gt;R i -&amp;gt; R o&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Remember, the main benefits of having the entire structure in the type was to help us &lt;em&gt;implement&lt;/em&gt; our functions more safely, with the compiler's help, and also for cute return type polymorphism tricks like &lt;code&gt;randomNet&lt;/code&gt; and &lt;code&gt;getNet&lt;/code&gt; and some stronger documentation. The &lt;em&gt;user&lt;/em&gt; of the network really only benefits from the second two types of benefits.&lt;/p&gt;
&lt;p&gt;One practical downside of having the structure in the type is that you can't store them in the same list or data structure. A &lt;code&gt;Network 10 '[5,3] 1&lt;/code&gt; won't share a list with a &lt;code&gt;Network 10 '[5,2] 1&lt;/code&gt;, despite having the same inputs/outputs (and API).&lt;/p&gt;
&lt;p&gt;Imagine that we had written a &lt;code&gt;Network&lt;/code&gt; type that &lt;em&gt;didn't&lt;/em&gt; have the internal structure in the type ---&lt;/p&gt;
&lt;p&gt;~~~haskell data OpaqueNet i o ~~~&lt;/p&gt;
&lt;p&gt;Recall that our issue earlier was that we had to write &lt;code&gt;Network i ??? o&lt;/code&gt;, but we had no idea what to put in for &lt;code&gt;???&lt;/code&gt;. But, if we worked with an &lt;code&gt;OpaqueNet i o&lt;/code&gt;, we wouldn't even care! We wouldn't have to tell GHC what the internal structure is.&lt;/p&gt;
&lt;p&gt;I'd actually argue that &lt;code&gt;OpaqueNet&lt;/code&gt; might often be the more useful type to offer to your users (or to use yourself), because it only exposes the types that are &lt;em&gt;relevant&lt;/em&gt; to its usage/API. You can store them in a list or MVar --- &lt;code&gt;[OpaqueNet 10 3]&lt;/code&gt; and &lt;code&gt;MVar (OpaqueNet 10 3)&lt;/code&gt;, serialize/deserialize them without knowing their internal structure in advance (&lt;code&gt;loadNet :: FilePath -&amp;gt; IO (OpaqueNet 10 3)&lt;/code&gt;), etc. (if you wanted to load a &lt;code&gt;Network&lt;/code&gt;, you would need to know exactly what internal structure was stored, in advance). Though &lt;code&gt;Network&lt;/code&gt; is a much easier type to &lt;em&gt;implement&lt;/em&gt;, &lt;code&gt;OpaqueNet&lt;/code&gt; is a often a more ideal type to &lt;em&gt;use&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can implement our vision for &lt;code&gt;OpaqueNet&lt;/code&gt; as an &amp;quot;existential&amp;quot; wrapper over &lt;code&gt;Network&lt;/code&gt;, actually:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L110-111 data OpaqueNet :: Nat -&amp;gt; Nat -&amp;gt; * where ONet :: Network i hs o -&amp;gt; OpaqueNet i o ~~~&lt;/p&gt;
&lt;p&gt;So, if you have &lt;code&gt;net :: Network 6 '[10,6,3] 2&lt;/code&gt;, you can create &lt;code&gt;ONet net :: OpaqueNet 6 2&lt;/code&gt;. When you use the &lt;code&gt;ONet&lt;/code&gt; constructor, the structure of the hidden layers disappears from the type!&lt;/p&gt;
&lt;p&gt;We can use the network inside by &lt;em&gt;pattern matching&lt;/em&gt; on &lt;code&gt;ONet&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L113-125 runOpaqueNet :: (KnownNat i, KnownNat o) =&amp;gt; OpaqueNet i o -&amp;gt; R i -&amp;gt; R o runOpaqueNet (ONet n) x = runNet n x&lt;/p&gt;
&lt;p&gt;numHiddens :: OpaqueNet i o -&amp;gt; Int numHiddens (ONet n) = go n where go :: Network i hs o -&amp;gt; Int go = \case O _ -&amp;gt; 0 _ :&amp;amp;~ n' -&amp;gt; 1 + go n' ~~~&lt;/p&gt;
&lt;p&gt;With the &lt;em&gt;ScopedTypeVariables&lt;/em&gt; extension, we can even bring &lt;code&gt;hs&lt;/code&gt; back into scope, as in:&lt;/p&gt;
&lt;p&gt;~~~haskell case oN of ONet (n :: Network i hs o) -&amp;gt; ... ~~~&lt;/p&gt;
&lt;p&gt;This pattern is sometimes called the &lt;strong&gt;dependent pair&lt;/strong&gt;, because pattern matching on &lt;code&gt;ONet&lt;/code&gt; yields the hidden &lt;strong&gt;existentially quantified&lt;/strong&gt; type (&lt;code&gt;hs&lt;/code&gt;) and also a value whose type is based on it (&lt;code&gt;Network i hs o&lt;/code&gt;). It's like &lt;code&gt;hs&lt;/code&gt; &amp;quot;paired&amp;quot; with &lt;code&gt;Network i hs o&lt;/code&gt;. Pattern match on the results to give both the type (&lt;code&gt;hs&lt;/code&gt;) &lt;em&gt;and&lt;/em&gt; the data structure. (To make this more explicit, we could have implemented it as &lt;code&gt;ONet :: Sing hs -&amp;gt; Network i hs o -&amp;gt; OpaqueNet i o&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;And here's the key to making this all work: once you pattern match on &lt;code&gt;ONet&lt;/code&gt;, you have to handle the &lt;code&gt;hs&lt;/code&gt; in a &lt;em&gt;completely polymorphic way&lt;/em&gt;. You're not allowed to assume anything about &lt;code&gt;hs&lt;/code&gt;...you have to provide a completely parametrically polymorphic way of dealing with it!&lt;/p&gt;
&lt;p&gt;For example, this function is completely &lt;em&gt;not&lt;/em&gt; ok:&lt;/p&gt;
&lt;p&gt;~~~haskell bad :: OpaqueNet i o -&amp;gt; Network i hs o bad (ONet n) = n -- nope, not ok at all. ~~~&lt;/p&gt;
&lt;p&gt;Why not? Well, a type signature like &lt;code&gt;OpaqueNet i o -&amp;gt; Network i hs o&lt;/code&gt; means that the &lt;em&gt;caller&lt;/em&gt; can decide what &lt;code&gt;hs&lt;/code&gt; can be --- just like &lt;code&gt;read :: Read a =&amp;gt; String -&amp;gt; a&lt;/code&gt;, where the caller decides what &lt;code&gt;a&lt;/code&gt; is.&lt;/p&gt;
&lt;p&gt;Of course, this isn't the case in the way we've written the function...the function can only return a &lt;em&gt;specific&lt;/em&gt; &lt;code&gt;hs&lt;/code&gt; (namely, the &lt;code&gt;hs&lt;/code&gt; of the network that &lt;code&gt;ONet&lt;/code&gt; hides). The &lt;em&gt;caller&lt;/em&gt; has to accommodate whatever is inside &lt;code&gt;ONet&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;The Universal and the Existential&lt;/h4&gt;
&lt;p&gt;We just brushed here on something at the heart of using existential types in Haskell: the issue of who has the power to decide what the types will be instantiated as. Most polymorphic functions you work with in Haskell are &amp;quot;universally qualified&amp;quot;. For example, for a function like&lt;/p&gt;
&lt;p&gt;~~~haskell map :: (a -&amp;gt; b) -&amp;gt; [a] -&amp;gt; [b] ~~~&lt;/p&gt;
&lt;p&gt;&lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are universally quantified, which means that the person who &lt;em&gt;uses&lt;/em&gt; &lt;code&gt;map&lt;/code&gt; gets to decide what &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are. To be more explicit, that type signature can be written as:&lt;/p&gt;
&lt;p&gt;~~~haskell map :: forall a b. (a -&amp;gt; b) -&amp;gt; [a] -&amp;gt; [b] ~~~&lt;/p&gt;
&lt;p&gt;This means that &lt;code&gt;map&lt;/code&gt; is defined in a way that will work for &lt;em&gt;any&lt;/em&gt; &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; that the &lt;em&gt;caller&lt;/em&gt; wants. As a caller, you can request:&lt;/p&gt;
&lt;p&gt;~~~haskell map :: (Int -&amp;gt; Bool) -&amp;gt; [Int] -&amp;gt; [Bool] map :: (Double -&amp;gt; Void) -&amp;gt; [Double] -&amp;gt; [Void] map :: (String -&amp;gt; (Bool -&amp;gt; Char)) -&amp;gt; [String] -&amp;gt; [Bool -&amp;gt; Char] ~~~&lt;/p&gt;
&lt;p&gt;Or anything else!&lt;/p&gt;
&lt;p&gt;Consequentially, the function has to be implemented in a way that will work for &lt;em&gt;any&lt;/em&gt; &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. The function's implementation has the burden of being flexible enough to handle whatever the caller asks for.&lt;/p&gt;
&lt;p&gt;But, for a function like:&lt;/p&gt;
&lt;p&gt;~~~haskell foo :: [Int] -&amp;gt; OpaqueNet i o ~~~&lt;/p&gt;
&lt;p&gt;While the caller can choose what &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;o&lt;/code&gt; are, the &lt;em&gt;function&lt;/em&gt; gets to choose what &lt;code&gt;hs&lt;/code&gt; (in the hidden &lt;code&gt;Network i hs o&lt;/code&gt;) is. If I want to &lt;em&gt;use&lt;/em&gt; the thing that &lt;code&gt;foo&lt;/code&gt; returns...then &lt;em&gt;I&lt;/em&gt; have to be flexible. &lt;em&gt;I&lt;/em&gt; have the burden of being flexible enough to handle whatever &lt;code&gt;hs&lt;/code&gt; the &lt;em&gt;function&lt;/em&gt; returns.&lt;/p&gt;
&lt;p&gt;In summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For universally quantified types, the &lt;em&gt;caller&lt;/em&gt; chooses the type being instanced, and the &lt;em&gt;function's implementation&lt;/em&gt; has to accommodate any choice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For existentially quantified types, the &lt;em&gt;function's implementation&lt;/em&gt; chooses the type being instanced, and the &lt;em&gt;caller&lt;/em&gt; has to accommodate any choice.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Indeed, we saw earlier that if we ever wanted to &lt;em&gt;use&lt;/em&gt; the &lt;code&gt;Network i hs o&lt;/code&gt; inside the &lt;code&gt;OpaqueNet i o&lt;/code&gt;, we were forced to deal with it in a parametrically polymorphic way. We had to be able to handle &lt;em&gt;any&lt;/em&gt; &lt;code&gt;hs&lt;/code&gt; that the &lt;code&gt;ONet&lt;/code&gt; could throw at us!&lt;/p&gt;
&lt;h4&gt;A familiar friend&lt;/h4&gt;
&lt;p&gt;I called &lt;code&gt;OpaqueNet i o&lt;/code&gt; a &amp;quot;dependent pair&amp;quot; earlier, pairing &lt;code&gt;hs&lt;/code&gt; with &lt;code&gt;Network i hs o&lt;/code&gt;. But there's another common term for it: a &lt;strong&gt;dependent sum&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;People familiar with Haskell might recognize that &amp;quot;sum types&amp;quot; are &lt;code&gt;Either&lt;/code&gt;-like types that can be one thing or another. Sum types are one of the first things you learn about in Haskell --- heck, even &lt;code&gt;Maybe a&lt;/code&gt; is the sum of &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;()&lt;/code&gt;. Dependent pairs/existential types actually are very similar to &lt;code&gt;Either&lt;/code&gt;/sum types, in spirit, and it might help to see the parallel so that you can see that they're nothing scary, and that the fundamentals/intuition of working with existential types in Haskell is no different than working with &lt;code&gt;Either&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;If I had:&lt;/p&gt;
&lt;p&gt;~~~haskell foo :: String -&amp;gt; Either Int Bool ~~~&lt;/p&gt;
&lt;p&gt;I have to handle the result for both the case where I get an &lt;code&gt;Int&lt;/code&gt; and the case where I get a &lt;code&gt;Bool&lt;/code&gt;. The &lt;em&gt;function&lt;/em&gt; gets to pick what type I have to handle (&lt;code&gt;Int&lt;/code&gt; or &lt;code&gt;Bool&lt;/code&gt;), and &lt;em&gt;I&lt;/em&gt; have to adapt to whatever it returns. Sound familiar? In fact, you can even imagine that &lt;code&gt;OpaqueNet i o&lt;/code&gt; as being just a infinite &lt;em&gt;Either&lt;/em&gt; over &lt;code&gt;'[]&lt;/code&gt;, &lt;code&gt;'[1]&lt;/code&gt;, &lt;code&gt;'[1,2]&lt;/code&gt;, etc.[^eithers]&lt;/p&gt;
&lt;p&gt;Remember that the basic way of handling an &lt;code&gt;Either&lt;/code&gt; and figuring out what the type of the value is inside is through &lt;em&gt;pattern matching&lt;/em&gt; on it. You can't know if an &lt;code&gt;Either Int Bool&lt;/code&gt; contains an &lt;code&gt;Int&lt;/code&gt; or &lt;code&gt;Bool&lt;/code&gt; until you pattern match. But, once you do, all is revealed, and GHC lets you take advantage of knowing the type.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;OpaqueNet i o&lt;/code&gt;, it's the same! You don't know the actual type of the &lt;code&gt;Network i hs o&lt;/code&gt; it contains until you &lt;em&gt;pattern match&lt;/em&gt; on the network (This time, it's a &amp;quot;dependent pattern match&amp;quot;). Once you pattern match on it, all is revealed...and GHC lets you take advantage of knowing the type!&lt;/p&gt;
&lt;h3&gt;Reification&lt;/h3&gt;
&lt;p&gt;For simplicity, let's re-write &lt;code&gt;randomNet&lt;/code&gt; the more sensible way --- with the explicit singleton input style:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L79-87 randomNet' :: forall m i hs o. (MonadRandom m, KnownNat i, KnownNat o) =&amp;gt; Sing hs -&amp;gt; m (Network i hs o) randomNet' = \case SNil -&amp;gt; O &amp;lt;$&amp;gt; randomWeights SNat &lt;code&gt;SCons&lt;/code&gt; ss -&amp;gt; (:&amp;amp;~) &amp;lt;$&amp;gt; randomWeights &amp;lt;*&amp;gt; randomNet' ss&lt;/p&gt;
&lt;p&gt;randomNet :: forall m i hs o. (MonadRandom m, KnownNat i, SingI hs, KnownNat o) =&amp;gt; m (Network i hs o) randomNet = randomNet' sing ~~~&lt;/p&gt;
&lt;p&gt;We use &lt;code&gt;sing :: SingI hs =&amp;gt; Sing hs&lt;/code&gt; to go call the &lt;code&gt;Sing hs -&amp;gt;&lt;/code&gt;-style function from the &lt;code&gt;SingI hs =&amp;gt;&lt;/code&gt; one.[^style]&lt;/p&gt;
&lt;p&gt;Now, we still need to somehow get our list of integers to the type level so that we can create a &lt;code&gt;Network i hs o&lt;/code&gt; to stuff into our &lt;code&gt;ONet&lt;/code&gt;. For that, the &lt;em&gt;singletons&lt;/em&gt; library offers the necessary tooling. It gives us &lt;code&gt;SomeSing&lt;/code&gt;, which is a lot like our &lt;code&gt;OpaqueNet&lt;/code&gt; above, wrapping the &lt;code&gt;Sing a&lt;/code&gt; inside an existential data constructor. &lt;code&gt;toSing&lt;/code&gt; takes the term-level value (for us, an &lt;code&gt;[Integer]&lt;/code&gt;) and returns a &lt;code&gt;SomeSing&lt;/code&gt; wrapping the type-level value (for us, a &lt;code&gt;[Nat]&lt;/code&gt;). When we pattern match on the &lt;code&gt;SomeSing&lt;/code&gt; constructor, we get &lt;code&gt;a&lt;/code&gt; in scope!&lt;/p&gt;
&lt;p&gt;As of &lt;em&gt;singletons-2.2&lt;/em&gt; and GHC 8[^old-singletons], &lt;code&gt;SomeSing&lt;/code&gt; is implemented as:&lt;/p&gt;
&lt;p&gt;~~~haskell data SomeSing :: * -&amp;gt; * where SomeSing :: Sing (a :: k) -&amp;gt; SomeSing k ~~~&lt;/p&gt;
&lt;p&gt;And you have:&lt;/p&gt;
&lt;p&gt;~~~haskell foo :: SomeSing Bool foo = SomeSing STrue&lt;/p&gt;
&lt;p&gt;bar :: SomeSing Nat bar = SomeSing (SNat :: Sing 10) ~~~&lt;/p&gt;
&lt;p&gt;Pattern matching looks like:&lt;/p&gt;
&lt;p&gt;~~~haskell main :: IO () main = do putStrLn &amp;quot;How many cats do you own?&amp;quot; c &amp;lt;- readLn :: IO Integer case toSing c of SomeSing (SNat :: Sing n) -&amp;gt; -- ... ~~~&lt;/p&gt;
&lt;p&gt;Now, inside the case statement branch (the &lt;code&gt;...&lt;/code&gt;), we have &lt;em&gt;type&lt;/em&gt; &lt;code&gt;n :: Nat&lt;/code&gt; in scope! And by pattern matching on the &lt;code&gt;SNat&lt;/code&gt; constructor, we also have a &lt;code&gt;KnownNat n&lt;/code&gt; instance (As discussed in &lt;a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html#on-typeclasses-and-dictionaries"&gt;previous part&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;toSing&lt;/code&gt; works using a simple typeclass mechanism with an associated type whose job is to connect the types of values with the kinds of their singletons. It associates &lt;code&gt;Bool&lt;/code&gt; (the type) with &lt;code&gt;Bool&lt;/code&gt; (the kind), &lt;code&gt;Integer&lt;/code&gt; (the type) with &lt;code&gt;Nat&lt;/code&gt; (the kind), &lt;code&gt;[Integer]&lt;/code&gt; (the type) with &lt;code&gt;[Nat]&lt;/code&gt; (the kind), etc., and it does it with simple applications of type families (here's a &lt;a href="https://ocharles.org.uk/blog/posts/2014-12-12-type-families.html"&gt;nice tutorial on type families&lt;/a&gt; courtesy of Oliver Charles, as a refresher). With it, we can convert any normal value &lt;code&gt;x&lt;/code&gt; of type &lt;code&gt;a&lt;/code&gt; to a singleton representing type &lt;code&gt;x&lt;/code&gt; with kind &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We now have enough to write our &lt;code&gt;randomONet&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L127-131 randomONet :: (MonadRandom m, KnownNat i, KnownNat o) =&amp;gt; [Integer] -&amp;gt; m (OpaqueNet i o) randomONet hs = case toSing hs of SomeSing ss -&amp;gt; ONet &amp;lt;$&amp;gt; randomNet' ss ~~~&lt;/p&gt;
&lt;p&gt;This process of bringing a term-level value into the type level is known in Haskell as &lt;strong&gt;reification&lt;/strong&gt;. With this, our original goal is (finally) within reach:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L205-213 main :: IO () main = do putStrLn &amp;quot;What hidden layer structure do you want?&amp;quot; hs &amp;lt;- readLn n &amp;lt;- randomONet hs case n of ONet (net :: Network 10 hs 3) -&amp;gt; do print net -- blah blah stuff with our dynamically generated net ~~~&lt;/p&gt;
&lt;h4&gt;The Boundary&lt;/h4&gt;
&lt;p&gt;With the power of existentially quantified types (like in &lt;code&gt;SomeSing&lt;/code&gt;), we essentially gained the ability to work with types that depend on runtime results.&lt;/p&gt;
&lt;p&gt;In a way, you can consider the &lt;code&gt;toSing&lt;/code&gt; and the &lt;code&gt;SomeSing&lt;/code&gt; as our &amp;quot;boundary&amp;quot; between the &amp;quot;untyped world&amp;quot; and the &amp;quot;typed world&amp;quot;. This layer (and the process of reification) cleanly separates the two.&lt;/p&gt;
&lt;p&gt;This boundary can be thought of as a lot like the boundary we talk about between &amp;quot;pure&amp;quot; functions and values and &amp;quot;impure&amp;quot; (IO, etc.) ones. People say to always write as much of your program as possible in the &amp;quot;pure&amp;quot; world --- to separate and pull out as much logic as you can to be pure logic. That's one of the first things you learn about as a Haskell programmer: how to separate logic that &lt;em&gt;can&lt;/em&gt; be pure from logic that is &amp;quot;impure&amp;quot; (IO, etc.), and then finally combine them at the very end, as late as possible.&lt;/p&gt;
&lt;p&gt;It's easy to think that just because the final program is going to &amp;quot;be in IO in the end anyway&amp;quot;, there isn't any point in separating out pure and impure parts of your program logic. But we know that we gain separation of concerns, the increased ability to reason with your code and analyze what it does, the compiler's ability to check what you write, the limitation of implementations, etc.&lt;/p&gt;
&lt;p&gt;You can think of the general philosophy of working with typed/untyped worlds as being the same thing. You try to write as much of your program as possible in the &amp;quot;typed&amp;quot; world, like we did in Part 1. Take advantage of the increased ability to reason with your code, parametric polymorphism helping you &lt;em&gt;write&lt;/em&gt; your code, limit your implementations, nab you compiler help, etc. All of those are benefits of working in the typed world.&lt;/p&gt;
&lt;p&gt;Then, write what you absolutely must in your &amp;quot;untyped&amp;quot; world, such as dealing with values that pop up at runtime like the &lt;code&gt;[Integer]&lt;/code&gt; above.&lt;/p&gt;
&lt;p&gt;Finally, at the very end, &lt;em&gt;unite&lt;/em&gt; them at the boundary. Pass the control football from the untyped world to the typed world!&lt;/p&gt;
&lt;p&gt;The great part about this all is that GHC and the type system is there at every step holding your hand, guiding you as you implement your programs and making sure everything is type-safe and fits together! (This, after all, is why dependently typed programming with dynamically generated types is &lt;em&gt;not&lt;/em&gt; the same thing as &amp;quot;&lt;em&gt;dynamically&lt;/em&gt; typed programming&amp;quot;!)&lt;/p&gt;
&lt;h3&gt;Continuation-Based Existentials&lt;/h3&gt;
&lt;p&gt;There's another way in Haskell that we work with existential types that can be more natural to use in a lot of cases. Remember that when we pattern match on an existential data type, we have to work with the values in the constructor in a parametrically polymorphic way. For example, if we had:&lt;/p&gt;
&lt;p&gt;~~~haskell oNetToFoo :: OpaqueNet i o -&amp;gt; Foo oNetToFoo (ONet n) = f n ~~~&lt;/p&gt;
&lt;p&gt;&lt;code&gt;f&lt;/code&gt; has to take a &lt;code&gt;Network i hs o&lt;/code&gt; but deal with it in a way that works &lt;em&gt;for all&lt;/em&gt; &lt;code&gt;hs&lt;/code&gt;. It can't be written for &lt;em&gt;only&lt;/em&gt; &lt;code&gt;'[5]&lt;/code&gt; or &lt;em&gt;only&lt;/em&gt; &lt;code&gt;'[6,3]&lt;/code&gt;...it has to work for &lt;em&gt;any&lt;/em&gt; &lt;code&gt;hs&lt;/code&gt;. That's the whole &amp;quot;existential vs. universal quantification&amp;quot; thing we just talked about.&lt;/p&gt;
&lt;p&gt;Well, we could really also just skip the constructor altogether and represent an existential type as something &lt;em&gt;taking&lt;/em&gt; the continuation &lt;code&gt;f&lt;/code&gt; and giving it what it needs.&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L154-154 type OpaqueNet' i o r = (forall hs. Network i hs o -&amp;gt; r) -&amp;gt; r ~~~&lt;/p&gt;
&lt;p&gt;&amp;quot;Tell me how you would make an &lt;code&gt;r&lt;/code&gt; if you had a &lt;code&gt;Network i hs o&lt;/code&gt; (that works for any &lt;code&gt;hs&lt;/code&gt;) and I'll make it for you!&amp;quot;&lt;/p&gt;
&lt;p&gt;(This takes advantage of Rank-N types. If you're unfamiliar with them, Gregor Riegler has a &lt;a href="http://sleepomeno.github.io/blog/2014/02/12/Explaining-Haskell-RankNTypes-for-all/"&gt;nice tutorial&lt;/a&gt; on the subject.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Using&lt;/em&gt; these types is very similar to using the constructor-style ones:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L159-176 runOpaqueNet' :: (KnownNat i, KnownNat o) =&amp;gt; OpaqueNet' i o (R o) -&amp;gt; R i -&amp;gt; R o runOpaqueNet' oN x = oN (\n -&amp;gt; runNet n x) -- :: ((forall hs. Network i hs o -&amp;gt; R o) -&amp;gt; R o) -- -&amp;gt; R i -- -&amp;gt; R o&lt;/p&gt;
&lt;p&gt;numHiddens' :: OpaqueNet' i o Int -&amp;gt; Int numHiddens' oN = oN go where go :: Network i hs o -&amp;gt; Int go = \case O _ -&amp;gt; 0 _ :&amp;amp;~ n' -&amp;gt; 1 + go n' -- :: ((forall hs. Network i hs o -&amp;gt; Int) -&amp;gt; Int) -- -&amp;gt; Int ~~~&lt;/p&gt;
&lt;p&gt;This &amp;quot;continuation transformation&amp;quot; is formally known as &lt;strong&gt;skolemization&lt;/strong&gt;.[^skolemization]&lt;/p&gt;
&lt;p&gt;We can &amp;quot;wrap&amp;quot; a &lt;code&gt;Network i hs o&lt;/code&gt; into an &lt;code&gt;OpaqueNet' i o r&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L156-157 oNet' :: Network i hs o -&amp;gt; OpaqueNet' i o r oNet' n = \f -&amp;gt; f n ~~~&lt;/p&gt;
&lt;p&gt;Let's write a version of &lt;code&gt;randomONet&lt;/code&gt; that returns a continuation-style existential:&lt;/p&gt;
&lt;p&gt;~~~haskell withRandomONet' :: (MonadRandom m, KnownNat i, KnownNat o) =&amp;gt; [Integer] -&amp;gt; (forall hs. Network i hs o -&amp;gt; m r) -&amp;gt; m r -- aka, =&amp;gt; [Integer] -- -&amp;gt; OpaqueNet' i o (m r) withRandomONet' hs f = case toSing hs of SomeSing ss -&amp;gt; do net &amp;lt;- randomNet' ss f net&lt;/p&gt;
&lt;p&gt;~~~&lt;/p&gt;
&lt;p&gt;But, hey, because we're skolemizing everything, let's do it with the skolemized version of &lt;code&gt;toSing&lt;/code&gt;/&lt;code&gt;SomeSing&lt;/code&gt;, &lt;code&gt;withSomeSing&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell -- a version of &lt;code&gt;toSing&lt;/code&gt; that returns a skolemized &lt;code&gt;SomeSing&lt;/code&gt; withSomeSing :: [Integer] -&amp;gt; (forall (hs :: [Nat]). Sing hs -&amp;gt; r) -&amp;gt; r ~~~&lt;/p&gt;
&lt;p&gt;Because why not? Skolemize all the things!&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L178-186 withRandomONet' :: (MonadRandom m, KnownNat i, KnownNat o) =&amp;gt; [Integer] -&amp;gt; (forall hs. Network i hs o -&amp;gt; m r) -&amp;gt; m r -- aka, =&amp;gt; [Integer] -- -&amp;gt; OpaqueNet' i o (m r) withRandomONet' hs f = withSomeSing hs $ \ss -&amp;gt; do net &amp;lt;- randomNet' ss f net ~~~&lt;/p&gt;
&lt;p&gt;We can use it to do the same things we used the constructor-based existential for, as well...and, in a way, it actually seems (oddly) more natural.&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L215-221 main' :: IO () main' = do putStrLn &amp;quot;What hidden layer structure do you want?&amp;quot; hs &amp;lt;- readLn withRandomONet' hs $ (net :: Network 10 hs 3) -&amp;gt; do print net -- blah blah stuff with our dynamically generated net ~~~&lt;/p&gt;
&lt;p&gt;Like the case statement pattern match represented the lexical &amp;quot;wall&amp;quot;/&amp;quot;boundary&amp;quot; between the untyped and typed world when using constructor-style existentials, the &lt;code&gt;... $ \net -&amp;gt; ...&lt;/code&gt; can be thought of the &amp;quot;wall&amp;quot; for the continuation-style existentials.&lt;/p&gt;
&lt;h2&gt;A Tale of Two Styles&lt;/h2&gt;
&lt;p&gt;We've just discussed two ways of doing the same thing. Two styles of representing/working with existential types. The two are equivalent, in that you can always &amp;quot;convert&amp;quot; between one or the other, but the choice of which one you use/reach for/offer can make a difference in code clarity. After working with both styles a lot (sometimes, libraries only offer one style), you start to get a feel for which one you like more in which situations. In the end, I don't think there are any hard or fast rules. Just use whichever one you feel is more readable!&lt;/p&gt;
&lt;p&gt;That being said, here are some general Pros and Cons that I've noticed over the years:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Most obviously, continuation-style doesn't require you to define a throwaway data type/constructor. While new types are cheap in Haskell, they force your users to learn a new set of types and constructors for every single existential type you return. If you or the library you're writing uses/returns a &lt;em&gt;lot&lt;/em&gt; of different existentially qualified types, all those extra dumb wrappers are a huge hassle.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Continuation-style existentials are in general smoother to use than constructor-style ones when functions &lt;em&gt;return&lt;/em&gt; existentials. Especially if you intend to immediately use them, continuation-style basically saves you an extraneous pattern match.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you have to use several existentials at once, continuation-style is much better because each nested existential doesn't force another level of indentation:&lt;/p&gt;
&lt;p&gt;~~~haskell foo = withSomeSing x $ \sx -&amp;gt; withSomeSing y $ \sy -&amp;gt; withSomeSing z $ \sz -&amp;gt; -- ... ~~~&lt;/p&gt;
&lt;p&gt;vs.&lt;/p&gt;
&lt;p&gt;~~~haskell foo = case toSing x of SomeSing sx -&amp;gt; case toSing y of SomeSing sy -&amp;gt; case toSing z of SomeSing sz -&amp;gt; -- ... ~~~&lt;/p&gt;
&lt;p&gt;Every time you nest a case statement, you actually waste &lt;em&gt;two&lt;/em&gt; levels of indentation, which can be annoying even at 2-space indentation. But you don't need &lt;em&gt;any&lt;/em&gt; to nest in the continuation style!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you're working monadically, though, you can take advantage of do notation and &lt;em&gt;ScopedTypeVariables&lt;/em&gt; for a nicer style that doesn't require any nesting at all:&lt;/p&gt;
&lt;p&gt;~~~haskell main = do ONet n1 &amp;lt;- randomONet [7,5,3] :: IO (OpaqueNet 10 1) ONet n2 &amp;lt;- randomONet [5,5,5] :: IO (OpaqueNet 10 1) ONet n3 &amp;lt;- randomONet [5,4,3] :: IO (OpaqueNet 10 1) hs &amp;lt;- readLn ONet (n4 :: Network 10 hs 1) &amp;lt;- randomONet hs -- ... ~~~&lt;/p&gt;
&lt;p&gt;Which is arguably nicer than&lt;/p&gt;
&lt;p&gt;~~~haskell main = withRandomONet' [7,5,3] $ \n1 -&amp;gt; withRandomONet' [5,5,5] $ \n2 -&amp;gt; withRandomONet' [5,4,3] $ \n3 -&amp;gt; do hs &amp;lt;- readLn withRandomONet' hs $ (n4 :: Network 10 hs 1) -&amp;gt; do -- ... ~~~&lt;/p&gt;
&lt;p&gt;A lot of libraries return existentials in &lt;code&gt;Maybe&lt;/code&gt;'s (&lt;a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/GHC-TypeLits.html#v:someNatVal"&gt;base is guilty&lt;/a&gt;), so this trick can be useful for those, too!&lt;/p&gt;
&lt;p&gt;This trick is less useful for functions like &lt;code&gt;toSing&lt;/code&gt; where things are &lt;em&gt;not&lt;/em&gt; returned in a monad. You could wrap it in Identity, but that's kind of silly:&lt;/p&gt;
&lt;p&gt;~~~haskell foo = runIdentity $ do SomeSing sx &amp;lt;- Identity $ toSing x SomeSing sy &amp;lt;- Identity $ toSing y SomeSing sz &amp;lt;- Identity $ toSing z return $ -- ... ~~~&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Constructor-style is necessary for writing typeclass instances. You can't write a &lt;code&gt;Show&lt;/code&gt; instance for &lt;code&gt;(forall hs. Network i hs o -&amp;gt; r) -&amp;gt; r&lt;/code&gt;, but you can write one for &lt;code&gt;OpaqueNet i o&lt;/code&gt;. We'll also be writing &lt;code&gt;Binary&lt;/code&gt; instances later for serialization/deserialization, and it all only works in constructor-style.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Haskell doesn't allow you to use Rank-N types as arguments to type constructors, so you can have &lt;code&gt;[OpaqueNet i o]&lt;/code&gt;, but &lt;em&gt;not&lt;/em&gt; &lt;code&gt;[OpaqueNet' i o r]&lt;/code&gt; or &lt;code&gt;[(forall hs. Network i hs o -&amp;gt; r) -&amp;gt; r]&lt;/code&gt;. You can have &lt;code&gt;MVar (OpaqueNet i o)&lt;/code&gt;, but not &lt;code&gt;MVar ((forall hs. Network i hs o -&amp;gt; r) -&amp;gt; r)&lt;/code&gt;. The latter are known as &lt;em&gt;impredicative&lt;/em&gt; types, which are a big no-no in GHC Haskell. Don't even go there! The constructor style is necessary in these situations.&lt;/p&gt;
&lt;p&gt;If the type constructor is a Monad, you can get away with a ContT-style skolemization, like &lt;code&gt;(forall hs. Network i hs o -&amp;gt; [r]) -&amp;gt; [r]&lt;/code&gt; and &lt;code&gt;(forall hs. Network i hs o -&amp;gt; IO r) -&amp;gt; IO r&lt;/code&gt;. But this doesn't work for &lt;code&gt;MVar&lt;/code&gt; and other useful type constructors you might want to put &lt;code&gt;OpaqueNet&lt;/code&gt; in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When writing functions that &lt;em&gt;take&lt;/em&gt; existentials as inputs, the constructor-style is arguably more natural. But barely.&lt;/p&gt;
&lt;p&gt;For example, we wrote a function to find the number of hidden layers in a network earlier:&lt;/p&gt;
&lt;p&gt;~~~haskell numHiddens :: OpaqueNet i o -&amp;gt; Int ~~~&lt;/p&gt;
&lt;p&gt;But the continuation-style version has a slightly messier type:&lt;/p&gt;
&lt;p&gt;~~~haskell numHiddens' :: ((forall hs. Network i hs o -&amp;gt; Int) -&amp;gt; Int) -&amp;gt; Int ~~~&lt;/p&gt;
&lt;p&gt;Even with with the type synonym, it's still a little awkward:&lt;/p&gt;
&lt;p&gt;~~~haskell numHiddens' :: OpaqueNet' i o Int -&amp;gt; Int ~~~&lt;/p&gt;
&lt;p&gt;This is why you'll encounter many more functions &lt;em&gt;returning&lt;/em&gt; continuation-style existentials in libraries than &lt;em&gt;taking&lt;/em&gt; them, for the most part.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are just general principles, not hard fast rules. This list is nowhere near exhaustive and reflects my current progress in my journey towards a dependently typed lifestyle. If you come back in a month, you might see more things listed here!&lt;/p&gt;
&lt;p&gt;All said, I do find myself very happy when I see that a library I'm using offers &lt;em&gt;both&lt;/em&gt; styles for me to use. And I've been known to submit PR's to a library to have it offer one style or another, if it's lacking.&lt;/p&gt;
&lt;p&gt;Be judicious. If you're writing a library, don't spam it with too many throwaway constructors. After a while, you'll begin to intuitively see which style shines in which situations! (And, in some case, there might not even be a definitive &amp;quot;better&amp;quot; style to use.)&lt;/p&gt;
&lt;h2&gt;Serializing Networks&lt;/h2&gt;
&lt;p&gt;To drive things home, let's apply what we learned about existential types and reification to another simple application: serialization.&lt;/p&gt;
&lt;h3&gt;Recap on the Binary Library&lt;/h3&gt;
&lt;p&gt;Serializing networks of &lt;em&gt;known&lt;/em&gt; size --- whose sizes are statically in their types --- is pretty straightforward, and its ease is one of the often-quoted advantages of having sizes in your types.[^storable] I'm going to be using the &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/binary"&gt;binary&lt;/a&gt;&lt;/em&gt; library, which offers a very standard typeclass-based approach for serializing and deserializing data. There are a lot of tutorials online (and I even &lt;a href="https://blog.jle.im/entry/streaming-huffman-compression-in-haskell-part-2-binary.html"&gt;wrote a small one&lt;/a&gt; myself a few years ago), but a very high-level view is that the library offers monads (&lt;code&gt;Get&lt;/code&gt;, &lt;code&gt;Put&lt;/code&gt;) for describing serialization schemes and also a typeclass used to provide serialization instructions for different types.&lt;/p&gt;
&lt;p&gt;In practice, we usually don't write our own instances from scratch. Instead, we use GHC's generics features to give us instances for free:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L25-30 data Weights i o = W { wBiases :: !(R o) , wNodes :: !(L o i) } deriving (Show, Generic)&lt;/p&gt;
&lt;p&gt;instance (KnownNat i, KnownNat o) =&amp;gt; Binary (Weights i o) ~~~&lt;/p&gt;
&lt;p&gt;For simple types like &lt;code&gt;Weights&lt;/code&gt;, which simply contain serializable things, the &lt;em&gt;binary&lt;/em&gt; library is smart enough to write your instances automatically for you! This gives us &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;put&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell get :: (KnownNat i, KnownNat o) =&amp;gt; Get (Weights i o)&lt;/p&gt;
&lt;p&gt;put :: (KnownNat i, KnownNat o) =&amp;gt; Weights i o -&amp;gt; Put ~~~&lt;/p&gt;
&lt;p&gt;However, for GADTs like &lt;code&gt;Network&lt;/code&gt;, we have to things manually.&lt;/p&gt;
&lt;h4&gt;Serializing &lt;code&gt;Network&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;Taking advantage of having the entire structure in the type, &lt;code&gt;put&lt;/code&gt; is simple:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L89-94 putNet :: (KnownNat i, KnownNat o) =&amp;gt; Network i hs o -&amp;gt; Put putNet = \case O w -&amp;gt; put w w :&amp;amp;~ n -&amp;gt; put w *&amp;gt; putNet n ~~~&lt;/p&gt;
&lt;p&gt;If it's an &lt;code&gt;O w&lt;/code&gt;, just serialize the &lt;code&gt;w&lt;/code&gt;. If it's a &lt;code&gt;w :&amp;amp;~ net&lt;/code&gt;, serialize the &lt;code&gt;w&lt;/code&gt; then the rest of the &lt;code&gt;net&lt;/code&gt;. Normally, we might have to put a &amp;quot;flag&amp;quot; to tell what constructor we serializing, so that the deserializer can know what constructor to deserialize at every step. But for &lt;code&gt;Network&lt;/code&gt;, we don't have to:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L96-101 getNet :: forall i hs o. (KnownNat i, KnownNat o) =&amp;gt; Sing hs -&amp;gt; Get (Network i hs o) getNet = \case SNil -&amp;gt; O &amp;lt;$&amp;gt; get SNat &lt;code&gt;SCons&lt;/code&gt; ss -&amp;gt; (:&amp;amp;~) &amp;lt;$&amp;gt; get &amp;lt;*&amp;gt; getNet ss ~~~&lt;/p&gt;
&lt;p&gt;&lt;code&gt;getNet&lt;/code&gt; doesn't need flags because we already &lt;em&gt;know&lt;/em&gt; how many &lt;code&gt;:&amp;amp;~&lt;/code&gt; layers to expect &lt;em&gt;just from the type&lt;/em&gt;. If we want to deserialize a &lt;code&gt;Network 5 '[10,6,3] 2&lt;/code&gt;, we &lt;em&gt;know&lt;/em&gt; we want three &lt;code&gt;(:&amp;amp;~)&lt;/code&gt;'s and one &lt;code&gt;O&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;getNet&lt;/code&gt; is written similarly to how we wrote &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L79-83"&gt;&lt;code&gt;randomNet'&lt;/code&gt;&lt;/a&gt;. We &amp;quot;pattern match&amp;quot; on &lt;code&gt;hs&lt;/code&gt; (using singletons) to get the constructors we are expecting to deserialize and just follow what the singleton's structure tells us.&lt;/p&gt;
&lt;p&gt;To write a &lt;code&gt;Binary&lt;/code&gt; instance for &lt;code&gt;Network&lt;/code&gt;, we can't have &lt;code&gt;get&lt;/code&gt; take a &lt;code&gt;Sing hs&lt;/code&gt; input --- that'd change the arity/type of the function. We have to switch to &lt;code&gt;SingI&lt;/code&gt;-style had have their &lt;code&gt;Binary&lt;/code&gt; instances require a &lt;code&gt;SingI hs&lt;/code&gt; constraint.&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L103-105 instance (KnownNat i, SingI hs, KnownNat o) =&amp;gt; Binary (Network i hs o) where put = putNet get = getNet sing ~~~&lt;/p&gt;
&lt;h3&gt;Serializating &lt;code&gt;OpaqueNet&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;It's arguably much more useful to serialize/deserialize &lt;code&gt;OpaqueNet&lt;/code&gt;s. Between different iterations of your program, you might have the same inputs/outputs, but want to try out different internal structures. You'd want to store them and access them uniformly, or send them over a network without requiring the receiver to know the internal structure beforehand. Remember, you can't even &lt;em&gt;load&lt;/em&gt; a &lt;code&gt;Network i hs o&lt;/code&gt; without knowing its complete structure!&lt;/p&gt;
&lt;p&gt;Because the complete structure of the network is not in the type, we need to encode it as a flag in the binary serialization so that the deserializer will know what constructors to expect and deserialize. We can write a simple function to get the &lt;code&gt;[Integer]&lt;/code&gt; of a network's structure:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L72-77 hiddenStruct :: Network i hs o -&amp;gt; [Integer] hiddenStruct = \case O _ -&amp;gt; [] _ :&amp;amp;~ (n' :: Network h hs' o) -&amp;gt; natVal (Proxy @h) : hiddenStruct n' ~~~&lt;/p&gt;
&lt;p&gt;Recall that &lt;code&gt;natVal :: KnownNat n =&amp;gt; Proxy n -&amp;gt; Integer&lt;/code&gt; returns the value-level &lt;code&gt;Integer&lt;/code&gt; corresponding to the type-level &lt;code&gt;n :: Nat&lt;/code&gt;. (I'm also using GHC 8's fancy &lt;em&gt;TypeApplications&lt;/em&gt; syntax, and &lt;code&gt;Proxy @h&lt;/code&gt; is the same as &lt;code&gt;Proxy :: Proxy h&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;natVal&lt;/code&gt; and &lt;code&gt;hiddenStruct&lt;/code&gt; are kind of interesting --- they take type-level information (&lt;code&gt;n&lt;/code&gt;, &lt;code&gt;hs&lt;/code&gt;) and turns them into term-level values (&lt;code&gt;Integer&lt;/code&gt;s, &lt;code&gt;[Integer]&lt;/code&gt;s). They are the opposites of our reification functions (like &lt;code&gt;toSing&lt;/code&gt;). Going from the &amp;quot;type level&amp;quot; to the &amp;quot;value level&amp;quot; is known in Haskell as &lt;strong&gt;reflection&lt;/strong&gt;, and is the dual concept of reification. (The &lt;em&gt;singletons&lt;/em&gt; library offers reflectors for all of its singletons, as &lt;code&gt;fromSing&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;That's all we need!&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L133-138 putONet :: (KnownNat i, KnownNat o) =&amp;gt; OpaqueNet i o -&amp;gt; Put putONet (ONet net) = do put (hiddenStruct net) putNet net ~~~&lt;/p&gt;
&lt;p&gt;&amp;quot;Put the structure (as a binary flag), and then put the network itself.&amp;quot;&lt;/p&gt;
&lt;p&gt;Now, to deserialize, we want to &lt;em&gt;load&lt;/em&gt; the list of &lt;code&gt;Integer&lt;/code&gt;s and reify it back to the type level to know what type of network we're expecting to load:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L140-145 getONet :: (KnownNat i, KnownNat o) =&amp;gt; Get (OpaqueNet i o) getONet = do hs &amp;lt;- get withSomeSing hs $ \ss -&amp;gt; ONet &amp;lt;$&amp;gt; getNet ss ~~~&lt;/p&gt;
&lt;p&gt;We load our flag, reify it, and once we're back in the typed land again, we can do our normal business. Isn't it nice that GHC is also there at every step to make sure we make the transition safely?&lt;/p&gt;
&lt;p&gt;Our final instance:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L147-149 instance (KnownNat i, KnownNat o) =&amp;gt; Binary (OpaqueNet i o) where put = putONet get = getONet ~~~&lt;/p&gt;
&lt;p&gt;And, of course, we used the constructor-style existential this whole time instead of the continuation-style one because we can't directly write typeclass instances for the latter.&lt;/p&gt;
&lt;h2&gt;An Existence For All&lt;/h2&gt;
&lt;p&gt;We've learned about how to &amp;quot;cross&amp;quot; from the untyped world to the typed world and bring about contexts involving types that can depend on runtime factors. To me, this is really the point where dependently typed programming starts --- when you start having to work with types that depend on run-time factors.&lt;/p&gt;
&lt;p&gt;We've already been able to reap a lot of benefits. All of the type safety we discovered in the last part is now available to us in a fully dynamic world, as well. We also learned the advantages of &lt;em&gt;separating&lt;/em&gt; the typed world from the untyped world and how the compiler helps us make the transition safely.&lt;/p&gt;
&lt;p&gt;But really, this is all just the &lt;em&gt;start&lt;/em&gt; of dependently typed programming. This is where things &lt;em&gt;really&lt;/em&gt; start to get fun.&lt;/p&gt;
&lt;p&gt;Stepping into this new world can be disorienting at first. There's a lot of unexpected things that come up when we start working more with these fancy new types. We have to deal with types coming from different sources, convince the type system about their properties and relationships between them, and deal with a whole bunch of other concerns that just don't happen when you program only at the value level. But don't worry! Like all things, it will more naturally with practice.&lt;/p&gt;
&lt;p&gt;Now that we have existential types and run-time types out of the way, come back for the next post in the series, where we start to have the &lt;em&gt;real&lt;/em&gt; fun! :D&lt;/p&gt;
&lt;h4&gt;Exercises&lt;/h4&gt;
&lt;p&gt;Here are some fun exercises you can try, if you want to test your understanding! Links are to the solutions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Implement &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L188-193"&gt;&lt;code&gt;putONet'&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L195-203"&gt;&lt;code&gt;getONet'&lt;/code&gt;&lt;/a&gt; using the continuation-style existentials, instead.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Work with an existential wrapper over the &lt;em&gt;entire&lt;/em&gt; network structure (inputs and outputs, too):&lt;/p&gt;
&lt;p&gt;~~~haskell data SomeNet where SNet :: (KnownNat i, KnownNat o) =&amp;gt; Network i hs o -&amp;gt; SomeNet ~~~&lt;/p&gt;
&lt;p&gt;(We need the &lt;code&gt;KnownNat&lt;/code&gt; constraints because of type erasure, to recover the original input/output dimensions back once we pattern match)&lt;/p&gt;
&lt;p&gt;And write:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A function to &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L231-234"&gt;convert &lt;code&gt;SomeNet&lt;/code&gt;s to &lt;code&gt;OpaqueNet&lt;/code&gt;s&lt;/a&gt;. Return the &lt;code&gt;OpaqueNet&lt;/code&gt; with existentially quantified &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;o&lt;/code&gt; in continuation-style. (You can write a data type to return it in constructor-style, too, for funsies.)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L236-245"&gt;&lt;code&gt;randomSNet&lt;/code&gt;&lt;/a&gt;, returning &lt;code&gt;m SomeNet&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;While you're at it, write it to return &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L247-258"&gt;a random continuation-style &lt;code&gt;SomeNet&lt;/code&gt;, too&lt;/a&gt;! (See the type of &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L178-186"&gt;&lt;code&gt;withRandomONet'&lt;/code&gt;&lt;/a&gt; for reference on how to write the type)&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped2.hs#L260-274"&gt;binary instance&lt;/a&gt; for &lt;code&gt;SomeNet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Hint: Remember &lt;code&gt;natVal :: KnownNat n =&amp;gt; Proxy n -&amp;gt; Integer&lt;/code&gt;!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hint: Remember that &lt;code&gt;toSomeSing&lt;/code&gt; also works for &lt;code&gt;Integer&lt;/code&gt;s, to get &lt;code&gt;Sing&lt;/code&gt;s for &lt;code&gt;Nat&lt;/code&gt;s, too!&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description><author>Justin Le</author><category>Haskell</category><category>Ramblings</category><guid isPermaLink="true">https://blog.jle.im/entry/practical-dependent-types-in-haskell-2.html</guid><pubDate>Thu, 30 Jun 2016 18:59:23 UTC</pubDate><creator>Justin Le</creator><subject>Haskell, Ramblings</subject><date>2016-06-30</date></item><item><title>Practical Dependent Types in Haskell: Type-Safe Neural Networks (Part 1)</title><link>https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html</link><description>&lt;p&gt;It seems these days like programming with dependent types in Haskell (and its advantages) is moving slowly but steadily to the mainstream of Haskell programming. In the current state of Haskell education, dependent types are often considered topics for &amp;quot;advanced&amp;quot; Haskell users. However, I can foresee a day where the ease of use of modern Haskell libraries relying on dependent types forces programming with dependent types to be an integral part of normal intermediate (or even beginner) Haskell education.&lt;/p&gt;
&lt;p&gt;There are &lt;a href="https://www.youtube.com/watch?v=rhWMhTjQzsU"&gt;more&lt;/a&gt; and &lt;a href="http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/"&gt;more&lt;/a&gt; and &lt;a href="https://www.schoolofhaskell.com/user/konn/prove-your-haskell-for-great-safety"&gt;more&lt;/a&gt; and &lt;a href="http://jozefg.bitbucket.org/posts/2014-08-25-dep-types-part-1.html"&gt;more&lt;/a&gt; great resources and tutorials and introductions to integrating dependent types into your Haskell every day. The point of this series is to show more some practical examples of using dependent types in guiding your programming, and to also walk through the &amp;quot;why&amp;quot; and high-level philosophy of the way you structure your Haskell programs. It'll also hopefully instill an intuition of a dependently typed work flow of &amp;quot;exploring&amp;quot; how dependent types can help your current programs. The intended audience of this post is for intermediate Haskell programmers in general, with no required knowledge of dependently typed programming. I should also point out that I'm no expert --- I'm still in the process of learning this all, myself :)&lt;/p&gt;
&lt;p&gt;The first project in this series will build up to a type-safe &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Artificial_neural_network"&gt;artificial neural network&lt;/a&gt;&lt;/strong&gt; implementation with back-propagation training.&lt;/p&gt;
&lt;h4&gt;Setup&lt;/h4&gt;
&lt;p&gt;This post is written on &lt;em&gt;&lt;a href="http://www.haskellstack.org"&gt;stack&lt;/a&gt;&lt;/em&gt; snapshot &lt;em&gt;&lt;a href="https://www.stackage.org/nightly-2016-06-28"&gt;nightly-2016-06-28&lt;/a&gt;&lt;/em&gt;, with &lt;em&gt;singletons-2.2&lt;/em&gt;, but uses an unreleased version of &lt;em&gt;hmatrix&lt;/em&gt;, &lt;em&gt;&lt;a href="https://github.com/albertoruiz/hmatrix/tree/42a88fbcb6bd1d2c4dc18fae5e962bd34fb316a1"&gt;hmatrix-0.18 (commit 42a88fb)&lt;/a&gt;&lt;/em&gt;. I &lt;a href="http://mstksg.github.io/hmatrix/"&gt;maintain my own documentation&lt;/a&gt; for reference.&lt;/p&gt;
&lt;p&gt;If you're forced to use GHC 7.10 for some reason, there's also a bug in &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/singletons-2.0.1"&gt;singletons-2.0.1&lt;/a&gt;&lt;/em&gt; package that's fixed in &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/singletons-2.1"&gt;singletons-2.1&lt;/a&gt;&lt;/em&gt;, but &lt;em&gt;2.1&lt;/em&gt; is not available with GHC 7.10 -- I have a &lt;a href="https://github.com/mstksg/singletons/releases/tag/v2.0.2"&gt;github fork&lt;/a&gt; that fixes the bug if you want to stay on GHC 7.10.&lt;/p&gt;
&lt;p&gt;You can add this:&lt;/p&gt;
&lt;p&gt;~~~yaml packages: - location: git: git@github.com:albertoruiz/hmatrix.git commit: 42a88fbcb6bd1d2c4dc18fae5e962bd34fb316a1 subdirs: - packages/base&lt;/p&gt;
&lt;h1&gt;# If stuck on GHC 7.10:&lt;/h1&gt;
&lt;h1&gt;- location:&lt;/h1&gt;
&lt;h1&gt;git: git@github.com:mstksg/singletons.git&lt;/h1&gt;
&lt;h1&gt;commit: v2.0.2&lt;/h1&gt;
&lt;p&gt;~~~&lt;/p&gt;
&lt;p&gt;to the &lt;code&gt;packages&lt;/code&gt; field of your directory or global &lt;em&gt;stack.yaml&lt;/em&gt; and &lt;em&gt;stack&lt;/em&gt; will know what version of &lt;em&gt;hmatrix&lt;/em&gt; and &lt;em&gt;singletons&lt;/em&gt; to use when you use &lt;code&gt;stack runghc&lt;/code&gt; or &lt;code&gt;stack ghc&lt;/code&gt;, etc. to build your files.&lt;/p&gt;
&lt;h2&gt;Neural Networks&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Artificial_neural_network"&gt;Artificial neural networks&lt;/a&gt; have been somewhat of a hot topic in computing recently. Implementations of training algorithms (like back-propagation) are tricky to implement correctly --- despite being simple, there are many locations where accidental bugs might pop up when multiplying the wrong matrices, for example.&lt;/p&gt;
&lt;p&gt;Though some might recognize that complicated matrix and vector arithmetic is a common application of phantom type-based dependent types, it's not necessarily always easy to gauge before-the-fact what would or would not be a good candidate for adding dependent types to. Often times, it can even be considered premature to start off with &amp;quot;as powerful types as you can&amp;quot;. So let's walk through programming things with as &amp;quot;dumb&amp;quot; types as possible, and see where types can help.&lt;/p&gt;
&lt;p&gt;We'll be following a process called &amp;quot;type-driven development&amp;quot; --- start with general and non-descriptive types, write the implementation and recognize partial functions and red flags, and slowly refine and add more and more powerful types to fix the problems.&lt;/p&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;&lt;img src="/img/entries/dependent-haskell-1/ffneural.png" title="Feed-forward ANN architecture" alt="Feed-forward ANN architecture" /&gt;&lt;/p&gt;
&lt;p&gt;Here's a quick run through on background for ANN's --- but remember, this isn't an article on ANN's, so we are going to be glossing over some of the details.&lt;/p&gt;
&lt;p&gt;We're going to be implementing a &lt;em&gt;feed-forward neural network&lt;/em&gt; with back-propagation training. These networks are layers of &amp;quot;nodes&amp;quot;, each connected to the each of the nodes of the previous layer. Input goes to the first layer, which feeds information to the next layer, which feeds it to the next, etc., until the final layer, where we read it off as the &amp;quot;answer&amp;quot; that the network is giving us. Layers between the input and output layers are called &lt;em&gt;hidden&lt;/em&gt; layers. Every node &amp;quot;outputs&amp;quot; a weighted sum of all of the outputs of the &lt;em&gt;previous&lt;/em&gt; layer, plus an always-on &amp;quot;bias&amp;quot; term (so that its result can be non-zero even when all of its inputs are zero). Symbolically, it looks like:&lt;/p&gt;
&lt;p&gt;$$ y&lt;em&gt;j = b&lt;/em&gt;j + \sum&lt;em&gt;i^m w&lt;/em&gt;{ij} x_i $$&lt;/p&gt;
&lt;p&gt;Or, if we treat the output of a layer and the list of list of weights as a matrix, we can write it a little cleaner:&lt;/p&gt;
&lt;p&gt;$$ \mathbf{y} = \mathbf{b} + W \mathbf{x} $$&lt;/p&gt;
&lt;p&gt;The result, the $n$-vector of nodes $\mathbf{y}$, is computed from the $n$-vector of biases $\mathbf{b}$ and the $n \times m$ weight matrix $W$ multiplied with the $m$-vector input, $\mathbf{x}$.&lt;/p&gt;
&lt;p&gt;To &amp;quot;scale&amp;quot; the result (and to give the system the magical powers of nonlinearity), we actually apply an &amp;quot;activation function&amp;quot; to the output before passing it down to the next step. We'll be using the popular &lt;a href="https://en.wikipedia.org/wiki/Logistic_function"&gt;logistic function&lt;/a&gt;, $f(x) = 1 / (1 + e^{-x})$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Training&lt;/em&gt; a network involves picking the right set of weights to get the network to answer the question you want.&lt;/p&gt;
&lt;h2&gt;Vanilla Types&lt;/h2&gt;
&lt;p&gt;We can store a network by storing the matrix of of weights and biases between each layer:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L18-20 data Weights = W { wBiases :: !(Vector Double) -- n , wNodes :: !(Matrix Double) -- n x m } -- &amp;quot;m to n&amp;quot; layer ~~~&lt;/p&gt;
&lt;p&gt;Now, a &lt;code&gt;Weights&lt;/code&gt; linking an &lt;em&gt;m&lt;/em&gt;-node layer to an &lt;em&gt;n&lt;/em&gt;-node layer has an &lt;em&gt;n&lt;/em&gt;-dimensional bias vector (one component for each output) and an &lt;em&gt;n&lt;/em&gt;-by-&lt;em&gt;m&lt;/em&gt; node weight matrix (one column for each output, one row for each input).&lt;/p&gt;
&lt;p&gt;(We're using the &lt;code&gt;Matrix&lt;/code&gt; type from the awesome &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hmatrix"&gt;hmatrix&lt;/a&gt;&lt;/em&gt; library for performant linear algebra, implemented using blas/lapack under the hood)&lt;/p&gt;
&lt;p&gt;A feed-forward neural network is then just a linked list of these weights:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L22-28 data Network :: * where O :: !Weights -&amp;gt; Network (:&amp;amp;~) :: !Weights -&amp;gt; !Network -&amp;gt; Network infixr 5 :&amp;amp;~ ~~~&lt;/p&gt;
&lt;p&gt;Note that we're using &lt;a href="https://en.wikibooks.org/wiki/Haskell/GADT"&gt;GADT&lt;/a&gt; syntax here, which just lets us define &lt;code&gt;Network&lt;/code&gt; (with a kind signature, &lt;code&gt;*&lt;/code&gt;) by providing the type of its &lt;em&gt;constructors&lt;/em&gt;, &lt;code&gt;O&lt;/code&gt; and &lt;code&gt;(:&amp;amp;~)&lt;/code&gt;. It'd be equivalent to the following normal data declaration:&lt;/p&gt;
&lt;p&gt;~~~haskell data Network = O Weights | Weights :&amp;amp;~ Network ~~~&lt;/p&gt;
&lt;p&gt;A network with one input layer, two inner layers, and one output layer would look like:&lt;/p&gt;
&lt;p&gt;~~~haskell ih :&amp;amp;~ hh :&amp;amp;~ O ho ~~~&lt;/p&gt;
&lt;p&gt;The first component is the weights from the input to first inner layer, the second is the weights between the two hidden layers, and the last is the weights between the last hidden layer and the output layer.&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- TODO: graphs using diagrams? --&amp;gt;&lt;/p&gt;
&lt;p&gt;We can write simple procedures, like generating random networks:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L46-56 randomWeights :: MonadRandom m =&amp;gt; Int -&amp;gt; Int -&amp;gt; m Weights randomWeights i o = do seed1 :: Int &amp;lt;- getRandom seed2 :: Int &amp;lt;- getRandom let wB = randomVector seed1 Uniform o * 2 - 1 wN = uniformSample seed2 o (replicate i (-1, 1)) return $ W wB wN&lt;/p&gt;
&lt;p&gt;randomNet :: MonadRandom m =&amp;gt; Int -&amp;gt; [Int] -&amp;gt; Int -&amp;gt; m Network randomNet i [] o = O &amp;lt;$&amp;gt; randomWeights i o randomNet i (h:hs) o = (:&amp;amp;~) &amp;lt;$&amp;gt; randomWeights i h &amp;lt;*&amp;gt; randomNet h hs o ~~~&lt;/p&gt;
&lt;p&gt;(We're using the &lt;code&gt;MonadRandom&lt;/code&gt; typeclass from the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/MonadRandom"&gt;MonadRandom&lt;/a&gt;&lt;/em&gt; library, which uses the mechanisms in &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/random-1.1/docs/System-Random.html"&gt;System.Random&lt;/a&gt;&lt;/em&gt; and gives us a generic way of working with monads where we can get random values with &lt;code&gt;getRandom&lt;/code&gt;, etc.)&lt;/p&gt;
&lt;p&gt;(&lt;a href="http://hackage.haskell.org/package/hmatrix-0.17.0.1/docs/Numeric-LinearAlgebra.html#v:randomVector"&gt;&lt;code&gt;randomVector&lt;/code&gt;&lt;/a&gt; and &lt;a href="http://hackage.haskell.org/package/hmatrix-0.17.0.1/docs/Numeric-LinearAlgebra.html#v:uniformSample"&gt;&lt;code&gt;uniformSample&lt;/code&gt;&lt;/a&gt; are from the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hmatrix"&gt;hmatrix&lt;/a&gt;&lt;/em&gt; library, generating random vectors and matrices from a random &lt;code&gt;Int&lt;/code&gt; seed. We manipulate them here to generate them with numbers between -1 and 1)&lt;/p&gt;
&lt;p&gt;And now we can write a function to &amp;quot;run&amp;quot; our network on a given input vector, following the matrix equation we wrote earlier:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L30-44 logistic :: Floating a =&amp;gt; a -&amp;gt; a logistic x = 1 / (1 + exp (-x))&lt;/p&gt;
&lt;p&gt;runLayer :: Weights -&amp;gt; Vector Double -&amp;gt; Vector Double runLayer (W wB wN) v = wB + wN #&amp;gt; v&lt;/p&gt;
&lt;p&gt;runNet :: Network -&amp;gt; Vector Double -&amp;gt; Vector Double runNet (O w) !v = logistic (runLayer w v) runNet (w :&amp;amp;~ n') !v = let v' = logistic (runLayer w v) in runNet n' v' ~~~&lt;/p&gt;
&lt;p&gt;(&lt;code&gt;#&amp;gt;&lt;/code&gt; is matrix-vector multiplication)&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- TODO: examples of running --&amp;gt;&lt;/p&gt;
&lt;p&gt;If you're a non-Haskell programmer, this might all seem perfectly fine and normal, and you probably have only a slightly elevated heart rate. If you are a Haskell programmer, you are most likely already having heart attacks. Let's imagine all of the bad things that could happen:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How do we know that we didn't accidentally mix up the dimensions for our implementation of &lt;code&gt;randomWeights&lt;/code&gt;? We could have switched parameters and be none the wiser.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do we even know that each subsequent matrix in the network is &amp;quot;compatible&amp;quot;? We want the outputs of one matrix to line up with the inputs of the next, but there's no way to know. It's possible to build a bad network, and things will just explode at runtime.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do we know the size of vector the network expects? What stops you from sending in a bad vector at run-time? We might do runtime-checks, but the compiler won't help us.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How do we verify that we have implemented &lt;code&gt;runLayer&lt;/code&gt; and &lt;code&gt;runNet&lt;/code&gt; in a way that they won't suddenly fail at runtime? We write &lt;code&gt;l #&amp;gt; v&lt;/code&gt;, but how do we know that it's even correct...what if we forgot to multiply something, or used something in the wrong places? We can it prove ourselves, but the compiler won't help us.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Back-propagation&lt;/h3&gt;
&lt;p&gt;Now, let's try implementing back-propagation! It's a textbook gradient descent algorithm. There are &lt;a href="https://en.wikipedia.org/wiki/Backpropagation"&gt;many explanations&lt;/a&gt; on the internet; the basic idea is that you try to minimize the squared error of what the neural network outputs for a given input vs. the actual expected output. You find the direction of change that minimizes the error (by finding the derivative), and move that direction. The implementation of backpropagation is found in many sources online and in literature, so let's see the implementation in Haskell:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L58-96 train :: Double -- ^ learning rate -&amp;gt; Vector Double -- ^ input vector -&amp;gt; Vector Double -- ^ target vector -&amp;gt; Network -- ^ network to train -&amp;gt; Network train rate x0 target = fst . go x0 where go :: Vector Double -- ^ input vector -&amp;gt; Network -- ^ network to train -&amp;gt; (Network, Vector Double) -- handle the output layer go !x (O w@(W wB wN)) = let y = runLayer w x o = logistic y -- the gradient (how much y affects the error) -- (logistic' is the derivative of logistic) dEdy = logistic' y * (o - target) -- new bias weights and node weights wB' = wB - scale rate dEdy wN' = wN - scale rate (dEdy &lt;code&gt;outer&lt;/code&gt; x) w' = W wB' wN' -- bundle of derivatives for next step dWs = tr wN #&amp;gt; dEdy in (O w', dWs) -- handle the inner layers go !x (w@(W wB wN) :&amp;amp;~ n) = let y = runLayer w x o = logistic y -- get dWs', bundle of derivatives from rest of the net (n', dWs') = go o n -- the gradient (how much y affects the error) dEdy = logistic' y * dWs' -- new bias weights and node weights wB' = wB - scale rate dEdy wN' = wN - scale rate (dEdy &lt;code&gt;outer&lt;/code&gt; x) w' = W wB' wN' -- bundle of derivatives for next step dWs = tr wN #&amp;gt; dEdy in (w' :&amp;amp;~ n', dWs) ~~~&lt;/p&gt;
&lt;p&gt;The algorithm computes the &lt;em&gt;updated&lt;/em&gt; network by recursively updating the layers, backwards up from the output layer. At every step, it returns the updated layer/network, as well as a bundle of derivatives (&lt;code&gt;dWs&lt;/code&gt;) for the next layer up to use to calculate its descent direction.&lt;/p&gt;
&lt;p&gt;At the output layer, all it needs to calculate the direction of descent is just &lt;code&gt;o - targ&lt;/code&gt;, the target. At the inner layers, it has to use the &lt;code&gt;dWs&lt;/code&gt; bundle it receives from the lower layers to figure it out. &lt;code&gt;dWs&lt;/code&gt; essentially &amp;quot;bubbles up&amp;quot; from the output layer up to the input layer calculations.&lt;/p&gt;
&lt;p&gt;Writing this is a bit of a struggle. I actually implemented this incorrectly several times before writing it as you see here. The type system doesn't help you like it normally does in Haskell, and you can't really use parametricity to help you write your code like normal Haskell. Everything is monomorphic, and everything multiplies with everything else. You don't have any hints about what to multiply with what at any point in time. It's like all of the bad things mentioned before, but amplified.&lt;/p&gt;
&lt;p&gt;In short, you're leaving yourself open to many potential bugs...and the compiler doesn't help you write your code at all! This is the nightmare of every Haskell programmer. There must be a better way![^better]&lt;/p&gt;
&lt;h4&gt;Putting it to the test&lt;/h4&gt;
&lt;p&gt;Pretty much the only way you can verify this code is to test it out on example cases. In the &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs"&gt;source file&lt;/a&gt;, I have &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L128-136"&gt;&lt;code&gt;main&lt;/code&gt;&lt;/a&gt; test out the backprop, training a network on a 2D function that was &amp;quot;on&amp;quot; for two small circles and &amp;quot;off&amp;quot; everywhere else (A nice cute non-linearly-separable function to test our network on). We basically train the network to be able to recognize the two-circle pattern. I implemented a simple printing function and tested the trained network on a grid:&lt;/p&gt;
&lt;p&gt;~~~bash $ stack install hmatrix MonadRandom $ stack ghc -- -O2 ./NetworkUntyped.hs $ ./NetworkUntyped&lt;/p&gt;
&lt;h1&gt;Training network...&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;.=########=&lt;/h1&gt;
&lt;h1&gt;.##############.&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;.##############-&lt;/h1&gt;
&lt;h1&gt;.&lt;/h1&gt;
&lt;h1&gt;... ...&lt;/h1&gt;
&lt;h1&gt;-##########.&lt;/h1&gt;
&lt;h1&gt;-##############.&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;=############=&lt;/h1&gt;
&lt;h1&gt;.#######=.&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;~~~&lt;/p&gt;
&lt;p&gt;Not too bad! The network learned to recognize the circles. But, I was basically forced to resort to unit testing to ensure my code was correct. Let's see if we can do better.&lt;/p&gt;
&lt;h3&gt;The Call of Types&lt;/h3&gt;
&lt;p&gt;Before we go on to the &amp;quot;typed&amp;quot; version of our program, let's take a step back and look at some big checks you might want to ask yourself after you write code in Haskell.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Are any of my functions either partial or implemented using partial functions?&lt;/li&gt;
&lt;li&gt;How could I have written things that are &lt;em&gt;incorrect&lt;/em&gt;, and yet still type check? Where does the compiler &lt;em&gt;not&lt;/em&gt; help me by restricting my choices?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of these questions usually yield some truth about the code you write and the things you should worry about. As a Haskeller, they should always be at the back of your mind!&lt;/p&gt;
&lt;p&gt;Looking back at our untyped implementation, we notice some things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Literally every single function we wrote is partial. Like, actually.[^literally] If we had passed in the incorrectly sized matrix/vector, or stored mismatched vectors in our network, everything would fall apart.&lt;/li&gt;
&lt;li&gt;There are billions of ways we could have implemented our functions where they would still typechecked. We could multiply mismatched matrices, or forget to multiply a matrix, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;With Static Size-Indexed Types&lt;/h2&gt;
&lt;h3&gt;Networks&lt;/h3&gt;
&lt;p&gt;Gauging our potential problems, it seems like the first major class of bugs we can address is improperly sized and incompatible matrices. If the compiler always made sure we used compatible matrices, we can avoid bugs at compile-time, and we also can get a friendly helper when we write programs (by knowing what works with what, and what we need were, and helping us organize our logic)&lt;/p&gt;
&lt;p&gt;Let's write a &lt;code&gt;Weights&lt;/code&gt; type that tells you the size of its output and the input it expects. Let's have, say, a &lt;code&gt;Weights 10 5&lt;/code&gt; be a set of weights that takes you from a layer of 10 nodes to a layer of 5 nodes. &lt;code&gt;w :: Weights 4 6&lt;/code&gt; would take you from a layer of 4 nodes to a layer of 6 nodes:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L21-23 data Weights i o = W { wBiases :: !(R o) , wNodes :: !(L o i) } -- an &amp;quot;o x i&amp;quot; layer ~~~&lt;/p&gt;
&lt;p&gt;The type constructor &lt;code&gt;Weights&lt;/code&gt; has the kind &lt;code&gt;Weights :: Nat -&amp;gt; Nat -&amp;gt; *&lt;/code&gt; --- it takes two types of kind &lt;code&gt;Nat&lt;/code&gt; (from the &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/base-4.8.2.0/docs/GHC-TypeLits.html"&gt;GHC.TypeLits&lt;/a&gt;&lt;/em&gt; module, which the integer type literals give us with &lt;em&gt;&lt;a href="https://www.schoolofhaskell.com/user/konn/prove-your-haskell-for-great-safety/dependent-types-in-haskell#type-level-naturals"&gt;DataKinds&lt;/a&gt;&lt;/em&gt; enabled) and returns a &lt;code&gt;*&lt;/code&gt; --- a &amp;quot;normal type&amp;quot;.&lt;/p&gt;
&lt;p&gt;We're using the &lt;em&gt;&lt;a href="http://mstksg.github.io/hmatrix/Numeric-LinearAlgebra-Static.html"&gt;Numeric.LinearAlgebra.Static&lt;/a&gt;&lt;/em&gt; module from &lt;em&gt;&lt;a href="http://hackage.haskell.org/package/hmatrix"&gt;hmatrix&lt;/a&gt;&lt;/em&gt;, which offers matrix and vector types with their size in their types: an &lt;code&gt;R 5&lt;/code&gt; is a vector of Doubles with 5 elements, and a &lt;code&gt;L 3 6&lt;/code&gt; is a 3x6 vector of Doubles.&lt;/p&gt;
&lt;p&gt;These types are called &amp;quot;dependent&amp;quot; types because the type itself &lt;em&gt;depends&lt;/em&gt; on its value. If an &lt;code&gt;R n&lt;/code&gt; contains a 5-element vector, its type is &lt;code&gt;R 5&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Static&lt;/em&gt; module in &lt;em&gt;hmatrix&lt;/em&gt; relies on the &lt;a href="http://hackage.haskell.org/package/base-4.8.2.0/docs/GHC-TypeLits.html#t:KnownNat"&gt;&lt;code&gt;KnownNat&lt;/code&gt;&lt;/a&gt; mechanism that GHC offers. Almost all operations in the library require a &lt;code&gt;KnownNat&lt;/code&gt; constraint on the type-level Nats --- for example, you can take the dot product of two vectors with &lt;code&gt;dot :: KnownNat n =&amp;gt; R n -&amp;gt; R n -&amp;gt; Double&lt;/code&gt;. It lets the library use the information in the &lt;code&gt;n&lt;/code&gt; at runtime as an &lt;code&gt;Integer&lt;/code&gt;. (More on this later!)&lt;/p&gt;
&lt;p&gt;Moving on, our network type for this post will be something like &lt;code&gt;Network 10 '[7,5,3] 2&lt;/code&gt;: Take 10 inputs, return 2 outputs --- and internally, have hidden layers of size 7, 5, and 3. (The &lt;code&gt;'[7,5,3]&lt;/code&gt; is a type-level list of Nats; the optional &lt;code&gt;'&lt;/code&gt; apostrophe is just for our own benefit to distinguish it from a value-level list of integers.)&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L25-32 data Network :: Nat -&amp;gt; [Nat] -&amp;gt; Nat -&amp;gt; * where O :: !(Weights i o) -&amp;gt; Network i '[] o (:&amp;amp;~) :: KnownNat h =&amp;gt; !(Weights i h) -&amp;gt; !(Network h hs o) -&amp;gt; Network i (h ': hs) o infixr 5 :&amp;amp;~ ~~~&lt;/p&gt;
&lt;p&gt;We use GADT syntax here again. The &lt;em&gt;kind signature&lt;/em&gt; of the type constructor means that the &lt;code&gt;Network&lt;/code&gt; type constructor takes three inputs: a &lt;code&gt;Nat&lt;/code&gt; (type-level numeral, like &lt;code&gt;10&lt;/code&gt; or &lt;code&gt;5&lt;/code&gt;), list of &lt;code&gt;Nat&lt;/code&gt;s, and another &lt;code&gt;Nat&lt;/code&gt; (the input, hidden layers, and output sizes). Let's go over the two constructors.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;O&lt;/code&gt; constructor takes a &lt;code&gt;Weights i o&lt;/code&gt; and returns a &lt;code&gt;Network i '[] o&lt;/code&gt;. That is, if your network is just weights from &lt;code&gt;i&lt;/code&gt; inputs to &lt;code&gt;o&lt;/code&gt; outputs, your network itself just takes &lt;code&gt;i&lt;/code&gt; inputs and returns &lt;code&gt;o&lt;/code&gt; outputs, with no hidden layers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;(:&amp;amp;~)&lt;/code&gt; constructor takes a &lt;code&gt;Network h hs o&lt;/code&gt; -- a network with &lt;code&gt;h&lt;/code&gt; inputs and &lt;code&gt;o&lt;/code&gt; outputs -- and &amp;quot;conses&amp;quot; an extra input layer in front. If you give it a &lt;code&gt;Weights i h&lt;/code&gt;, its outputs fit perfectly into the inputs of the subnetwork, and you get a &lt;code&gt;Network i (h ': hs) o&lt;/code&gt;. (&lt;code&gt;(':)&lt;/code&gt;, or &lt;code&gt;(:)&lt;/code&gt;, is the same as normal &lt;code&gt;(:)&lt;/code&gt;, but is for type-level lists. The apostrophe is optional here too, but it's just nice to be able to visually distinguish the two)&lt;/p&gt;
&lt;p&gt;We add a &lt;code&gt;KnownNat&lt;/code&gt; constraint on the &lt;code&gt;h&lt;/code&gt;, so that whenever you pattern match on &lt;code&gt;w :&amp;amp;~ net&lt;/code&gt;, you automatically get a &lt;code&gt;KnownNat&lt;/code&gt; constraint for the input size of &lt;code&gt;net&lt;/code&gt; (and the output of &lt;code&gt;w&lt;/code&gt;) that the &lt;em&gt;hmatrix&lt;/em&gt; library can use.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can still construct them the same way:&lt;/p&gt;
&lt;p&gt;~~~haskell -- given: ih :: Weights 10 7 hh :: Weights 7 4 ho :: Weights 4 2&lt;/p&gt;
&lt;p&gt;-- we have: O ho :: Network 4 '[] 2 hh :&amp;amp;~ O ho :: Network 7 '[4] 2 ih :&amp;amp;~ hh :&amp;amp;~ O ho :: Network 10 '[7,4] 2 ~~~&lt;/p&gt;
&lt;p&gt;Note that the shape of the constructors requires all of the weight vectors to &amp;quot;fit together&amp;quot;. &lt;code&gt;ih :&amp;amp;~ O ho&lt;/code&gt; would be a type error (feeding a 7-output layer to a 4-input layer). Also, if we ever pattern match on &lt;code&gt;:&amp;amp;~&lt;/code&gt;, we know that the resulting matrices and vectors are compatible!&lt;/p&gt;
&lt;p&gt;One neat thing is that this approach is also self-documenting. I don't need to specify what the dimensions are in the docs and trust the users to read it and obey it. The types tell them! And if they don't listen, they get a compiler error! (You should, of course, still provide reasonable documentation. But, in this case, the compiler actually enforces your documentation's statements!)&lt;/p&gt;
&lt;p&gt;Generating random weights and networks is even nicer now:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L57-64 randomWeights :: (MonadRandom m, KnownNat i, KnownNat o) =&amp;gt; m (Weights i o) randomWeights = do s1 :: Int &amp;lt;- getRandom s2 :: Int &amp;lt;- getRandom let wB = randomVector s1 Uniform * 2 - 1 wN = uniformSample s2 (-1) 1 return $ W wB wN ~~~&lt;/p&gt;
&lt;p&gt;Notice that the &lt;em&gt;Static&lt;/em&gt; versions of &lt;a href="http://mstksg.github.io/hmatrix/Numeric-LinearAlgebra-Static.html#v:randomVector"&gt;&lt;code&gt;randomVector&lt;/code&gt;&lt;/a&gt; and &lt;a href="http://mstksg.github.io/hmatrix/Numeric-LinearAlgebra-Static.html#v:uniformSample"&gt;&lt;code&gt;uniformSample&lt;/code&gt;&lt;/a&gt; don't actually require the size of the vector/matrix you want as an input -- they just use &lt;em&gt;type inference&lt;/em&gt; to figure out what size you want! This is the same process that &lt;a href="http://hackage.haskell.org/package/base-4.8.2.0/docs/Prelude.html#v:read"&gt;&lt;code&gt;read&lt;/code&gt;&lt;/a&gt; uses to figure out what type of thing you want to return. You would use &lt;code&gt;randomVector s Uniform :: R 10&lt;/code&gt;, and type inference would give you a 10-element vector the same way &lt;code&gt;read &amp;quot;hello&amp;quot; :: Int&lt;/code&gt; would give you an &lt;code&gt;Int&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It's important to note that it's much harder to implement this incorrectly. Before, you could give the matrix the wrong dimensions (maybe you flipped the parameters?), or gave the wrong parameter to the vector generator.&lt;/p&gt;
&lt;p&gt;But here, you are guaranteed/forced to return the correctly sized vectors and matrices. In fact, you &lt;em&gt;don't even have to worry&lt;/em&gt; about it --- it's handled automatically by the magic of type inference[^hindley]! I consider this a very big victory. One of the whole points of types is to give you less to &amp;quot;worry about&amp;quot;, as a programmer. Here, we completely eliminate an &lt;em&gt;entire dimension&lt;/em&gt; of programmer concern.&lt;/p&gt;
&lt;h4&gt;Benefits to the user&lt;/h4&gt;
&lt;p&gt;Not only is this style nicer for you as the implementer, it's also very beneficial for the &lt;em&gt;user&lt;/em&gt; of the function. Consider looking at the two competing type signatures side-by-side:&lt;/p&gt;
&lt;p&gt;~~~haskell randomWeights :: Int -&amp;gt; Int -&amp;gt; m Weights randomWeights :: m (Weights i o) ~~~&lt;/p&gt;
&lt;p&gt;If you want to &lt;em&gt;use&lt;/em&gt; this function, you have to look up some things from the documentation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What do the two arguments represent?&lt;/li&gt;
&lt;li&gt;What &lt;em&gt;order&lt;/em&gt; is the function expecting these two arguments?&lt;/li&gt;
&lt;li&gt;What will be the dimension of the result?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These are three things you &lt;em&gt;need&lt;/em&gt; to look up in the documentation. There's simply no way around it.&lt;/p&gt;
&lt;p&gt;But, here, all of these questions are answered &lt;em&gt;immediately&lt;/em&gt;, just from the type (which you can get from GHC, or from ghci). You don't need to worry about arguments. You don't need to worry about what order the function is expecting the arguments to be in. And you already know &lt;em&gt;exactly&lt;/em&gt; what the dimensions of the result is, right in the type.&lt;/p&gt;
&lt;p&gt;I often implement many of my functions in this style, even if the rest of my program isn't intended to be dependently typed (I can just convert the type to a &amp;quot;dumb&amp;quot; type as soon as I get the result). All of these benefits come even when the caller doesn't &lt;em&gt;care&lt;/em&gt; at all about dependently typed programming --- it's just a better style of defining functions/offering an API!&lt;/p&gt;
&lt;h3&gt;Singletons and Induction&lt;/h3&gt;
&lt;p&gt;The code for the updated &lt;code&gt;randomNet&lt;/code&gt; takes a bit of background to understand, so let's take a quick detour through the concepts of singletons, dependent pattern matching, and induction on dependent data types.[^old]&lt;/p&gt;
&lt;p&gt;Let's say we want to implement an algorithm that can create any &lt;code&gt;Network i hs o&lt;/code&gt;, so that we can construct a &lt;code&gt;Network 4 '[3,2] 1&lt;/code&gt; or something. In true Haskell fashion, we want do this recursively (&amp;quot;inductively&amp;quot;). After all, we know how to make a &lt;code&gt;Network i '[] o&lt;/code&gt; (just &lt;code&gt;O &amp;lt;$&amp;gt; randomWeights&lt;/code&gt;), and we know how to create a &lt;code&gt;Network i (h ': hs) o&lt;/code&gt; if we had a &lt;code&gt;Network h hs o&lt;/code&gt; (just use &lt;code&gt;(:&amp;amp;~)&lt;/code&gt; with &lt;code&gt;randomWeights&lt;/code&gt;). Now all we have to do is just &amp;quot;pattern match&amp;quot; on the type-level list, and...&lt;/p&gt;
&lt;p&gt;Oh wait. We can't pattern match on types like that in Haskell. This is a consequence of one of Haskell's fundamental design decisions: types are &lt;strong&gt;erased&lt;/strong&gt; at runtime. We need to have a way to &amp;quot;access&amp;quot; the type (at run-time) as a &lt;em&gt;value&lt;/em&gt; so we can pattern match on it and do things with it.&lt;/p&gt;
&lt;p&gt;In Haskell, the popular way to deal with this is by using &lt;em&gt;singletons&lt;/em&gt; --- (parameterized) types which only have valid constructor. The canonical method of working with singletons in Haskell is with the &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/singletons"&gt;singletons&lt;/a&gt;&lt;/em&gt; library, which provides a uniform interface for all sorts of singletons of types you'll encounter in everyday use.&lt;/p&gt;
&lt;p&gt;We want to &amp;quot;pattern match&amp;quot; on a type-level list, so we want a singleton for lists. The &lt;em&gt;singletons&lt;/em&gt; library provides them:&lt;/p&gt;
&lt;p&gt;~~~haskell SNil :: Sing '[] SCons :: Sing a -&amp;gt; Sing as -&amp;gt; Sing (a ': as) ~~~&lt;/p&gt;
&lt;p&gt;This means that if we ever get value of type &lt;code&gt;Sing as&lt;/code&gt; (and &lt;code&gt;as&lt;/code&gt; is a type-level list), we can pattern match on it. If we match on the &lt;code&gt;SNil&lt;/code&gt; constructor, we &lt;em&gt;know&lt;/em&gt; it's a &lt;code&gt;Sing '[]&lt;/code&gt; in that branch, and if we match on the &lt;code&gt;SCons&lt;/code&gt; constructor, we &lt;em&gt;know&lt;/em&gt; it's a &lt;code&gt;Sing (a ': as)&lt;/code&gt; -- a non-empty list. This is called &lt;em&gt;dependent pattern matching&lt;/em&gt;. Every &amp;quot;branch&amp;quot; of your case statement has a different inferred type of the arguments, depending on the constructor you match on.&lt;/p&gt;
&lt;p&gt;~~~haskell case foo of SNil -&amp;gt; ... -- here, GHC knows &lt;code&gt;foo :: Sing '[]&lt;/code&gt; SCons _ _ -&amp;gt; ... -- here, GHC knows &lt;code&gt;foo :: Sing (a ': as)&lt;/code&gt; ~~~&lt;/p&gt;
&lt;p&gt;&lt;em&gt;singletons&lt;/em&gt; actually provides a whole bunch of singleton constructors for different types and kinds, like for &lt;code&gt;Bool&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell STrue :: Sing 'True SFalse :: Sing 'False ~~~&lt;/p&gt;
&lt;p&gt;(That's the &lt;em&gt;type&lt;/em&gt; &lt;code&gt;'True&lt;/code&gt;, of &lt;em&gt;kind&lt;/em&gt; &lt;code&gt;Bool&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;So, if we ever are given a &lt;code&gt;Sing b&lt;/code&gt; with some type-level &lt;code&gt;Bool&lt;/code&gt; we don't know, we can pattern match on it. And in the branch that &lt;code&gt;STrue&lt;/code&gt; matches on, &lt;code&gt;b&lt;/code&gt; is &lt;code&gt;'True&lt;/code&gt;, and in the branch that &lt;code&gt;SFalse&lt;/code&gt; matches on, &lt;code&gt;b&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Singletons give us a way to pattern match on types by having an actual term-level value we can pattern match on. So, we &lt;em&gt;could&lt;/em&gt; implement:&lt;/p&gt;
&lt;p&gt;~~~haskell randomNet :: (MonadRandom m, KnownNat i, KnownNat o) =&amp;gt; Sing hs -&amp;gt; m (Network i hs o) ~~~&lt;/p&gt;
&lt;p&gt;And &lt;code&gt;randomNet&lt;/code&gt; gets to directly pattern match and deconstruct on &lt;code&gt;Sing hs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;However, for actual API's, it's often more convenient to &lt;em&gt;not&lt;/em&gt; require the extra parameter, and have it be &amp;quot;inferred&amp;quot; in the way we've been doing it before. That way the &lt;em&gt;user&lt;/em&gt; doesn't have the burden of supplying it. The &lt;em&gt;singletons&lt;/em&gt; library offers a typeclass we can use to implicitly conjure up values of a singleton type -- &lt;code&gt;SingI&lt;/code&gt;. We can use &lt;code&gt;sing :: SingI s =&amp;gt; Sing s&lt;/code&gt; to generate the &amp;quot;inferred&amp;quot; singleton:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; sing :: Sing '[] SNil ghci&amp;gt; sing :: Sing 'True STrue ghci&amp;gt; sing :: Sing '['True, 'False, 'True] STrue &lt;code&gt;SCons&lt;/code&gt; SFalse &lt;code&gt;SCons&lt;/code&gt; STrue &lt;code&gt;SCons&lt;/code&gt; SNil ~~~&lt;/p&gt;
&lt;p&gt;So if you have a function &lt;code&gt;SingI hs =&amp;gt; ...&lt;/code&gt;, it's really no different than &lt;code&gt;Sing hs -&amp;gt; ...&lt;/code&gt;. The function itself gets to use a &lt;code&gt;Sing hs&lt;/code&gt; either way ... but for the first, the argument is implicit.&lt;/p&gt;
&lt;p&gt;The final piece of the puzzle is the singleton for a type-level &lt;code&gt;Nat&lt;/code&gt;. It's a little different because when you pattern match on it, instead of directly learning about the type, you &amp;quot;receive&amp;quot; a &lt;code&gt;KnownNat&lt;/code&gt; instance you can use.&lt;/p&gt;
&lt;p&gt;~~~haskell SNat :: KnownNat n =&amp;gt; Sing n ~~~&lt;/p&gt;
&lt;p&gt;~~~haskell -- &lt;code&gt;foo :: Sing n&lt;/code&gt;, but we don't know what &lt;code&gt;n&lt;/code&gt; it is case foo of SNat -&amp;gt; ... -- in this branch, we have a &lt;code&gt;KnownNat n&lt;/code&gt; instance ~~~&lt;/p&gt;
&lt;p&gt;Essentially, the data constructor comes &amp;quot;packaged&amp;quot; with a &lt;code&gt;KnownNat n&lt;/code&gt; instance. The &lt;em&gt;creation&lt;/em&gt; of &lt;code&gt;SNat :: Sing n&lt;/code&gt; requires the presence of &lt;code&gt;KnownNat n&lt;/code&gt;. So if you ever pattern match on a validly created &lt;code&gt;SNat&lt;/code&gt;, the fact that that &lt;code&gt;SNat&lt;/code&gt; constructor even exists (instead of, say, being &lt;code&gt;undefined&lt;/code&gt;) is a &lt;em&gt;witness&lt;/em&gt; to that very &lt;code&gt;KnownNat&lt;/code&gt; instance, and the type system lets us use this. It's as if you &amp;quot;pattern match&amp;quot; out the instance itself, like any other value the constructor might have.&lt;/p&gt;
&lt;p&gt;Now we have enough pieces of the puzzle:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L66-75 randomNet :: forall m i hs o. (MonadRandom m, KnownNat i, SingI hs, KnownNat o) =&amp;gt; m (Network i hs o) randomNet = go sing where go :: forall h hs'. KnownNat h =&amp;gt; Sing hs' -&amp;gt; m (Network h hs' o) go = \case SNil -&amp;gt; O &amp;lt;$&amp;gt; randomWeights SNat &lt;code&gt;SCons&lt;/code&gt; ss -&amp;gt; (:&amp;amp;~) &amp;lt;$&amp;gt; randomWeights &amp;lt;*&amp;gt; go ss ~~~&lt;/p&gt;
&lt;p&gt;The real heavy lifting is done by &lt;code&gt;go&lt;/code&gt; (written with &lt;a href="https://www.schoolofhaskell.com/school/to-infinity-and-beyond/pick-of-the-week/guide-to-ghc-extensions/basic-syntax-extensions#lambdacase"&gt;LambdaCase&lt;/a&gt;), which takes the singleton structure it needs and recursively calls it until it reaches the base case (&lt;code&gt;SNil&lt;/code&gt;, an output layer). We just call &lt;code&gt;go sing&lt;/code&gt; to give it the initial structure it needs. Note there, &lt;code&gt;sing :: Sing hs&lt;/code&gt;, but this is inferred, because &lt;code&gt;go&lt;/code&gt; is &lt;code&gt;Sing hs -&amp;gt; Network i hs o&lt;/code&gt;, and it's being asked to return a &lt;code&gt;Network i hs o&lt;/code&gt;, so it's safely inferable that we want &lt;code&gt;Sing hs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Remember that we can write &lt;code&gt;O &amp;lt;$&amp;gt; randomWeights&lt;/code&gt; because &lt;code&gt;randomWeights&lt;/code&gt;, like &lt;code&gt;read&lt;/code&gt;, adapts to whatever type we want from it --- in this case, we ask for a &lt;code&gt;Weights h o&lt;/code&gt;, and type inference is the real hero.&lt;/p&gt;
&lt;p&gt;When possible, we like to write functions like &lt;code&gt;go&lt;/code&gt; that take &lt;em&gt;explicit&lt;/em&gt; singletons. In a lot of situations, we'll actually write our internal &lt;em&gt;logic&lt;/em&gt; itself using explicit singletons, and only use &lt;code&gt;SingI&lt;/code&gt; and implicit singletons at the &lt;em&gt;external&lt;/em&gt; boundaries of our API (like &lt;code&gt;randomNet&lt;/code&gt;) for convenience to the user.&lt;/p&gt;
&lt;p&gt;We've stumbled upon common pattern in dependent Haskell: &amp;quot;building up&amp;quot; a value-level singleton &lt;em&gt;structure&lt;/em&gt; from a type that we want (either explicitly given as an argument, or provided through a typeclass like &lt;code&gt;SingI&lt;/code&gt;) and then inductively piggybacking on that structure's constructors to build the thing you &lt;em&gt;really&lt;/em&gt; want (called &amp;quot;elimination&amp;quot;). Here, we use &lt;code&gt;SingI hs&lt;/code&gt; and &amp;quot;eliminate&amp;quot; that structure to create our &lt;code&gt;Network i hs o&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;On Typeclasses and Dictionaries&lt;/h4&gt;
&lt;p&gt;One of the more bizarre things here, to me, is that &lt;code&gt;SNat&lt;/code&gt; somehow gave us a &lt;code&gt;KnownNat n&lt;/code&gt; instance that we can use and pass off to &lt;code&gt;randomWeights&lt;/code&gt;. However, once you realize that typeclasses in Haskell really aren't any more than a way to pass in implicit arguments, it starts to make sense.&lt;/p&gt;
&lt;p&gt;The only thing you can really do from a &lt;code&gt;KnownNat&lt;/code&gt; is to get an &lt;code&gt;Integer&lt;/code&gt; from it with &lt;code&gt;natVal&lt;/code&gt;. So really, &lt;code&gt;KnownNat n =&amp;gt; ...&lt;/code&gt; is more or less the same as &lt;code&gt;Integer -&amp;gt; ...&lt;/code&gt;. That's right --- at runtime, a &lt;code&gt;KnownNat n&lt;/code&gt; constraint is more or less just an &lt;code&gt;Integer&lt;/code&gt; that GHC passes around automatically for you, to save you the hassle of manually passing it in yourself. (We say that the &amp;quot;dictionary&amp;quot; of &lt;code&gt;KnownNat&lt;/code&gt; is &lt;code&gt;Integer&lt;/code&gt;.)&lt;/p&gt;
&lt;p&gt;So, the constructor:&lt;/p&gt;
&lt;p&gt;~~~haskell SNat :: KnownNat n =&amp;gt; Sing n ~~~&lt;/p&gt;
&lt;p&gt;Is really &lt;em&gt;kind&lt;/em&gt; of like:&lt;/p&gt;
&lt;p&gt;~~~haskell SNat :: Integer -&amp;gt; Sing n -- or, in normal data type notation SNat Integer -- kinda! ~~~&lt;/p&gt;
&lt;p&gt;The GADT constructor for &lt;code&gt;SNat&lt;/code&gt; requires a &lt;code&gt;KnownNat n&lt;/code&gt; instance in scope to produce. That instance is essentially stored inside the constructor (as if it were just an &lt;code&gt;Integer&lt;/code&gt;). Then, later, when you pattern match on it, you pattern match out the instance that was originally put in there, and you can use it!&lt;/p&gt;
&lt;p&gt;So what's the big deal, why not just ditch &lt;code&gt;KnownNat&lt;/code&gt; and just pass around integers? The difference is that GHC and the compiler can now &lt;em&gt;track&lt;/em&gt; these at compile-time to give you &lt;em&gt;checks&lt;/em&gt; on how your Nat's act together on the type level, allowing it to catch mismatches with compile-time checks instead of run-time checks.&lt;/p&gt;
&lt;h3&gt;Running with it&lt;/h3&gt;
&lt;p&gt;So now, you can use &lt;code&gt;randomNet :: IO (Network 5 '[4,3] 2)&lt;/code&gt; to get a random network of the desired dimensions! (&lt;code&gt;IO&lt;/code&gt; is an instance of &lt;code&gt;MonadRandom&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Can we just pause right here to just appreciate how awesome it is that we can generate random networks of whatever size we want by &lt;em&gt;just requesting something by its type&lt;/em&gt;? Our implementation is also &lt;em&gt;guaranteed&lt;/em&gt; to have the right sized matrices --- no worrying about using the right size parameters for the right matrix in the right order. GHC does it for you automatically! And, for the person who &lt;em&gt;uses&lt;/em&gt; &lt;code&gt;randomNet&lt;/code&gt;, they don't have to bungle around with figuring out what function argument indicates what, and in what order, and they don't have to play a guessing game about the shape of the returned matrix.&lt;/p&gt;
&lt;p&gt;The code for &lt;em&gt;running&lt;/em&gt; the nets is actually literally identical from before:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L42-55 runLayer :: (KnownNat i, KnownNat o) =&amp;gt; Weights i o -&amp;gt; R i -&amp;gt; R o runLayer (W wB wN) v = wB + wN #&amp;gt; v&lt;/p&gt;
&lt;p&gt;runNet :: (KnownNat i, KnownNat o) =&amp;gt; Network i hs o -&amp;gt; R i -&amp;gt; R o runNet = \case O w -&amp;gt; (!v) -&amp;gt; logistic (runLayer w v) (w :&amp;amp;~ n') -&amp;gt; (!v) -&amp;gt; let v' = logistic (runLayer w v) in runNet n' v' ~~~&lt;/p&gt;
&lt;p&gt;But now, we get the assurance that the matrices and vectors all fit each-other, at compile-time. GHC basically writes our code for us. The operations all demand vectors and matrices that &amp;quot;fit together&amp;quot;, so you can only ever multiply a matrix by a properly sized vector.&lt;/p&gt;
&lt;p&gt;~~~haskell (+) :: KnownNat n =&amp;gt; R n -&amp;gt; R n -&amp;gt; R n (#&amp;gt;) :: (KnownNat n, KnownNat m) =&amp;gt; L n m -&amp;gt; R m -&amp;gt; R n&lt;/p&gt;
&lt;p&gt;logistic :: KnownNat n =&amp;gt; R n -&amp;gt; R n ~~~&lt;/p&gt;
&lt;p&gt;The source code is the same from before, so there isn't any extra overhead in annotation. The correctness proofs and guarantees basically come without any extra work --- they're free!&lt;/p&gt;
&lt;p&gt;Our back-prop algorithm is ported pretty nicely too:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L77-116 train :: forall i hs o. (KnownNat i, KnownNat o) =&amp;gt; Double -- ^ learning rate -&amp;gt; R i -- ^ input vector -&amp;gt; R o -- ^ target vector -&amp;gt; Network i hs o -- ^ network to train -&amp;gt; Network i hs o train rate x0 target = fst . go x0 where go :: forall j js. KnownNat j =&amp;gt; R j -- ^ input vector -&amp;gt; Network j js o -- ^ network to train -&amp;gt; (Network j js o, R j) go !x (O w@(W wB wN)) = let y = runLayer w x o = logistic y -- the gradient (how much y affects the error) -- (logistic' is the derivative of logistic) dEdy = logistic' y * (o - target) -- new bias weights and node weights wB' = wB - konst rate * dEdy wN' = wN - konst rate * (dEdy &lt;code&gt;outer&lt;/code&gt; x) w' = W wB' wN' -- bundle of derivatives for next step dWs = tr wN #&amp;gt; dEdy in (O w', dWs) -- handle the inner layers go !x (w@(W wB wN) :&amp;amp;~ n) = let y = runLayer w x o = logistic y -- get dWs', bundle of derivatives from rest of the net (n', dWs') = go o n -- the gradient (how much y affects the error) dEdy = logistic' y * dWs' -- new bias weights and node weights wB' = wB - konst rate * dEdy wN' = wN - konst rate * (dEdy &lt;code&gt;outer&lt;/code&gt; x) w' = W wB' wN' -- bundle of derivatives for next step dWs = tr wN #&amp;gt; dEdy in (w' :&amp;amp;~ n', dWs) ~~~&lt;/p&gt;
&lt;p&gt;It's pretty much again almost an exact copy-and-paste, but now with GHC checking to make sure everything fits together in our implementation.&lt;/p&gt;
&lt;p&gt;One thing that's hard for me to convey here without walking through the implementation step-by-step is how much the types &lt;em&gt;help you&lt;/em&gt; in writing this code.&lt;/p&gt;
&lt;p&gt;Before starting writing a back-prop implementation without the help of types, I'd probably be a bit concerned. I mentioned earlier that writing the untyped version was no fun at all. But, with the types, writing the implementation became a &lt;em&gt;joy&lt;/em&gt; again. And, you have the help of &lt;em&gt;hole driven development&lt;/em&gt;, too.&lt;/p&gt;
&lt;p&gt;If you need, say, an &lt;code&gt;R n&lt;/code&gt;, there might be only one way get it! And if you have something that you need to combine with something you don't know about, you can use typed holes (&lt;code&gt;_&lt;/code&gt;) and GHC will give you a list of all the values you have in scope that can fit there. Your programs basically write themselves!&lt;/p&gt;
&lt;p&gt;The more you can restrict the implementations of your functions with your types, the more of a joy programming in Haskell is. Things fit together and fall together before your eyes...and the best part is that if they're wrong, the compiler will nudge you gently into the correct direction.&lt;/p&gt;
&lt;p&gt;The most stressful part of programming happens when you have to tenuously hold a complex and fragile network of ideas and constraints in your brain, and any slight distraction or break in focus causes everything to crash down in your mind. Over time, people have begun to believe that this is &amp;quot;normal&amp;quot; in programming. Don't believe this lie --- it's &lt;em&gt;not&lt;/em&gt;! A good programming experience involves maintaining as &lt;em&gt;little&lt;/em&gt; in your head as possible, and letting the compiler handle remembering/checking the rest.&lt;/p&gt;
&lt;h4&gt;The final test&lt;/h4&gt;
&lt;p&gt;You can download the &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs"&gt;typed network&lt;/a&gt; source code and run it yourself. Again, the &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L151-159"&gt;&lt;code&gt;main&lt;/code&gt;&lt;/a&gt; method is written identically to that of the other file and tests the identical function.&lt;/p&gt;
&lt;p&gt;~~~bash $ stack install hmatrix MonadRandom singletons $ stack ghc -- -O2 ./NetworkTyped.hs $ ./NetworkTyped&lt;/p&gt;
&lt;h1&gt;Training network...&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;-#########-&lt;/h1&gt;
&lt;h1&gt;-#############=&lt;/h1&gt;
&lt;h1&gt;-###############-&lt;/h1&gt;
&lt;h1&gt;=###############=&lt;/h1&gt;
&lt;h1&gt;##############=.&lt;/h1&gt;
&lt;h1&gt;.##########=.&lt;/h1&gt;
&lt;h1&gt;.==#=-&lt;/h1&gt;
&lt;h1&gt;-###########-&lt;/h1&gt;
&lt;h1&gt;=##############.&lt;/h1&gt;
&lt;h1&gt;.###############=&lt;/h1&gt;
&lt;h1&gt;=##############-&lt;/h1&gt;
&lt;h1&gt;=############-&lt;/h1&gt;
&lt;h1&gt;-######=-.&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;~~~&lt;/p&gt;
&lt;h2&gt;Finding Something to Depend on&lt;/h2&gt;
&lt;p&gt;We wrote out an initial &amp;quot;non-typed&amp;quot; implementation and recognized a lot red flags that you might already be trained to recognize if you have been programming Haskell for a while: &lt;em&gt;partial functions&lt;/em&gt; and &lt;em&gt;multiple potential implementations&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We followed our well-tuned Haskell guts, listened to our hearts, and introduced extra power in our types to remove all partial functions and eliminate &lt;em&gt;most&lt;/em&gt; potential implementations (though not all, yet --- there are more gains to be made from pulling in more parametric polymorphism).&lt;/p&gt;
&lt;p&gt;Though we might have been able to find the bugs we avoided &amp;quot;eventually&amp;quot;, we were able to remove entire &lt;em&gt;dimensions&lt;/em&gt; of programmer concern and also leverage parametric polymorphism to help write our programs for us. We found joy again in programming.&lt;/p&gt;
&lt;p&gt;In the process, however, we encountered some unexpected resistance from Haskell (the language). We couldn't directly pattern match on our types, so we ended up playing games with singletons and GADT constructors to pass instances.&lt;/p&gt;
&lt;p&gt;In practice, using types as powerful and descriptive as these begin to require a whole new set of tools once you get past the simplest use cases here. For example, our &lt;code&gt;Network&lt;/code&gt; types so far required you to specify their size in the program itself (&lt;code&gt;Network 2 '[16, 8] 1&lt;/code&gt; in the example source code, for instance). But what if we wanted to generate a network that has runtime-determined size (For example, getting the size from user input)? What if we wanted to load a pre-trained network whose size we don't know? How can we manipulate our networks in a &amp;quot;dynamic&amp;quot; and generic way that still gives us all of the benefits of type-safe programming? We'll found out next post!&lt;/p&gt;
&lt;p&gt;What we're looking at here is a world where &lt;em&gt;types&lt;/em&gt; can depend on run-time values ... and values can depend on types. A world where types can be returned from functions and where types become as much of a manipulatable citizen of as values are.&lt;/p&gt;
&lt;p&gt;The art of working with types like this is &lt;em&gt;dependently typed programming&lt;/em&gt;. We're going to feel a bit of push back from Haskell at first, but after we hit our stride and tame the tools we need, we're going to open up a whole new world of potential!&lt;/p&gt;
&lt;h3&gt;Exercises&lt;/h3&gt;
&lt;p&gt;Here are some exercises you can do for fun to test your understanding and apply some of the concepts! The links are to the solutions in the source file.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Write a function that &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L167-168"&gt;&amp;quot;pops&amp;quot; the input layer&lt;/a&gt; off of a &lt;code&gt;Network&lt;/code&gt;, returning both the input layer's weights and the rest of the network, &lt;code&gt;(Weights i h, Network h hs o)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Think about what its type would have to be. Could it possibly be called with a network that cannot be popped? (that is, that has only one layer?)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L170-187"&gt;function that takes two networks of the same dimensions and adds together their weights&lt;/a&gt;. Remember that &lt;code&gt;L m n&lt;/code&gt; has a &lt;code&gt;Num&lt;/code&gt; instance that adds the matrices together element-by-element.&lt;/p&gt;
&lt;p&gt;Could this function ever be accidentally called on two networks that have different internal structures?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write a function that takes a &lt;code&gt;Network i hs o&lt;/code&gt; and &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L189-192"&gt;returns the singleton representing its hidden layer structure&lt;/a&gt; --- &lt;code&gt;Sing hs&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell hiddenSing :: Network i hs o -&amp;gt; Sing hs ~~~&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; n &amp;lt;- randomNet :: IO (Network 10 '[5,3] 1) ghci&amp;gt; let s = hiddenSing n ghci&amp;gt; :t s s :: Sing '[5,3] ghci&amp;gt; s SNat &lt;code&gt;SCons&lt;/code&gt; SNat &lt;code&gt;SCons&lt;/code&gt; SNil ~~~&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;</description><author>Justin Le</author><category>Haskell</category><category>Ramblings</category><guid isPermaLink="true">https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html</guid><pubDate>Wed, 25 May 2016 19:46:21 UTC</pubDate><creator>Justin Le</creator><subject>Haskell, Ramblings</subject><date>2016-05-25</date></item><item><title>Automatic Propagation of Uncertainty with AD</title><link>https://blog.jle.im/entry/automatic-propagation-of-uncertainty-with-ad.html</link><description>&lt;blockquote&gt;
&lt;p&gt;This post and &lt;a href="https://blog.jle.im/entries/series/+uncertain.html"&gt;series&lt;/a&gt; is a walk-through of the implementation of my &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/uncertain"&gt;uncertain&lt;/a&gt;&lt;/em&gt; library, now on hackage!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some of my favorite Haskell &amp;quot;tricks&amp;quot; involve working with exotic numeric types with custom &amp;quot;overloaded&amp;quot; numeric functions and literals that let us work with data in surprisingly elegant and expressive ways.&lt;/p&gt;
&lt;p&gt;Here is one example --- from my work in experimental physics and statistics, we often deal with experimental/sampled values with inherent uncertainty. If you ever measure something to be $12.3\,\mathrm{cm}$, that doesn't mean it's $12.300000\,\mathrm{cm}$ --- it means that it's somewhere between $12.2\,\mathrm{cm}$ and $12.4\,\mathrm{cm}$...and we don't know exactly. We can write it as $12.3 \pm 0.1\,\mathrm{cm}$. The interesting thing happens when we try to add, multiply, divide numbers with uncertainty. What happens when you &amp;quot;add&amp;quot; $12 \pm 3$ and $19 \pm 6$?&lt;/p&gt;
&lt;p&gt;The initial guess might be $31 \pm 9$, because one is $\pm 3$ and the other is $\pm 6$. But! If you actually do experiments like this several times, you'll see that this isn't the case. If you tried this out experimentally and simulate several hundred trials, you'll see that the answer is actually something like $31 \pm 7$. (We'll explain why later, but feel free to stop reading this article now and try this out yourself![^try])&lt;/p&gt;
&lt;p&gt;Let's write ourselves a Haskell data type that lets us work with &amp;quot;numbers with inherent uncertainty&amp;quot;:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; let x = 14.6 +/- 0.8 ghci&amp;gt; let y = 31 +/- 2 ghci&amp;gt; x + y 46 +/- 2 ghci&amp;gt; x * y 450 +/- 40 ghci&amp;gt; sqrt (x + y) 6.8 +/- 0.2 ghci&amp;gt; logBase y x 0.78 +/- 0.02 ghci&amp;gt; log (x**y) 85.9 +/- 0.3 ~~~&lt;/p&gt;
&lt;p&gt;Along the way, we'll also learn how to harness the power of awesome &lt;a href="https://hackage.haskell.org/package/ad"&gt;ad&lt;/a&gt; library, a library used in implementing back-propagation and other optimization algorithms, to analyze numerical functions in a mathematical way and break down their derivatives and gradients.&lt;/p&gt;
&lt;p&gt;You can follow along with &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs"&gt;the source code&lt;/a&gt;, which is actually a &lt;em&gt;&lt;a href="http://www.haskellstack.org"&gt;stack&lt;/a&gt;&lt;/em&gt; executable! If you download the source and you have &lt;em&gt;&lt;a href="http://www.haskellstack.org"&gt;stack&lt;/a&gt;&lt;/em&gt; installed, you can run it (and run the tests above) as an executable:&lt;/p&gt;
&lt;p&gt;~~~bash $ ./Uncertain.hs ~~~&lt;/p&gt;
&lt;p&gt;Otherwise, you can run it directly with stack (using &lt;code&gt;runhaskell&lt;/code&gt;) and the &lt;a href="shttp://hackage.haskell.org/package/linear/docs/Linear-V2.html"&gt;linear&lt;/a&gt; and &lt;a href="https://hackage.haskell.org/package/ad"&gt;ad&lt;/a&gt; packages installed...or load it up with &lt;code&gt;stack ghci&lt;/code&gt; to play with it. If you want to be sure to reproduce the behavior, this article was written under &lt;a href="https://www.stackage.org/"&gt;stackage&lt;/a&gt; snapshot &lt;a href="https://www.stackage.org/lts-5.15"&gt;lts-5.15&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Dealing with Uncertainty Precisely&lt;/h2&gt;
&lt;p&gt;First of all, let's think about why adding two &amp;quot;uncertain&amp;quot; values doesn't involve simply adding the uncertainties linearly. (If you don't care about the math and just want to get on to the Haskell, feel free to skip this section!)&lt;/p&gt;
&lt;p&gt;If I have a value $16 \pm 3$ (maybe I have a ruler whose ticks are 3 units apart, or an instrument that produces measurements with 3 units of noise), it either means that it's a little below 16 or a little above 16. If I have an independently sampled value $25 \pm 4$, it means that it's a little below 25 or a little above 25.&lt;/p&gt;
&lt;p&gt;What happens if I want to think about their sum? Well, it's going to be somewhere around 41. But, the uncertainty won't be $\pm 7$. It would only be $\pm 7$ if the errors in the two values are &lt;em&gt;always aligned&lt;/em&gt;. Only if or when every &amp;quot;little bit above&amp;quot; 16 error lines up perfectly with a &amp;quot;little bit above&amp;quot; 25 error, and when every single &amp;quot;little bit below&amp;quot; 16 error lines up perfectly with a &amp;quot;little bit above&amp;quot; 25 error, would you really get something that is $\pm 7$. But, because the two values are sampled independently, you shouldn't expect such alignment. So, you'll get an uncertainty that's &lt;em&gt;less than&lt;/em&gt; $\pm 7$. In fact, it'll actually be around $\pm 5$.&lt;/p&gt;
&lt;p&gt;In general, we find that for &lt;em&gt;independent&lt;/em&gt; $X$ and $Y$:&lt;/p&gt;
&lt;p&gt;$$ \operatorname{Var}[aX + bY + c] = a^2 \sigma&lt;em&gt;X^2 + b^2 \sigma&lt;/em&gt;Y^2 $$&lt;/p&gt;
&lt;p&gt;Where $\sigma&lt;em&gt;X^2$ is the variance in $X$. We consider $\sigma&lt;/em&gt;X$ to be the standard deviation of $X$, or the &amp;quot;plus or minus&amp;quot; part of our numbers. In the simple case of addition, we have $\operatorname{Var}[X + Y] = \sigma&lt;em&gt;X^2 + \sigma&lt;/em&gt;Y^2$, so our new uncertainty[^var] is $\sqrt{\sigma&lt;em&gt;X^2 + \sigma&lt;/em&gt;Y^2}$.&lt;/p&gt;
&lt;p&gt;However, not all functions that combine $X$ and $Y$ can be expressed as simple linear combinations $aX + bY + c$. But! If you dig back to your days of high school calculus, you might remember a method for expressing any arbitrary function as a linear approximation -- the &lt;a href="https://en.wikipedia.org/wiki/Taylor_series"&gt;Taylor Expansion&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;In general, we can attempt to approximate any well-behaving function around a point as its tangent hyperplane:&lt;/p&gt;
&lt;p&gt;$$ f(x&lt;em&gt;0 + x, y&lt;/em&gt;0 + y) \approx f&lt;em&gt;x(x&lt;/em&gt;0, y&lt;em&gt;0) x + f&lt;/em&gt;y(x&lt;em&gt;0, y&lt;/em&gt;0) y + f(x&lt;em&gt;0, y&lt;/em&gt;0) $$&lt;/p&gt;
&lt;p&gt;Where $f&lt;em&gt;x(x&lt;/em&gt;0,y&lt;em&gt;0)$ is the first (partial) derivative with respect to $x$ at $(x&lt;/em&gt;0, y&lt;em&gt;0)$. This gives us an approximation of $f$ at locations close to $(x&lt;/em&gt;0, y_0)$.&lt;/p&gt;
&lt;p&gt;Look familiar? This is exactly the form that we used earlier to calculate &amp;quot;combined&amp;quot; variance! If we approximate the functions around $(\mu&lt;em&gt;X, \mu&lt;/em&gt;Y)$, the center/expected value of $X$ and $Y$, we see:&lt;/p&gt;
&lt;p&gt;$$ \operatorname{Var}[f(X,Y)] \approx f&lt;em&gt;x(\mu&lt;/em&gt;X, \mu&lt;em&gt;Y)^2 \sigma&lt;/em&gt;X^2 + f&lt;em&gt;y(\mu&lt;/em&gt;X,\mu&lt;em&gt;Y)^2 \sigma&lt;/em&gt;Y^2 $$&lt;/p&gt;
&lt;p&gt;A similar analysis can be used to figure out how the expected value changes by taking the taylor expansion to the &lt;em&gt;second&lt;/em&gt; degree:&lt;/p&gt;
&lt;p&gt;$$ \operatorname{E}[f(X,Y)] \approx f(\mu&lt;em&gt;X, \mu&lt;/em&gt;Y) + \frac{1}{2} \left[ f&lt;em&gt;{xx}(\mu&lt;/em&gt;X, \mu&lt;em&gt;Y) \sigma&lt;/em&gt;X^2 + f&lt;em&gt;{yy}(\mu&lt;/em&gt;X, \mu&lt;em&gt;Y) \sigma&lt;/em&gt;Y^2 \right] $$&lt;/p&gt;
&lt;p&gt;Where $f&lt;em&gt;{xx}(\mu&lt;/em&gt;X, \mu&lt;em&gt;Y)$ is the second (partial) derivative with respect to $x$ twice at $(\mu&lt;/em&gt;X, \mu_Y)$&lt;/p&gt;
&lt;p&gt;For our case of simple addition, $\operatorname{E}[X + Y] = \mu&lt;em&gt;X + \mu&lt;/em&gt;Y$, because the second-order partials of $f(x,y) = x + y$ are 0.&lt;/p&gt;
&lt;h2&gt;Uncertain Values in Haskell&lt;/h2&gt;
&lt;p&gt;So, how are we going to model our uncertain values in Haskell ... ? With an Algebraic Data Type, of course!&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L18-20 data Uncert a = Un { uMean :: !a , uVar :: !a } ~~~&lt;/p&gt;
&lt;p&gt;We'll keep track of the mean (the central point) and the &lt;em&gt;variance&lt;/em&gt;, which is the standard deviation &lt;em&gt;squared&lt;/em&gt;. We keep track of the variance and not the standard deviation (the &amp;quot;plus or minus&amp;quot;) because the mathematics is a bit more straightforward.&lt;/p&gt;
&lt;p&gt;We can write a function to turn a &amp;quot;plus or minus&amp;quot; statement into an &lt;code&gt;Uncert&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L22-23 (+/-) :: Num a =&amp;gt; a -&amp;gt; a -&amp;gt; Uncert a x +/- dx = Un x (dx*dx) ~~~&lt;/p&gt;
&lt;p&gt;Give the &lt;code&gt;dx&lt;/code&gt; (the standard deviation) and store &lt;code&gt;dx^2&lt;/code&gt;, the variance.&lt;/p&gt;
&lt;p&gt;Let's also throw in a handy helper function for &amp;quot;exact&amp;quot; values:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L25-26 exact :: Num a =&amp;gt; a -&amp;gt; Uncert a exact x = x +/- 0 ~~~&lt;/p&gt;
&lt;p&gt;But, we can do better (if just for fun). We can use pattern synonyms to basically &amp;quot;abstract&amp;quot; away the data type itself, and let people pattern match on a mean and standard deviation:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L29-34 -- pattern (:+/-) :: () =&amp;gt; Floating a =&amp;gt; a -&amp;gt; a -&amp;gt; Uncert a -- [GHC 8.0:] -- pattern (:+/-) :: Floating a =&amp;gt; a -&amp;gt; a -&amp;gt; Uncert a pattern x :+/- dx &amp;lt;- Un x (sqrt-&amp;gt;dx) where x :+/- dx = Un x (dx*dx) ~~~&lt;/p&gt;
&lt;p&gt;(Note that the type signature you need for this is different depending on if you're in GHC 8.0 and GHC 7.10; they're mutually incompatible. How unfortunate!)&lt;/p&gt;
&lt;p&gt;Now, people can pattern match on &lt;code&gt;x :+/- dx&lt;/code&gt; and receive the mean and uncertainty directly. Neat!&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L36-37 uStdev :: Floating a =&amp;gt; Uncert a -&amp;gt; a uStdev (_ :+/- dx) = dx ~~~&lt;/p&gt;
&lt;h3&gt;Making it Numeric&lt;/h3&gt;
&lt;p&gt;Now, time for the magic! Let's write a &lt;code&gt;Num&lt;/code&gt; instance!&lt;/p&gt;
&lt;p&gt;~~~haskell instance Num a =&amp;gt; Num (Uncert a) where fromIntegral = exact . fromIntegral Un x vx + Un y vy = Un (x + y) (vx + vy) Un x vx - Un y vy = Un (x - y) (vx + vy) Un x vx * Un y vy = Un (x * y) (y^2 * vx + x^2 * vy) negate (Un x vx) = Un (negate x) vx -- ... ~~~&lt;/p&gt;
&lt;p&gt;And...that's it! Do the same thing for every numeric typeclass, and you get automatic propagation of uncertainty.&lt;/p&gt;
&lt;h3&gt;The Problem&lt;/h3&gt;
&lt;p&gt;But, wait --- this method is definitely not ideal. It's pretty repetitive, and involves a but of copy-and-pasting code that is slightly different in ways the typechecker can't verify. What if we didn't change something we were supposed to? And, if you look at the &lt;code&gt;Fractional&lt;/code&gt; instance...&lt;/p&gt;
&lt;p&gt;~~~haskell instance Fractional a =&amp;gt; Fractional (Uncert a) where fromRational = exact . fromRational Un x vx / Un y vy = Un (x/y + x/y^3&lt;em&gt;vy) (x^2/y^4&lt;/em&gt;vx + vy/y^2) recip (Un x vx) = Un (recip x + vx/x^3) (vx / x^4) ~~~&lt;/p&gt;
&lt;p&gt;Yikes. All that ugly and complicated numerical code that the typechecker can't help us with (and, honestly, I'm not very confident in the results myself!). Those are runtime bugs just waiting to happen. How do we even &lt;em&gt;know&lt;/em&gt; that we calculated the right derivatives, and implemented the formula correctly?&lt;/p&gt;
&lt;p&gt;What if we could reduce this boilerplate? What if we could somehow analytically compute derivatives for functions instead of computing them manually?&lt;/p&gt;
&lt;h2&gt;Automatic Differentiation&lt;/h2&gt;
&lt;p&gt;Automatic differentiation is honestly one of the coolest Haskell tricks you can show that any beginner can immediately understand. Like our trick with &lt;code&gt;Uncert&lt;/code&gt;, it's nice to use because of its overloaded &lt;code&gt;Num&lt;/code&gt;/numeric typeclasses.&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; diff (\x -&amp;gt; x^2) 10 -- 2&lt;em&gt;x 20 ghci&amp;gt; diff (\x -&amp;gt; sin x) 0 -- cos x 1.0 ghci&amp;gt; diff (\x -&amp;gt; x^3 - 3&lt;/em&gt;x^2 + 2&lt;em&gt;x - 4) 3 -- 3&lt;/em&gt;x^2 - 6*x + 2 11 ~~~&lt;/p&gt;
&lt;p&gt;A very rough explanation about how forward-mode automatic differentiation works is that it uses a wrapper type (like ours) that defines &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;negate&lt;/code&gt;, etc. so that they also compute the &lt;em&gt;derivative(s)&lt;/em&gt; of the function, instead of just the &lt;em&gt;result&lt;/em&gt;, like normal. There are a lot of nice tutorials online, like &lt;a href="http://www.danielbrice.net/blog/10/"&gt;this one&lt;/a&gt; by Daniel Brice, if you want to follow up on this fun little subject.&lt;/p&gt;
&lt;h3&gt;Single-variable functions&lt;/h3&gt;
&lt;p&gt;And, now that we can automatically differentiate functions, we can use this knowledge directly in our implementations. Let's define a universal &amp;quot;lifter&amp;quot; of single-variable functions.&lt;/p&gt;
&lt;p&gt;We use the function &lt;code&gt;diffs0&lt;/code&gt; to get a &amp;quot;tower&amp;quot; of derivatives:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; diffs0 (\x -&amp;gt; x^2 - 2 x^3) 4 [-112, -88, -46, -12, 0, 0, 0, 0... ~~~&lt;/p&gt;
&lt;p&gt;The first value is actually $4^2 - 2 \times 4^3$. The second is the derivative ($2 x - 6x^2$) at 4, the third is the second derivative $2 - 12 x$ at 4, then the third derivative $-12$, then the fourth derivative $0$, etc.&lt;/p&gt;
&lt;p&gt;We only need the actual value and the first two derivatives, so we can pattern match them as &lt;code&gt;fx:dfx:ddfx:_ = diffs0 f x&lt;/code&gt;, the derivatives and values of the function we lift, &lt;code&gt;f&lt;/code&gt;, around the mean &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;At that point, the equations we have from before just translate nicely:&lt;/p&gt;
&lt;p&gt;$$ \operatorname{E}[f(X)] = f(\mu&lt;em&gt;X) + \frac{1}{2} f&lt;/em&gt;{xx}(\mu&lt;em&gt;X) \sigma&lt;/em&gt;X^2 $$&lt;/p&gt;
&lt;p&gt;$$ \operatorname{Var}[f(X)] = f&lt;em&gt;x(\mu&lt;/em&gt;X)^2 \sigma_X^2 $$&lt;/p&gt;
&lt;p&gt;And we call $\mu&lt;em&gt;X$ &lt;code&gt;x&lt;/code&gt; and $\sigma&lt;/em&gt;X^2$ &lt;code&gt;vx&lt;/code&gt;, and this becomes:&lt;/p&gt;
&lt;p&gt;~~~haskell y = fx + ddfx * vx / 2 vy = dfx^2 * vx ~~~&lt;/p&gt;
&lt;p&gt;Putting it all together:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L39-47 liftU :: Fractional a =&amp;gt; (forall s. AD s (Tower a) -&amp;gt; AD s (Tower a)) -&amp;gt; Uncert a -&amp;gt; Uncert a liftU f (Un x vx) = Un y vy where fx:dfx:ddfx:_ = diffs0 f x y = fx + ddfx * vx / 2 vy = dfx^2 * vx ~~~&lt;/p&gt;
&lt;p&gt;The type &lt;code&gt;forall s. AD s (Tower a) -&amp;gt; AD s (Tower a)&lt;/code&gt; looks a little scary, but you can think of it as representing a function on &lt;code&gt;a&lt;/code&gt; (like &lt;code&gt;negate&lt;/code&gt;, &lt;code&gt;(*2)&lt;/code&gt;, etc.) that the &lt;em&gt;ad&lt;/em&gt; library can differentiate several times --- something you could use with &lt;code&gt;diff0&lt;/code&gt; to get a &amp;quot;tower&amp;quot; of derivatives.&lt;/p&gt;
&lt;p&gt;And ... that's it! We can already define things like:&lt;/p&gt;
&lt;p&gt;~~~haskell negate = liftU negate recip = liftU recip sqrt = liftU sqrt sin = liftU sin ~~~&lt;/p&gt;
&lt;h3&gt;Multivariable functions&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;ad&lt;/em&gt; also lets you work multivariable functions, too. To model multivariable functions, it takes a function from a &lt;code&gt;Traversable&lt;/code&gt; of vales to a single value. We can use the &lt;code&gt;V2&lt;/code&gt; type from the &lt;em&gt;&lt;a href="shttp://hackage.haskell.org/package/linear/docs/Linear-V2.html"&gt;linear&lt;/a&gt;&lt;/em&gt; package to pass in a two-variable function:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; grad ((V2 x y) -&amp;gt; x * y^2 + 3*x) (V2 3 1) V2 4 6 ~~~&lt;/p&gt;
&lt;p&gt;The gradient of $f(x, y) = x y^2 + 3x$ is $(y^2 + 3, 2xy)$, which, at $(3, 1)$, is indeed $(4, 6)$.&lt;/p&gt;
&lt;p&gt;The gradient gives us the first order partials, but we need the second order partials to calculate the new mean, so for that, we can use &lt;code&gt;hessian&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; hessian ((V2 x y) -&amp;gt; x * y^2 + 3*x) (V2 3 1) V2 (V2 0 2) (V2 2 6) ~~~&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Hessian_matrix"&gt;hessian&lt;/a&gt; of a function $f(x,y)$ is basically a matrix of second-order partial derivatives:&lt;/p&gt;
&lt;p&gt;$$ \begin{bmatrix} f&lt;em&gt;{xx}(x, y) &amp;amp; f&lt;/em&gt;{yx}(x, y) \ f&lt;em&gt;{yx}(x, y) &amp;amp; f&lt;/em&gt;{yy}(x, y) \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;In our case, we only care about the diagonal -- the repeated double-derivatives, $f&lt;em&gt;{xx}$ and $f&lt;/em&gt;{yy}$. Indeed, the double-partial of our function respect to $x$ is $0$, and the double-partial with respect to $y$ is $2x$, which gives us a hessian with a diagonal $(0, 6)$ for the input $(3, 1)$.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;ad&lt;/em&gt; package generously gives us a function that lets us calculate the function's result, its gradient, and its hessian all in one pass:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; hessian' ((V2 x y) -&amp;gt; x * y^2 + 3*x) (V2 3 1) (12, V2 (4, V2 0 3) V2 (6, V2 2 6) ) ~~~&lt;/p&gt;
&lt;p&gt;We can access the gradient by using &lt;code&gt;fmap fst&lt;/code&gt; on the second component of the tuple and access the hessian by using &lt;code&gt;fmap snd&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We need a couple of helpers, first --- one to get the &amp;quot;diagonal&amp;quot; of our hessian, because we only care about the repeated partials:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L49-53 diag :: [[a]] -&amp;gt; [a] diag = \case [] -&amp;gt; [] [] :yss -&amp;gt; diag (drop 1 &amp;lt;$&amp;gt; yss) (x:_):yss -&amp;gt; x : diag (drop 1 &amp;lt;$&amp;gt; yss) ~~~&lt;/p&gt;
&lt;p&gt;And then a &amp;quot;dot product&amp;quot;, utility function, which just multiplies two lists together component-by-component and sums the results:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L55-56 dot :: Num a =&amp;gt; [a] -&amp;gt; [a] -&amp;gt; a dot xs ys = sum (zipWith (*) xs ys) ~~~&lt;/p&gt;
&lt;p&gt;And now we can write our multi-variate function lifter:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L58-75 liftUF :: (Traversable f, Fractional a) =&amp;gt; (forall s. f (AD s (Sparse a)) -&amp;gt; AD s (Sparse a)) -&amp;gt; f (Uncert a) -&amp;gt; Uncert a liftUF f us = Un y vy where xs = uMean &amp;lt;$&amp;gt; us vxs = toList (uVar &amp;lt;$&amp;gt; us) (fx, hgrad) = hessian' f xs dfxs = fst &amp;lt;$&amp;gt; hgrad hess = snd &amp;lt;$&amp;gt; hgrad y = fx + partials / 2 where partials = dot vxs . diag $ toList (fmap toList hess) -- from f (f a) to [[a]] vy = dot vxs $ toList ((^2) &amp;lt;$&amp;gt; dfxs) ~~~&lt;/p&gt;
&lt;p&gt;(Again, don't mind the scary type &lt;code&gt;forall s. f (AD s (Sparse a)) -&amp;gt; AD s (Sparse a)&lt;/code&gt;, it's just &lt;em&gt;ad&lt;/em&gt;'s type for things you can use &lt;code&gt;hessian'&lt;/code&gt; on)&lt;/p&gt;
&lt;p&gt;And we can write some nice helper functions so we can use them more naturally:&lt;/p&gt;
&lt;p&gt;~~~haskell -- source: https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs#L77-90 liftU2 :: Fractional a =&amp;gt; (forall s. AD s (Sparse a) -&amp;gt; AD s (Sparse a) -&amp;gt; AD s (Sparse a)) -&amp;gt; Uncert a -&amp;gt; Uncert a -&amp;gt; Uncert a liftU2 f x y = liftUF ((V2 x' y') -&amp;gt; f x' y') (V2 x y)&lt;/p&gt;
&lt;p&gt;liftU3 :: Fractional a =&amp;gt; (forall s. AD s (Sparse a) -&amp;gt; AD s (Sparse a) -&amp;gt; AD s (Sparse a) -&amp;gt; AD s (Sparse a)) -&amp;gt; Uncert a -&amp;gt; Uncert a -&amp;gt; Uncert a -&amp;gt; Uncert a liftU3 f x y z = liftUF ((V3 x' y' z') -&amp;gt; f x' y' z') (V3 x y z) ~~~&lt;/p&gt;
&lt;p&gt;At this point, our code is pretty much complete. We can fill in the other two-argument functions from the numeric typeclasses:&lt;/p&gt;
&lt;p&gt;~~~haskell (+) = liftU2 (+) (&lt;em&gt;) = liftU2 (&lt;/em&gt;) (/) = liftU2 (/) (&lt;strong&gt;) = liftU2 (&lt;/strong&gt;) logBase = liftU2 logBase ~~~&lt;/p&gt;
&lt;p&gt;Admittedly, there's still some slight boilerplate (that you can get rid of with some Template Haskell, maybe), but you have a &lt;em&gt;lot&lt;/em&gt; less room for error, and a lot simpler to check over and read to make sure you didn't miss any bugs.&lt;/p&gt;
&lt;h2&gt;Wrapping it up&lt;/h2&gt;
&lt;p&gt;The full code (with all of the numeric instances fully implemented) is up &lt;a href="https://github.com/mstksg/inCode/tree/master/code-samples/uncertain/Uncertain.hs"&gt;on github&lt;/a&gt;, which you can run and explore and test by executing it or loading it with &lt;code&gt;stack ghci&lt;/code&gt;. I've added a special &lt;em&gt;Show&lt;/em&gt; instance that &amp;quot;rounds&amp;quot; your values to as many digits that your uncertainty suggests, to give more meaningful &lt;code&gt;show&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;All of what's in this post is actually up on my &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/uncertain"&gt;uncertain&lt;/a&gt;&lt;/em&gt; package on hackage, if you want to use it in your own projects, or see how I take this and make it more robust for real-world applications. The project also has more features on top of the basic things shown here.&lt;/p&gt;
&lt;h3&gt;Verification and Accuracy&lt;/h3&gt;
&lt;p&gt;My &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/uncertain"&gt;uncertain&lt;/a&gt;&lt;/em&gt; package has a Monte Carlo module to propagate uncertainty through Monte Carlo simulations. Let's see how the values compare!&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; x + y -- Monte Carlo Results: 46 +/- 2 -- actually 46 +/- 2 ghci&amp;gt; x * y 450 +/- 40 -- actually 450 +/- 40 ghci&amp;gt; sqrt (x + y) 6.8 +/- 0.2 -- actually 6.8 +/- 0.2 ghci&amp;gt; logBase y x 0.78 +/- 0.02 -- actually 0.78 +/- 0.02 ghci&amp;gt; log (x**y) 85.9 +/- 0.3 -- actually 83 +/- 6 ~~~&lt;/p&gt;
&lt;p&gt;So, it looks like the mathematical model of uncertainty propagation matched up well with the &amp;quot;actual&amp;quot; results we gain from Monte Carlo simulations! The only one of our examples that was significantly wrong was the $\operatorname{log}(x^y)$ example, which heavily underestimated the uncertainty by about a factor of 20. But, remember, the model was derived after dropping the 2nd, 3rd, 4th, etc. terms of the taylor expansion for the calculation of the new uncertainty, and the 4th, 6th, etc. terms of the taylor expansion for the calculation of the new mean. For functions that have high second, third, fourth derivatives relative to the mean and the uncertainty, it's going to be a bit off.&lt;/p&gt;
&lt;h3&gt;What next?&lt;/h3&gt;
&lt;p&gt;For an extension on the mathematics behind this method, Dan Piponi has a &lt;a href="http://blog.sigfpe.com/2011/08/computing-errors-with-square-roots-of.html"&gt;great article&lt;/a&gt; with a lot of good references for further reading on the formal method.&lt;/p&gt;
&lt;p&gt;Going off of what we've done here, a simple extension of this would be to implement the Monte Carlo simulator I mentioned above, which is pretty straightforward to implement with the &lt;em&gt;&lt;a href="https://hackage.haskell.org/package/mwc-random"&gt;mwc-random&lt;/a&gt;&lt;/em&gt; package.&lt;/p&gt;
&lt;p&gt;However, the most unsettling thing here that we never deal with is what happens correlated terms that are combined. All of our math assumed uncorrelated samples. But what happens if we have expressions that involve additions of correlated values?&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;p&gt;~~~haskell ghci&amp;gt; let x = 14.6 +/- 0.8 in x + x 29 +/- 1 ghci&amp;gt; let x = 14.6 +/- 0.8 in 2*x 29 +/- 2 ~~~&lt;/p&gt;
&lt;p&gt;Unfortunately, &lt;code&gt;x + x&lt;/code&gt; is different than &lt;code&gt;2*x&lt;/code&gt;. This is because &lt;code&gt;x&lt;/code&gt; acts like an &lt;em&gt;independent generator&lt;/em&gt;, so when you say &lt;code&gt;x + x&lt;/code&gt;, it expands to &lt;code&gt;(14.6 +/- 0.8) + (14.6 +/- 0.8)&lt;/code&gt;, which represents the addition of two independent samples.&lt;/p&gt;
&lt;p&gt;When you say &lt;code&gt;2*x&lt;/code&gt;, that represents sampling &lt;code&gt;x&lt;/code&gt; &lt;em&gt;once&lt;/em&gt; and &lt;em&gt;doubling&lt;/em&gt; it. If you sample &lt;code&gt;x&lt;/code&gt; and double it, any error in &lt;code&gt;x&lt;/code&gt; will also be doubled. That's why the uncertainty is greater in the &lt;code&gt;2*x&lt;/code&gt; version.&lt;/p&gt;
&lt;p&gt;How can we account for correlated values that are combined in complex ways? Stay tuned for the next part of the &lt;a href="https://blog.jle.im/entries/series/+uncertain.html"&gt;series&lt;/a&gt;![^spoilers]&lt;/p&gt;</description><author>Justin Le</author><category>Haskell</category><category>Tutorials</category><guid isPermaLink="true">https://blog.jle.im/entry/automatic-propagation-of-uncertainty-with-ad.html</guid><pubDate>Mon,  9 May 2016 19:38:24 UTC</pubDate><creator>Justin Le</creator><subject>Haskell, Tutorials</subject><date>2016-05-09</date></item><item><title>Blog Rewrite with Hakyll and Purescript</title><link>https://blog.jle.im/entry/blog-rewrite-with-hakyll-and-purescript.html</link><description>&lt;p&gt;It's been almost a year since my last post! Things have been a bit hectic with research and related things, and with the unrelenting academia publishing cycle, any time I can get to write or explore has been a nice escape.&lt;/p&gt;
&lt;p&gt;Admittedly, I've also run into some friction updating my blog because it was a compiled web server with some delicate dependencies and required environment configuration to build/deploy. It was written/built at a time when a lot of the infrastructure we have now in the Haskell ecosystem either wasn't there, or wasn't mature. We didn't have easy &lt;a href="https://haskellonheroku.com/"&gt;Heroku deployment&lt;/a&gt;, and we didn't have great tools like &lt;a href="http://haskellstack.org/"&gt;stack&lt;/a&gt; to let us create reproducible builds. One of my &lt;a href="http://blog.jle.im/entry/deploying-medium-to-large-haskell-apps-to-heroku.html"&gt;first posts&lt;/a&gt; in 2013 was actually about hoops to jump through &lt;em&gt;just&lt;/em&gt; to get a simple Heroku deployment. I've had to maintain a virtual machine just to compile and push changes!&lt;/p&gt;
&lt;p&gt;My blog was one of my first Haskell projects ever, and if I had started it now, in 2016, things would definitely be a bit different. By this point, it's been long enough and the slight inconveniences have been building up enough that I thought it'd be time to sit down and finally migrate my &amp;quot;first large-ish Haskell project&amp;quot; and bring it into modern times, by using &lt;a href="https://jaspervdj.be/hakyll/"&gt;hakyll&lt;/a&gt; and &lt;a href="http://www.purescript.org/"&gt;purescript&lt;/a&gt;. Here are my thoughts and observations on how the migration went, with insight on Haskell migrations in general!&lt;/p&gt;
&lt;p&gt;My blog engine is open-source, and the &lt;a href="https://github.com/mstksg/inCode"&gt;source for this specific instance&lt;/a&gt; is up on github, for those interested in checking it out!&lt;/p&gt;
&lt;h2&gt;Hakyll&lt;/h2&gt;
&lt;p&gt;To be honest, there was little actual practical reasons why my site wasn't static to begin with. The main reason, feature-wise, was for me to be able to schedule blog posts and updates without requiring me to actually re-render and re-push every time I wanted to make a post. The real underlying reason, however, was that this blog was my first major Haskell project, and I wanted to take the opportunity to be able to learn how to interface with databases in Haskell.&lt;/p&gt;
&lt;p&gt;Now that that learning impetus is behind me, I felt free to throw it all out the window and rewrite things to be a completely 100% static site!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://jaspervdj.be/hakyll/"&gt;Hakyll&lt;/a&gt; was great; it's basically like a very specialized &lt;em&gt;make&lt;/em&gt;-like tool for building sites. It takes a bit of time to get used to &amp;quot;thinking in Hakyll&amp;quot; --- generating standalone pages instead of just ones based off of files, getting used to the identifier/snapshot system --- but once you do, things go pretty smoothly. I started thinking about snapshots as customized &amp;quot;object files&amp;quot; that you can leave behind in the process of creating pages that other pages can use. Hakyll manages all the dependencies for you, so pages that depend on the things left from other pages will be sequenced properly, and rebuilding your website only requires rebuilding pages that depend on files you changed. Neat!&lt;/p&gt;
&lt;p&gt;Before, I had gotten the impression that Hakyll was mostly for generating &amp;quot;simple&amp;quot;, pre-built blog layouts, but I was able to use Hakyll (without much friction, at all) to generate the complex, intricate, and arbitrary site map that I had designed for my &lt;a href="http://hackage.haskell.org/package/scotty"&gt;scotty&lt;/a&gt;-based blog. I definitely recommend it for any static site generating needs, blogs or not.&lt;/p&gt;
&lt;p&gt;An unexpected consequence of the static-site-hosted-by-github-pages approach, however, is that I don't have any control over MIME types anymore (or 301 redirects), so I had to do some migrations to move pages over to &amp;quot;.html&amp;quot; and set up redirects and stuff (and get redirects to work with google analytics), but those were made super simple with Hakyll.&lt;/p&gt;
&lt;h2&gt;Refactoring Haskell Code&lt;/h2&gt;
&lt;p&gt;One thing that did not disappoint me was how &lt;em&gt;easy&lt;/em&gt; and &lt;em&gt;painless&lt;/em&gt; it is to refactor Haskell code. This is something I always trumpet/brag about Haskell, and getting the opportunity to actually refactor a major-ish codebase.&lt;/p&gt;
&lt;p&gt;And, yes, I was not disappointed! For the most part, I already had my html templates, CSS, static javascript, etc. in place. All of the mechanisms were extremely modular and very easy to port. The type system made sure everything fit together well at the boundaries. They also instantly told me what did what, and ensured that sweeping changes in my code were safe. The &amp;quot;if it compiles, it works&amp;quot; mantra served me greatly here. I can't even begin to imagine migrating one of my old ruby projects in the same way. With this, I was confident that my compiled code was correct and did what I wanted. The types were a guide and also a avenue of insight into my 3-years-removed past self.&lt;/p&gt;
&lt;p&gt;Thanks to the types, I was able to pick up something I hadn't touched in 3 years, figure out how all things fit together, and completely gut everything apart and use them for a new build system ... with compile-time assurances that I didn't do anything incorrectly!&lt;/p&gt;
&lt;p&gt;It's hard for me to really explain how amazing the feeling of refactoring Haskell code is. I used to dread refactors and migrations, but now I look forward to them and find any opportunity to do one! :D It's something that's difficult to convey the sublime joy of until you actually try it, so I recommend trying it some day :)&lt;/p&gt;
&lt;h2&gt;Purescript&lt;/h2&gt;
&lt;h3&gt;on Fay&lt;/h3&gt;
&lt;p&gt;With my &lt;a href="http://blog.jle.im/entry/blog-engine-updates-markdown-preprocessor-fay-scripts.html#fay"&gt;last major blog update&lt;/a&gt;, I ported all of my one-off javascript scripts to fay. This time around, I figured I'd move away from &lt;a href="https://github.com/faylang/fay/wiki"&gt;fay&lt;/a&gt;, because it was slightly clunky to build/get working/integrate in the way that GHCJS spoiled me to be accustomed to. In the future, I might return ... but at this point in time, Fay seems a bit awkward in the ecosystem. GHCJS lets you use the full power of Haskell (including all of &lt;em&gt;base&lt;/em&gt;'s concurrency mechanisms and almost every library on hackage), at the expense of creating large and unreadable javascript blobs.&lt;/p&gt;
&lt;p&gt;Fay seemed like just a &lt;em&gt;weaker&lt;/em&gt; GHCJS to me, but in all the ways that mattered. It doesn't have all of the awesome GHC things that make modern Haskell what it is (not just the lack of base's identical API, but also ... no typeclasses? Lens abstractions? Hackage libraries?), so almost all of my normal Haskell programming flow is thrown out the window. It's a subset of Haskell, but lacks most of the tools people use to write &lt;em&gt;actual&lt;/em&gt; Haskell like they'd write everyday. The generated javascript blobs are still decently opaque.&lt;/p&gt;
&lt;p&gt;So, if you're going to be spending your time writing something that is like Haskell, but forces you to write it in a way that is nothing like any actual Haskell code you'd normally write... why even bother keeping up with Haskell semantics and Haskell compatibility? Why not break out and try something new and fresh, unbound by Haskell and compatibility issues?[^bash][^haste]&lt;/p&gt;
&lt;h3&gt;on Purescript&lt;/h3&gt;
&lt;p&gt;With that mindset, I looked at &lt;em&gt;&lt;a href="http://www.purescript.org/"&gt;purescript&lt;/a&gt;&lt;/em&gt;, which is a language that's inspired by Haskell, with a lot of Haskell features we use every day, and throws in things we all wish we had in Haskell, like extensible records!&lt;/p&gt;
&lt;p&gt;(Note --- I &lt;em&gt;did&lt;/em&gt; rewrite all of my fay in GHCJS at first. This resulted in a javascript blob that was &lt;em&gt;1.4 MB&lt;/em&gt; in size for just a bunch of small DOM manipulation scripts. Definitely not practical, unfortunately!)&lt;/p&gt;
&lt;p&gt;I liked that purescript was able to throw away a lot of warts in the Haskell ecosystem, with a cleaner typeclass hierarchy and just a lot of design decisions &amp;quot;done right&amp;quot;, that we'd all change in Haskell if we could. And extensible records being built into the language is quite refreshing; not having to deal with fancy GADT's in Haskell was a nice step back from the craziness that is type-level programming in Haskell. Alongside all of that, I was also able to rely and seamlessly use a lot of Haskell idioms that we all know and love, like lenses and traversals and compositions.&lt;/p&gt;
&lt;p&gt;At many moments, I felt like writing in Purescript felt like writing in &lt;em&gt;the language that Haskell should have been&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But one of my favorite aspects about purescript ended up being the sheer beauty and conciseness of the generated javascript. Look at how[^pshighlight]:&lt;/p&gt;
&lt;p&gt;~~~purescript appendTopLinks doc = do hs &amp;lt;- querySelectorAll headers (documentToParentNode doc) flip traverseNodeList_ hs \h -&amp;gt; do topLink &amp;lt;- createElement &amp;quot;a&amp;quot; doc let topLinkNode = elementToNode topLink setAttribute &amp;quot;href&amp;quot; &amp;quot;#title&amp;quot; topLink setClassName &amp;quot;top-link&amp;quot; topLink setTextContent &amp;quot;top&amp;quot; topLinkNode appendChild topLinkNode (elementToNode h) return unit ~~~&lt;/p&gt;
&lt;p&gt;gets translated to:&lt;/p&gt;
&lt;p&gt;~~~javascript var appendTopLinks = function (doc) { return function __do() { var v = querySelectorAll(headers)(documentToParentNode(doc))(); return flip(traverseNodeList_(monadEffEff))(v)(function (h) { return function __do() { var v1 = createElement(&amp;quot;a&amp;quot;)(doc)(); var topLinkNode = elementToNode(v1); setAttribute(&amp;quot;href&amp;quot;)(&amp;quot;#title&amp;quot;)(v1)(); setClassName(&amp;quot;top-link&amp;quot;)(v1)(); setTextContent(&amp;quot;top&amp;quot;)(topLinkNode)(); appendChild(topLinkNode)(elementToNode(h))(); return unit; }; })(); }; }; ~~~&lt;/p&gt;
&lt;p&gt;And it's not just the IO-based imperative code that looks nice, either. Everything gets compiled to clean, readable javascript that you'd be happy to import in your node/normal javascript project.&lt;/p&gt;
&lt;p&gt;The total exported javascript blob is only &lt;em&gt;88 kB&lt;/em&gt;, even smaller than fay's &lt;em&gt;100 kB&lt;/em&gt; output (but not significantly so), and much smaller than GHCJS's &lt;em&gt;1.4 MB&lt;/em&gt;[^140MB] output (which, to be fair, has to also contain the entire Haskell runtime, implementing Haskell semantics, as well).&lt;/p&gt;
&lt;p&gt;Interestingly enough, the &lt;em&gt;original&lt;/em&gt; raw javacript I wrote in 2013 came out to about the same size, about &lt;em&gt;80 kB&lt;/em&gt;. (Well, it is about &lt;em&gt;2 kB&lt;/em&gt; of actual script, but it utilized all of &lt;em&gt;jquery&lt;/em&gt;, which implements a lot of the functionality.) Getting comparable filesizes to jquery bundles is something that's pretty impressive to me!&lt;/p&gt;
&lt;p&gt;I'd recommend purescript to anyone who has to write simple javascript &lt;em&gt;scripts&lt;/em&gt; and wants to do it in a sane, beautiful language. I still use &lt;em&gt;ghcjs&lt;/em&gt; for actual &lt;em&gt;applications&lt;/em&gt;, for now, because I still love Haskell and its ecosystem, along with the free data type sharing and code re-usage. But for small scripts like these, purescript might just be the ideal and perfect solution!&lt;/p&gt;
&lt;p&gt;You can check out &lt;a href="https://github.com/mstksg/inCode/blob/28f6a5da4c83356c4be87067ab88171879c68784/app-purescript/Entry.purs"&gt;the actual purescript script&lt;/a&gt; on github!&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;My main takeways ---&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I will never be able to never work on a Haskell project/application without &lt;em&gt;stack&lt;/em&gt; again (how did we even survive before &lt;em&gt;stack&lt;/em&gt;?)&lt;/li&gt;
&lt;li&gt;Hakyll is a fun little library that is a great specialized &lt;em&gt;make&lt;/em&gt; for building static websites&lt;/li&gt;
&lt;li&gt;Refactoring Haskell is an amazing experience; I would recommend it to anyone to try it out at least once in their lives&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Purescript&lt;/em&gt; is an amazing and beautiful technology that I had the pleasure of learning during this process, and generates elegant, readable javascript scripts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This reflection post has been to help me organize my thoughts, but I hope they can be useful for those of you looking for new technologies to learn and ways to implement/approach your stack or next programming project, as well!&lt;/p&gt;</description><author>Justin Le</author><category>Meta</category><guid isPermaLink="true">https://blog.jle.im/entry/blog-rewrite-with-hakyll-and-purescript.html</guid><pubDate>Fri, 25 Mar 2016 16:59:18 UTC</pubDate><creator>Justin Le</creator><subject>Meta</subject><date>2016-03-25</date></item><language>en</language><copyright>Copyright 2016 Justin Le</copyright><managingEditor>justin@jle.im (Justin Le)</managingEditor><webMaster>justin@jle.im (Justin Le)</webMaster><lastBuildDate>Sun, 14 Jan 2018 07:09:19 UTC</lastBuildDate><generator>feed-1.0.0.0 (Sigbjorn Finne)</generator><image><url>https://blog.jle.im/img/site_logo.jpg</url><title>in Code</title><link>https://blog.jle.im/</link></image><creator>Justin Le</creator><language>en</language><rights>Copyright 2016 Justin Le</rights><date>2018-01-14</date><description>Weblog of Justin Le, covering his various adventures in programming and explorations in the vast worlds of computation physics, and knowledge.</description></channel></rss>