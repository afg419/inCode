<!DOCTYPE HTML>
<html><head><title>A Purely Functional Typed Approach to Trainable Models · in Code</title><meta name="description" content="Weblog of Justin Le, covering his various adventures in programming and explorations in the vast worlds of computation physics, and knowledge."><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><meta property="og:site_name" content="in Code"><meta property="og:description" content="With the release of backprop, I’ve been exploring the space of parameterized models of all sorts, from linear and logistic regression and other statistical models to artificial neural networks, feed-forward and recurrent (stateful). I wanted to see to what extent we can really apply automatic differentiation and iterative gradient decent-based training to all of these different models. I’m starting to see a picture unifying all of these models, painted in the language of purely typed functional programming. I’m already applying these to models I’m using in real life and in my research, and I thought I’d take some time to put my thoughts to writing in case anyone else finds these illuminating or useful. As a big picture, I really believe that a purely functional typed approach is the way to move forward in the future for models like artificial neural networks – and that one day, object-oriented and imperative approaches will seem quaint. I’m not the first person to attempt to build a conceptual framework for these types of models in a purely functional typed sense – Christopher Olah’s famous post comes to mind, and is definitely worth a read. However, Olah’s post is more of an abstract piece; the approach I am describing here can be applied today, to start building and discovering effective models and training them. And I have code! :) The code in this post is written in Haskell, using the backprop, hmatrix (with hmatrix-backprop), and vector-sized libraries."><meta property="og:type" content="article"><meta property="og:title" content="A Purely Functional Typed Approach to Trainable Models"><meta property="og:image" content="https://blog.jle.im/img/site_logo.jpg"><meta property="og:locale" content="en_US"><meta property="og:url" content="https://blog.jle.im/entry/purely-functional-typed-models.html"><meta name="twitter:card" content="summary"><meta name="twitter:creator:id" content="mstk"><link rel="author" href="https://plus.google.com/107705320197444500140"><link rel="alternate" type="application/rss+xml" title="in Code (RSS Feed)" href="http://feeds.feedburner.com/incodeblog"><link rel="canonical" href="https://blog.jle.im/entry/purely-functional-typed-models.html"><link href="https://blog.jle.im/favicon.ico" rel="shortcut icon"><link href="https://blog.jle.im/css/toast.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/font.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/main.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/page/entry.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/pygments.css" rel="stylesheet" type="text/css"><script type="text/javascript">var page_data = {};
var disqus_shortname='incode';
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-443711-8', 'jle.im');
ga('send', 'pageview');
</script><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5234d67a6b68dcd4"></script><script type="text/javascript" src="https://blog.jle.im/js/page/entry_toc.js"></script><script type="text/javascript" src="https://blog.jle.im/js/disqus_count.js"></script><script type="text/javascript" src="https://blog.jle.im/js/social.js"></script><script type="text/javascript" src="https://blog.jle.im/js/jquery/jquery.toc.js"></script><script type="text/javascript" src="https://blog.jle.im/purescript/entry.js"></script></head><body><div id="fb-root"><script>(function(d, s, id) {
 var js, fjs = d.getElementsByTagName(s)[0];
 if (d.getElementById(id)) return;
 js = d.createElement(s); js.id = id;
 js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=641852699171929";
 fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script></div><div id="header-container"><div id="navbar-container" class="tile"><nav id="navbar-content"><div class="nav-info"><h1 class="site-title"><a href="https://blog.jle.im/" class="nav-title">in Code</a></h1><span class="nav-author">Justin Le</span></div><ul class="nav-links"><li><a href="https://blog.jle.im/">home</a></li><li><a href="https://blog.jle.im/entries.html">archives</a></li><div class="clear"></div></ul></nav></div><div id="header-content"></div></div><div id="body-container" class="container"><div id="main-container" class="grid"><div class="entry-section unit span-grid" role="main"><article class="tile article"><header><div class="unposted-banner">Unposted entry</div><h1 id="title">A Purely Functional Typed Approach to Trainable Models</h1><p class="entry-info">by <a class="author" href="https://blog.jle.im/">Justin Le</a></p><p><span class="source-info"><a class="source-link" href="https://github.com/mstksg/inCode/tree/master/copy/entries/functional-models.md">Source</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://github.com/mstksg/inCode/tree/gh-pages/entry/purely-functional-typed-models.md">Markdown</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://blog.jle.im/entry/purely-functional-typed-models.tex">LaTeX</a><span class="info-separator"> &diams; </span></span>Posted in <a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category" title="Functional, pure, non-strict, statically and strongly typed, natively
compiled...really just the king of great languages.">Haskell</a><span class="info-separator"> &diams; </span><a class="comment-link" href="#disqus_thread">Comments</a></p></header><hr><aside class="contents-container"><h5 id="contents-header">Contents</h5><div id="toc"></div></aside><div class="main-content copy-content"><p>With the release of <a href="http://hackage.haskell.org/package/backprop">backprop</a>, I’ve been exploring the space of parameterized models of all sorts, from linear and logistic regression and other statistical models to artificial neural networks, feed-forward and recurrent (stateful). I wanted to see to what extent we can really apply automatic differentiation and iterative gradient decent-based training to all of these different models.</p>
<p>I’m starting to see a picture unifying all of these models, painted in the language of purely typed functional programming. I’m already applying these to models I’m using in real life and in my research, and I thought I’d take some time to put my thoughts to writing in case anyone else finds these illuminating or useful.</p>
<p>As a big picture, I really believe that a purely functional typed approach is <em>the</em> way to move forward in the future for models like artificial neural networks – and that one day, object-oriented and imperative approaches will seem quaint.</p>
<p>I’m not the first person to attempt to build a conceptual framework for these types of models in a purely functional typed sense – <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">Christopher Olah’s famous post</a> comes to mind, and is definitely worth a read. However, Olah’s post is more of an abstract piece; the approach I am describing here can be applied <em>today</em>, to start building and <em>discovering</em> effective models and training them. And I have code! :)</p>
<p>The code in this post is written in Haskell, using the <a href="http://hackage.haskell.org/package/backprop">backprop</a>, <a href="http://hackage.haskell.org/package/hmatrix">hmatrix</a> (with <a href="http://hackage.haskell.org/package/hmatrix-backprop">hmatrix-backprop</a>), and <a href="http://hackage.haskell.org/package/vector-sized">vector-sized</a> libraries.</p>
<h2 id="essence-of-a-model">Essence of a Model</h2>
<p>For the purpose of this post, a <em>parameterized model</em> is a function from some input “question” (predictor, independent variable) to some output “answer” (predictand, dependent variable)</p>
<p>Notationally, we might write it as a function:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af_p%28x%29%20%3D%20y%0A" alt="
f_p(x) = y
" title="
f_p(x) = y
" /><br /></p>
<p>The important thing is that, for every choice of <em>parameterization</em> <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" />, we get a <em>different function</em> <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29" alt="f_p(x)" title="f_p(x)" />.</p>
<p>For example, you might want to write a model that, when given an email, outputs whether or not that email is spam.</p>
<p>The parameterization <em>p</em> is some piece of data that we tweak to produce a different <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29" alt="f_p(x)" title="f_p(x)" />. So, “training” (or “learning”, or “estimating”) a model is a process of picking the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> that gives the “correct” function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29" alt="f_p(x)" title="f_p(x)" /> — that is, the function that accurately predicts spam or whatever thing you are trying to predict.</p>
<p>For example, for linear regression, you are trying to “fit” your <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%28x%2C%20y%29" alt="(x, y)" title="(x, y)" /> data points to some function <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%28x%29%20%3D%20%5Cbeta%20%2B%20%5Calpha%20x" alt="f(x) = \beta + \alpha x" title="f(x) = \beta + \alpha x" />. The <em>parameters</em> are <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Calpha" alt="\alpha" title="\alpha" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta" alt="\beta" title="\beta" />, the <em>input</em> is <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x" alt="x" title="x" />, and the <em>output</em> is <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta%20%2B%20%5Calpha%20x" alt="\beta + \alpha x" title="\beta + \alpha x" />.</p>
<p>As it so happens, a <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29" alt="f_p(x)" title="f_p(x)" /> is really just a “partially applied” <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%28p%2Cx%29" alt="f(p,x)" title="f(p,x)" />. Imagining that function, it has type:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af%20%3A%20P%20%5Ccross%20A%20%5Crightarrow%20B%0A" alt="
f : P \cross A \rightarrow B
" title="
f : P \cross A \rightarrow B
" /><br /></p>
<p>If we <a href="https://en.wikipedia.org/wiki/Currying">curry</a> this, we get the original model representation we talked about:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af%20%3A%20P%20%5Crightarrow%20%28A%20%5Crightarrow%20B%29%0A" alt="
f : P \rightarrow (A \rightarrow B)
" title="
f : P \rightarrow (A \rightarrow B)
" /><br /></p>
<h3 id="optimizing-models-with-observations">Optimizing Models with Observations</h3>
<p>Something interesting happens if we flip the script. What if, instead of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29" alt="f_p(x)" title="f_p(x)" />, we talked about <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_x%28p%29" alt="f_x(p)" title="f_x(p)" />? That is, we fix the input and vary the parameter, and see what type of outputs we get for the same output while we vary the parameter?</p>
<p>If we have an “expected output” for our input, then one thing we can do is look at <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29" alt="f_p(x)" title="f_p(x)" /> and see when the result is close to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?y_x" alt="y_x" title="y_x" /> (the expected output of our model when given <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x" alt="x" title="x" />).</p>
<p>In fact, we can turn this into an optimization problem by trying to pick <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> that minimizes the difference between <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_x%28p%29" alt="f_x(p)" title="f_x(p)" /> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?y_x" alt="y_x" title="y_x" />. We can say that our model with parameter <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> predicts <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?y_x" alt="y_x" title="y_x" /> the best when we minimize:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%28f_x%28p%29%20-%20y_x%29%5E2%0A" alt="
(f_x(p) - y_x)^2
" title="
(f_x(p) - y_x)^2
" /><br /></p>
<p>If we minimize the squared error between the result of picking the parameter and the expected result, we find the best parameters for that given input!</p>
<p>In general, picking the best parameter for the model involves picking the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> that minimizes the relationship</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bloss%7D%28y_x%2C%20f_x%28p%29%29%0A" alt="
\text{loss}(y_x, f_x(p))
" title="
\text{loss}(y_x, f_x(p))
" /><br /></p>
<p>Where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctext%7Bloss%7D%20%3A%20B%20%5Ctimes%20B%20%5Crightarrow%20%5Cmathbb%7BR%7D" alt="\text{loss} : B \times B \rightarrow \mathbb{R}" title="\text{loss} : B \times B \rightarrow \mathbb{R}" /> gives a measure of “how badly” the model result differs from the expected target. Common loss functions include squared error, cross-entropy, etc.</p>
<p>This gives us a supervised way to train any model: if we have enough observations (<img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%28x%2C%20y_x%29" alt="(x, y_x)" title="(x, y_x)" /> pairs) we can just pick a <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> that does its best to make the loss between all observations as small as possible.</p>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>If our model is a <em>differentiable function</em>, then we have a nice tool we can use: <em>stochastic gradient descent</em> (SGD).</p>
<p>That is, we can always calculate the <em>gradient</em> of the loss function with respect to our parameters. This gives us the direction we can “nudge” our parameters to make the loss bigger or smaller.</p>
<p>That is, if we get the gradient of the loss with respect to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" />:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_p%20%5Ctext%7Bloss%7D%28f_x%28p%29%2C%20y_x%29%0A" alt="
\nabla_p \text{loss}(f_x(p), y_x)
" title="
\nabla_p \text{loss}(f_x(p), y_x)
" /><br /></p>
<p>We now have a nice way to “train” our model:</p>
<ol type="1">
<li>Start with an initial guess at the parameter</li>
<li>Look at a random <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%28x%2C%20y_x%29" alt="(x, y_x)" title="(x, y_x)" /> observation pair.</li>
<li>Compute the gradient <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cnabla_p%20%5Ctext%7Bloss%7D%28f_x%28p%29%2C%20y_x%29" alt="\nabla_p \text{loss}(f_x(p), y_x)" title="\nabla_p \text{loss}(f_x(p), y_x)" /> of our current <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" />, which tells us a direction we can “nudge” <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> in to make the loss smaller.</li>
<li>Nudge <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> in that direction</li>
<li>Pick a new <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%28x%2C%20y_x%29" alt="(x, y_x)" title="(x, y_x)" /> observation pair.</li>
</ol>
<p>With every new observation, we see how we can nudge the parameter to make the model more accurate, and then we perform that nudge.</p>
<h2 id="functional-implementation">Functional Implementation</h2>
<p>This naturally lends itself well to a functional implementation. That’s because, in this light, a model is nothing more than a function. And a model that is trainable using SGD is simply a differentiable function.</p>
<p>Using the <em><a href="http://hackage.haskell.org/package/backprop">backprop</a></em> library, we can easily write functions to be differentiable.</p>
<p>Let’s write the type of our models. A model from type <code>a</code> to type <code>b</code> with parameter <code>p</code> can be written as</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">type</span> <span class="dt">Model</span> p a b <span class="fu">=</span> p <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> b</a></code></pre></div>
<p>Not normally differentiable, but we can make it a differentiable function by having it work with <code>BVar s p</code> and <code>BVar s a</code> (<code>BVar</code>s containing those values) instead:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L41-L42</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw">type</span> <span class="dt">Model</span> p a b <span class="fu">=</span> forall s<span class="fu">.</span> <span class="dt">Reifies</span> s <span class="dt">W</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">                 <span class="ot">=&gt;</span> <span class="dt">BVar</span> s p <span class="ot">-&gt;</span> <span class="dt">BVar</span> s a <span class="ot">-&gt;</span> <span class="dt">BVar</span> s b</a></code></pre></div>
<p>We can write a simple linear regression model:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af_%7B%5Calpha%2C%20%5Cbeta%7D%28x%29%20%3D%20%5Cbeta%20x%20%2B%20%5Calpha%0A" alt="
f_{\alpha, \beta}(x) = \beta x + \alpha
" title="
f_{\alpha, \beta}(x) = \beta x + \alpha
" /><br /></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L44-L48</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"></a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="ot">linReg ::</span> <span class="dt">Model</span> (<span class="dt">T2</span> <span class="dt">Double</span> <span class="dt">Double</span>) <span class="dt">Double</span> <span class="dt">Double</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">linReg ab x <span class="fu">=</span> b <span class="fu">*</span> x <span class="fu">+</span> a</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">    a <span class="fu">=</span> ab <span class="fu">^^.</span> _1</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    b <span class="fu">=</span> ab <span class="fu">^^.</span> _2</a></code></pre></div>
<p>Here <code>T2 Double Double</code> is a tuple of two <code>Double</code>s, which contains the parameters (<code>a</code> and <code>b</code>). We extract the first item using <code>^^. _1</code> and the second item with <code>^^. _2</code>, and then talk about the function</p>
<p>We can <em>run</em> <code>linReg</code> using <code>evalBP2</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" data-line-number="1">ghci<span class="fu">&gt;</span> evalBP2 linReg (<span class="dt">T2</span> <span class="fl">0.3</span> <span class="fu">-</span><span class="fl">0.1</span>) <span class="dv">5</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="fu">-</span><span class="fl">0.2</span>        <span class="co">-- (-0.1) * 5 + 0.3</span></a></code></pre></div>
<p>But the neat thing is that we can also get the gradient of the parameters, too, if we identify a loss function:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_p%20%28f_x%28p%29%20-%20y_x%29%5E2%0A" alt="
\nabla_p (f_x(p) - y_x)^2
" title="
\nabla_p (f_x(p) - y_x)^2
" /><br /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L50-L58</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">squaredErrorGrad</a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">Num</span> p, <span class="dt">Num</span> b)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">Model</span> p a b      <span class="co">-- ^ Model</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">    <span class="ot">-&gt;</span> a                <span class="co">-- ^ Observed input</span></a>
<a class="sourceLine" id="cb5-7" data-line-number="7">    <span class="ot">-&gt;</span> b                <span class="co">-- ^ Observed output</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8">    <span class="ot">-&gt;</span> p                <span class="co">-- ^ Parameter guess</span></a>
<a class="sourceLine" id="cb5-9" data-line-number="9">    <span class="ot">-&gt;</span> p                <span class="co">-- ^ Gradient</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10">squaredErrorGrad f x targ <span class="fu">=</span> gradBP <span class="fu">$</span> \p <span class="ot">-&gt;</span></a>
<a class="sourceLine" id="cb5-11" data-line-number="11">    (f p (constVar x) <span class="fu">-</span> constVar targ) <span class="fu">^</span> <span class="dv">2</span></a></code></pre></div>
<p>We use <code>constVar :: a -&gt; BVar s a</code>, to lift a normal value to a <code>BVar</code> holding that value, since our model <code>f</code> takes <code>BVar</code>s.</p>
<p>And finally, we can train it using stochastic gradient descent, with just a simple fold over all observations:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L60-L66</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">trainModel</a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">Fractional</span> p, <span class="dt">Num</span> b)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">Model</span> p a b      <span class="co">-- ^ model to train</span></a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    <span class="ot">-&gt;</span> p                <span class="co">-- ^ initial parameter guess</span></a>
<a class="sourceLine" id="cb6-7" data-line-number="7">    <span class="ot">-&gt;</span> [(a,b)]          <span class="co">-- ^ list of observations</span></a>
<a class="sourceLine" id="cb6-8" data-line-number="8">    <span class="ot">-&gt;</span> p                <span class="co">-- ^ updated parameter guess</span></a>
<a class="sourceLine" id="cb6-9" data-line-number="9">trainModel f <span class="fu">=</span> foldl&#39; <span class="fu">$</span> \p (x,y) <span class="ot">-&gt;</span> p <span class="fu">-</span> <span class="fl">0.1</span> <span class="fu">*</span> squaredErrorGrad f x y p</a></code></pre></div>
<p>Let’s train our linear regression model to fit the points <code>(1,1)</code>, <code>(2,3)</code>, <code>(3,5)</code>, <code>(4,7)</code>, and <code>(5,9)</code>! This should follow <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%28x%29%20%3D%202%20x%20-%201" alt="f(x) = 2 x - 1" title="f(x) = 2 x - 1" />, or <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Calpha%20%3D%20-1%2C%5C%2C%20%5Cbeta%20%3D%202" alt="\alpha = -1,\, \beta = 2" title="\alpha = -1,\, \beta = 2" />:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" data-line-number="1">ghci<span class="fu">&gt;</span> samps <span class="fu">=</span> [(<span class="dv">1</span>,<span class="dv">1</span>),(<span class="dv">2</span>,<span class="dv">3</span>),(<span class="dv">3</span>,<span class="dv">5</span>),(<span class="dv">4</span>,<span class="dv">7</span>),(<span class="dv">5</span>,<span class="dv">9</span>)]</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">ghci<span class="fu">&gt;</span> trainModel linReg (<span class="dt">T2</span> <span class="dv">0</span> <span class="dv">0</span>) (concat (replicate <span class="dv">1000</span> samps))</a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="dt">T2</span> (<span class="fu">-</span><span class="fl">1.0000000000000024</span>) <span class="fl">2.0000000000000036</span></a></code></pre></div>
<p>Neat! After going through all of those observations a thousand times, the model nudges itself all the way to the right parameters to fit our model!</p></div><footer><ul class="entry-series"></ul><ul class="tag-list"><li><a href="https://blog.jle.im/entries/tagged/machine-learning.html" class="tag-a-tag">#machine learning</a></li><li><a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category">@HASKELL</a></li></ul><aside class="social-buttons"><div class="addthis_toolbox addthis_default_style addthis-buttons"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a><a class="addthis_button_tweet"></a><a class="addthis_button_google_plusone" g:plusone:size="medium"></a><a class="addthis_counter addthis_pill_style"></a></div><div class="custom-social-buttons"><div class="custom-social-button"><a href="https://www.reddit.com/submit" onclick="window.location = &#39;https://www.reddit.com/submit?url=&#39;+ encodeURIComponent(window.location); return false"><img src="https://www.reddit.com/static/spreddit7.gif" alt="submit to reddit"></a></div></div></aside><nav class="next-prev-links"><ul><li class="next-entry-link">(Next) <a href="https://blog.jle.im/entry/introducing-in-code.html">Introducing &quot;in Code&quot;!</a> &rarr;</li></ul></nav></footer></article><div class="post-entry"><div class="tile"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blog.jle.im/entry/purely-functional-typed-models.html';
    this.page.identifier = 'functional-models';
};
(function() {
    var d = document, s = d.createElement('script');
    s.src = '//incode.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a><br></noscript><a href="http://disqus.com" class="dsq-brlink">Comments powered by <span class="logo-disqus">Disqus</span></a></div></div></div></div></div><div id="footer-container"><div id="footer-content"><div class="tile"><div class="footer-copyright">&copy; 2018 Justin Le</div><div class="footer-follow social-follows"><ul class="social-follows-list"><li><ul class="social-follows-list-social"><li><a class="social-follow-twitter" title="Follow me on Twitter!" href="https://twitter.com/intent/user?user_id=mstk" onclick="window.open(
  &#39;http://twitter.com/intent/user?user_id=907281&#39;,
  &#39;facebook-share-dialog&#39;,
  &#39;width=550,height=520&#39;);
return false;
">Twitter</a></li><li><a class="social-follow-gplus" title="Add me on Google+!" href="https://plus.google.com/+JustinLe">Google+</a></li><li><a class="social-follow-linkedin" title="Connect with me on LinkedIn!" href="https://linkedin.com/in/lejustin">LinkedIn</a></li><li><a class="social-follow-github" title="Fork me on Github!" href="https://github.com/mstksg">Github</a></li><li><a class="social-follow-keybase" title="Track me on Keybase!" href="https://keybase.io/mstksg">Keybase</a></li><li><a class="social-follow-bitcoin" title="Donate via bitcoin!" href="https://coinbase.com/mstksg">Bitcoin</a></li></ul></li><li><ul class="social-follows-list-site"><li><a class="social-follow-rss" title="Subscribe to my RSS Feed!" href="http://feeds.feedburner.com/incodeblog">RSS</a></li><li><a class="social-follow-email" title="Subscribe to the mailing list!" href="https://feedburner.google.com/fb/a/mailverify?loc=en_US&amp;uri=incodeblog">Mailing list</a></li></ul></li></ul></div></div></div></div></body></html>